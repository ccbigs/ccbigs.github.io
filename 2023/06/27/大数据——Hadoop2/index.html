<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据——Hadoop.2.x | ccbigs blog</title><meta name="author" content="DingQuan Zuo"><meta name="copyright" content="DingQuan Zuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="入门大数据概论大数据概念​	大数据(Big Data)︰指无法在一定时间范围内用常规软件工具进行捕捉、管理合理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 ​	主要解决，海量数据的存储和海量数据的分析计算问题。 大数据特点(4V)1.Volume(大量) ​	截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共说">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据——Hadoop.2.x">
<meta property="og:url" content="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/index.html">
<meta property="og:site_name" content="ccbigs blog">
<meta property="og:description" content="入门大数据概论大数据概念​	大数据(Big Data)︰指无法在一定时间范围内用常规软件工具进行捕捉、管理合理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 ​	主要解决，海量数据的存储和海量数据的分析计算问题。 大数据特点(4V)1.Volume(大量) ​	截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共说">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/ahead.jpg">
<meta property="article:published_time" content="2023-06-27T10:09:04.000Z">
<meta property="article:modified_time" content="2023-06-27T10:24:14.295Z">
<meta property="article:author" content="DingQuan Zuo">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/ahead.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据——Hadoop.2.x',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-27 18:24:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/ahead.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/1558.png')"><nav id="nav"><span id="blog-info"><a href="/" title="ccbigs blog"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">ccbigs blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据——Hadoop.2.x</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-27T10:09:04.000Z" title="发表于 2023-06-27 18:09:04">2023-06-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-27T10:24:14.295Z" title="更新于 2023-06-27 18:24:14">2023-06-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">19.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>84分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据——Hadoop.2.x"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><h2 id="大数据概论"><a href="#大数据概论" class="headerlink" title="大数据概论"></a>大数据概论</h2><h3 id="大数据概念"><a href="#大数据概念" class="headerlink" title="大数据概念"></a>大数据概念</h3><p>​	大数据(Big Data)︰指无法在一定时间范围内用常规软件工具进行捕捉、管理合理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。</p>
<p>​	<strong>主要解决，海量数据的存储和海量数据的分析计算问题。</strong></p>
<h3 id="大数据特点-4V"><a href="#大数据特点-4V" class="headerlink" title="大数据特点(4V)"></a>大数据特点(4V)</h3><p><strong>1.Volume(大量)</strong></p>
<p>​	截至目前，人类生产的所有<strong>印刷材料的数据量是200PB</strong>，而历史上全人类总共<strong>说过的话的数据量大约是5EB</strong>。当前，典型个人计算机硬盘的容量为TB量级，而—些大<strong>企业的数居量已经接近EB</strong>量级。</p>
<p><strong>2.Velocity(高速)</strong></p>
<p>​	这是大数据区分于传统数据挖掘的最显著特征。根据IDC的“数字宁宙”的报告，<strong>预计到2020年，全球数据使用量将达到35.2ZB</strong>。在如此海量的数据面前，处理数据的效率就是企业的生命。<br>​	天猫双十一:2017年3分01秒，天猫交易额超寸100亿</p>
<p><strong>3.Variety(多样)</strong></p>
<p>​	这种类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的<strong>以数据库&#x2F;文本为主的结构化数据，非结构化数据</strong>越来越多，包括<strong>网络日志、音频、视频、图片、地理位置信息</strong>等，这些多类型的数据对数据的处理能力提出了更高要求。</p>
<p><strong>4.Value(低价值密度)</strong></p>
<p>​	价值密度的高低与数据总量的大小成反比。比如，在—天监控视频中，我们只关心宋宋老师晚上在床上健身那一分钟，如何<strong>快速对有价值数据“提纯”成为目前大数据背景下待解决的难题</strong>。</p>
<h3 id="大数据应用场景"><a href="#大数据应用场景" class="headerlink" title="大数据应用场景"></a>大数据应用场景</h3><ol>
<li>物流仓储：大数据分析系统助力商家精细化运营、提升销量、节约成本。</li>
<li>零售：分析用户消费习惯，为用户购买商品提供方便，从而提升商品销量。经典案例，子尿布+啤酒。</li>
<li>旅游：深度结合大数据能力与旅游行业需求，共建旅游产业智慧管理、智慧服务和智慧营销的未来。</li>
<li>商品广告推荐：给用户推荐可能喜欢的商品。</li>
<li>保险：海量数据挖掘及风险预测，助力保险行业精准营销,提升精细化定价能力。</li>
<li>金融：多维度体现用户特征，帮助金融机构推荐优质客户，防范欺诈风险。</li>
<li>房产：大数据全面助力房地产行业，打造精准投策与营销，选出更合适的地，建造更合适的楼，卖给更合适的人。</li>
<li>人工智能。</li>
</ol>
<h3 id="大数据发展前景"><a href="#大数据发展前景" class="headerlink" title="大数据发展前景"></a>大数据发展前景</h3><p>​	1、党的十八大提出“实施国家大数据战略”，国务院印发《促进大数据发展行动纲要》，大数居支术和应用处于创新突破期，国内市场需求处于爆发期，我国大数据产业面临重要的发展机遇。</p>
<p>​	2、党的十九大提出“推动互联网、大数据、人工智能和实体经济深度融合”。</p>
<p>​	3、国际数据公司IDC预测，到2020年，企业基于大数据计算分析平台的支出将突破5000亿美元。目前，我国大数据人才只有46万，未来3到5年人才缺口达150万之多。</p>
<p>​	4、2017年北京大学、中国人民大学、北京邮电大学等25所高校成功申请开设大数居课程。</p>
<p>​	5、大数据属于新兴技术，大牛少，竞争力少。</p>
<p>​	6、在北京大数据开发工程帕的平均薪水已经到24060元(数据充计来职友集)，而且目前还保持强劲的发展势头。</p>
<h3 id="大数据业务流程"><a href="#大数据业务流程" class="headerlink" title="大数据业务流程"></a>大数据业务流程</h3><p>​	<img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211124165807190.png" alt="image-20211124165807190"></p>
<h3 id="大数据部门组织结构-重点"><a href="#大数据部门组织结构-重点" class="headerlink" title="大数据部门组织结构(重点)"></a>大数据部门组织结构(重点)</h3><p>大数据部门组织结构，适合中大企业：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211124165901962.png" alt="image-20211124165901962"></p>
<h2 id="从Hadoop框架讨论大数据生态"><a href="#从Hadoop框架讨论大数据生态" class="headerlink" title="从Hadoop框架讨论大数据生态"></a>从Hadoop框架讨论大数据生态</h2><h3 id="Hadoop是什么"><a href="#Hadoop是什么" class="headerlink" title="Hadoop是什么"></a>Hadoop是什么</h3><p>1 ) Hadoop是一个由Apache基金会所开发的分布式系统基础架构。</p>
<p>2)主要解决，海量数据的存储和海量数据的分析计算问题。</p>
<p>3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211124171613679.png" alt="image-20211124171613679"></p>
<h3 id="Hadoop发展历史"><a href="#Hadoop发展历史" class="headerlink" title="Hadoop发展历史"></a>Hadoop发展历史</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211124173139543.png" alt="image-20211124173139543"></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211124173223902.png" alt="image-20211124173223902"></p>
<h3 id="Hadoop的发行版本"><a href="#Hadoop的发行版本" class="headerlink" title="Hadoop的发行版本"></a>Hadoop的发行版本</h3><p>​	Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p>
<p>​	Apache版本最原始（最基础）的版本，对于入门学习最好。</p>
<p>​	Cloudera在大型互联网企业中用的较多。</p>
<p>Hortonworks文档较好。</p>
<ol>
<li><strong>Apache Hadoop</strong></li>
</ol>
<p>​		官网地址：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/releases.html">http://hadoop.apache.org/releases.html</a></p>
<p>​		下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/">https://archive.apache.org/dist/hadoop/common/</a></p>
<p>​	<strong>2.Cloudera Hadoop</strong> </p>
<p>​		官网地址：<a target="_blank" rel="noopener" href="https://www.cloudera.com/downloads/cdh/5-10-0.html">https://www.cloudera.com/downloads/cdh/5-10-0.html</a></p>
<p>​		下载地址：<a target="_blank" rel="noopener" href="http://archive-primary.cloudera.com/cdh5/cdh/5/">http://archive-primary.cloudera.com/cdh5/cdh/5/</a></p>
<p>​	（1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。</p>
<p>​	<strong>（2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司</strong>。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support</p>
<p>​	（3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。</p>
<p>​	（4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。</p>
<p>​	（5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。</p>
<ol start="3">
<li><strong>Hortonworks Hadoop</strong></li>
</ol>
<p>​	官网地址：<a target="_blank" rel="noopener" href="https://hortonworks.com/products/data-center/hdp/">https://hortonworks.com/products/data-center/hdp/</a></p>
<p>​	下载地址：<a target="_blank" rel="noopener" href="https://hortonworks.com/downloads/#data-platform">https://hortonworks.com/downloads/#data-platform</a></p>
<p>​	（1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。</p>
<p>​	<strong>（2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。</strong></p>
<p>​	（3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。</p>
<p>​	（4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。</p>
<p>​	（5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</p>
<p>​	（6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的Microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。</p>
<h3 id="Hadoop的优势（4高）"><a href="#Hadoop的优势（4高）" class="headerlink" title="Hadoop的优势（4高）"></a>Hadoop的优势（4高）</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211124174001877.png" alt="image-20211124174001877"></p>
<h3 id="Hadoop1-x和hadoop2-x的区别"><a href="#Hadoop1-x和hadoop2-x的区别" class="headerlink" title="Hadoop1.x和hadoop2.x的区别"></a>Hadoop1.x和hadoop2.x的区别</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211125160738890.png" alt="image-20211125160738890"></p>
<h4 id="HDFS架构概述"><a href="#HDFS架构概述" class="headerlink" title="HDFS架构概述"></a>HDFS架构概述</h4><p>HDFS（Hadoop Distributed File System）的架构概述，如图2-23所示。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211125161352781.png" alt="image-20211125161352781"></p>
<h4 id="Yarn架构"><a href="#Yarn架构" class="headerlink" title="Yarn架构"></a>Yarn架构</h4><p>YARN架构概述，如图2-24所示。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211125162644186.png" alt="image-20211125162644186"></p>
<h4 id="MapReduce架构概述"><a href="#MapReduce架构概述" class="headerlink" title="MapReduce架构概述"></a>MapReduce架构概述</h4><p>MapReduce将计算过程分为两个阶段：Map和Reduce，如图2-25所示</p>
<p>​	1）Map阶段并行处理输入数据</p>
<p>​	2）Reduce阶段对Map结果进行汇总</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211125163541985.png" alt="image-20211125163541985"></p>
<h3 id="大数据技术生态体系"><a href="#大数据技术生态体系" class="headerlink" title="大数据技术生态体系"></a>大数据技术生态体系</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211125163655668.png" alt="image-20211125163655668"></p>
<p>图中涉及的技术名词解释如下：</p>
<p>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
<p>2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p>
<p>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：</p>
<p>（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。</p>
<p>（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。</p>
<p>（3）支持通过Kafka服务器和消费机集群来分区消息。</p>
<p>（4）支持Hadoop并行数据加载。</p>
<p>4）Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</p>
<p>5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p>
<p>6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p>
<p>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
<p>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
<p>10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。</p>
<p>11）Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。</p>
<p>12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
<h3 id="推荐系统框架图"><a href="#推荐系统框架图" class="headerlink" title="推荐系统框架图"></a>推荐系统框架图</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211125165149870.png" alt="image-20211125165149870"></p>
<h2 id="Hadoop环境搭建"><a href="#Hadoop环境搭建" class="headerlink" title="Hadoop环境搭建"></a>Hadoop环境搭建</h2><p>首先最小化的安装Linux</p>
<p><strong>配置：首先关闭防火墙</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> firewalld.service <span class="comment">#永久关闭防火墙</span></span><br></pre></td></tr></table></figure>



<p><strong>设置本机名称</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname hadoopxxx</span><br></pre></td></tr></table></figure>



<h3 id="准备操作"><a href="#准备操作" class="headerlink" title="准备操作"></a>准备操作</h3><p>下载好之后我们可以配置yum源，配置yum源参考文档。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://blog.csdn.net/zuodingquan666/article/details/119352716?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163783080616780265452585%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=163783080616780265452585&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-1-119352716.pc_v2_rank_blog_default&amp;utm_term=yum&amp;spm=1018.2226.3001.4450</span><br></pre></td></tr></table></figure>

<p>案后配置好ip地址，同时下载好net-tools工具包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install vim</span><br><span class="line">yum install net-tools</span><br><span class="line">yum install -y epel-release</span><br></pre></td></tr></table></figure>

<h3 id="前题：配置用户等信息"><a href="#前题：配置用户等信息" class="headerlink" title="前题：配置用户等信息"></a>前题：配置用户等信息</h3><p><strong>配置主机名称</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]<span class="comment"># vim /etc/hosts</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211210172416060.png" alt="image-20211210172416060"></p>
<p><strong>创建用户</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]<span class="comment"># useradd atguigu</span></span><br><span class="line">[root@hadoop100 ~]<span class="comment"># passwd atguigu</span></span><br><span class="line"><span class="comment">#密码7760608abc</span></span><br></pre></td></tr></table></figure>



<h3 id="配置系统用户"><a href="#配置系统用户" class="headerlink" title="配置系统用户"></a>配置系统用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]<span class="comment"># vim /etc/sudoers</span></span><br><span class="line"><span class="comment">#atguigu ALL=(ALL) NOPASSWD:ALL</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126150645412.png" alt="image-20211126150645412"></p>
<p>随后我们就可以成功切换到atguigu用户了</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126150740628.png" alt="image-20211126150740628"></p>
<h3 id="创建配置目录"><a href="#创建配置目录" class="headerlink" title="创建配置目录"></a>创建配置目录</h3><p><strong>在&#x2F;opt目录下面创建两个目录module、software两个文件夹</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 opt]$ sudo mkdir module</span><br><span class="line">[atguigu@hadoop100 opt]$ sudo mkdir software</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126153306480.png" alt="image-20211126153306480"></p>
<p>修改两个目录的所有组和所有用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 opt]$ sudo chown atguigu:atguigu -R /opt/module/ /software/</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126153516263.png" alt="image-20211126153516263"></p>
<h3 id="上传应用文件"><a href="#上传应用文件" class="headerlink" title="上传应用文件"></a>上传应用文件</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126160606777.png" alt="image-20211126160606777"></p>
<p>将应用解压到&#x2F;opt&#x2F;module</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 ~]$ tar -zxvf jdk-8u281-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>进入到jdk解压的目录，查看jdk所在的路径：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126161426139.png" alt="image-20211126161426139"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/jdk1.8.0_281</span><br></pre></td></tr></table></figure>

<p><strong>解压Hadoop</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 ~]$ tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126162808778.png" alt="image-20211126162808778"></p>
<h3 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h3><p>编辑Java的配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 jdk1.8.0_281]$ sudo vim /etc/profile</span><br></pre></td></tr></table></figure>

<p>在最下面新增以下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_281</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>更新配置的变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 jdk1.8.0_281]$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>检测Java的环境变量是否生效：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 jdk1.8.0_281]$ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_281&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_281-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode)</span><br></pre></td></tr></table></figure>



<p><strong>配置Hadoop的环境变量</strong></p>
<p>先复制hadoop所处的路径</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<p>修改配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ sudo vim /etc/profile</span><br></pre></td></tr></table></figure>

<p>在profile下面继续追加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>

<p>更新一下配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>查看hadoop是否生效：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 module]$ hadoop</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126164831297.png" alt="image-20211126164831297"></p>
<h3 id="Hadoop的目录"><a href="#Hadoop的目录" class="headerlink" title="Hadoop的目录"></a>Hadoop的目录</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126165341406.png" alt="image-20211126165341406"></p>
<p>（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本</p>
<p>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</p>
<p>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
<p>（4）sbin目录：存放启动或停止Hadoop相关服务的脚本</p>
<p>（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例</p>
<h2 id="Hadoop运行模式"><a href="#Hadoop运行模式" class="headerlink" title="Hadoop运行模式"></a>Hadoop运行模式</h2><p>Hadoop运行模式包括：本地模式、伪分布式模式以及完全分布式模式。</p>
<p>Hadoop官方网站：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
<h3 id="本地运行模式"><a href="#本地运行模式" class="headerlink" title="本地运行模式"></a>本地运行模式</h3><h4 id="官方Grep案例"><a href="#官方Grep案例" class="headerlink" title="官方Grep案例"></a>官方Grep案例</h4><ol>
<li>创建在hadoop-2.7.2文件下面创建一个input文件夹</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ mkdir input</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>将Hadoop的xml配置文件复制到input</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ cp etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>执行share目录下的MapReduce程序</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ bin/hadoop jar</span><br><span class="line">share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>查看输出结果</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ cat output/*</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211126172108132.png" alt="image-20211126172108132"></p>
<h4 id="官方WordCount案例"><a href="#官方WordCount案例" class="headerlink" title="官方WordCount案例"></a>官方WordCount案例</h4><ol>
<li>创建在hadoop-2.7.2文件下面创建一个wcinput文件夹</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">mkdir</span> wcinput</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在wcinput文件下创建一个wc.input文件</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">cd</span> wcinput/</span><br><span class="line">[atguigu@hadoop100 wcinput]$ <span class="built_in">touch</span> wc.input</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>编辑wc.input文件</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 wcinput]$ vim wc.input </span><br></pre></td></tr></table></figure>

<p>输入以下内容</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">atguigu</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure>

<p>​	4.回到Hadoop目录&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2</p>
<ol start="5">
<li>执行程序</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>查看结果</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">cat</span> wcoutput/part-r-00000 </span><br><span class="line">atguigu	2</span><br><span class="line">hadoop	2</span><br><span class="line">mapreduce	1</span><br><span class="line">yarn	1</span><br></pre></td></tr></table></figure>



<h3 id="伪分布式的运行"><a href="#伪分布式的运行" class="headerlink" title="伪分布式的运行"></a>伪分布式的运行</h3><h4 id="启动HDFS并运行MapReduce程序"><a href="#启动HDFS并运行MapReduce程序" class="headerlink" title="启动HDFS并运行MapReduce程序"></a>启动HDFS并运行MapReduce程序</h4><ol>
<li><strong>分析</strong></li>
</ol>
<p>​    （1）配置集群</p>
<p>​    （2）启动、测试集群增、删、查</p>
<p>​    （3）执行WordCount案例</p>
<ol start="2">
<li><strong>执行步骤</strong></li>
</ol>
<p>（1）配置集群</p>
<p>​         （a）配置：hadoop-env.sh</p>
<p>​			Linux系统中获取JDK的安装路径：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@ hadoop101 ~]# echo $JAVA_HOME</span><br><span class="line">/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

<p>​			修改JAVA_HOME 路径：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

<p>​		（b）配置：core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		（c）配置：hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）启动集群</p>
<p>​		（a）<strong>格式化NameNode</strong>（第一次启动时格式化，以后就不要总格式化）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>​         （b）启动NameNode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>​         （c）启动DataNode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p>（3）查看集群</p>
<p>​         （a）查看是否启动成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ jps</span><br><span class="line">13586 NameNode</span><br><span class="line">13668 DataNode</span><br><span class="line">13786 Jps</span><br></pre></td></tr></table></figure>

<p>注意：jps是JDK中的命令，不是Linux命令。不安装JDK不能使用jps</p>
<p>​         （b）web端查看HDFS文件系统</p>
<p>​			<a target="_blank" rel="noopener" href="http://hadoop101:50070/dfshealth.html#tab-overview">http://hadoop101:50070/dfshealth.html#tab-overview</a></p>
<p>​			注意：如果不能查看，看如下帖子处理</p>
<p>​			<a target="_blank" rel="noopener" href="http://www.cnblogs.com/zlslch/p/6604189.html">http://www.cnblogs.com/zlslch/p/6604189.html</a></p>
<p>​		（c）查看产生的Log日志</p>
<p>​          说明：在企业中遇到Bug时，经常根据日志提示信息去分析问题、解决Bug。</p>
<p>​		  当前目录：&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;logs</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 logs]$ <span class="built_in">ls</span></span><br><span class="line">hadoop-atguigu-datanode-hadoop.atguigu.com.log</span><br><span class="line">hadoop-atguigu-datanode-hadoop.atguigu.com.out</span><br><span class="line">hadoop-atguigu-namenode-hadoop.atguigu.com.log</span><br><span class="line">hadoop-atguigu-namenode-hadoop.atguigu.com.out</span><br><span class="line">SecurityAuth-root.audit</span><br><span class="line">[atguigu@hadoop101 logs]<span class="comment"># cat hadoop-atguigu-datanode-hadoop101.log</span></span><br></pre></td></tr></table></figure>

<p><strong>（d）思考：为什么不能一直格式化NameNode，格式化NameNode</strong>，要注意什么？</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ <span class="built_in">cd</span> data/tmp/dfs/name/current/</span><br><span class="line">[atguigu@hadoop101 current]$ <span class="built_in">cat</span> VERSION</span><br><span class="line">clusterID=CID-f0330a58-36fa-4a2a-a65f-2688269b5837</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ <span class="built_in">cd</span> data/tmp/dfs/data/current/</span><br><span class="line">clusterID=CID-f0330a58-36fa-4a2a-a65f-2688269b5837</span><br></pre></td></tr></table></figure>



<h2 id="完全分布式运行模式"><a href="#完全分布式运行模式" class="headerlink" title="完全分布式运行模式"></a>完全分布式运行模式</h2><p>分析：</p>
<p>​    1）准备3台客户机（关闭防火墙、静态ip、主机名称）</p>
<p>​    2）安装JDK</p>
<p>​    3）配置环境变量</p>
<p>​    4）安装Hadoop</p>
<p>​    5）配置环境变量</p>
<p>​	6）配置集群</p>
<p>​	7）单点启动</p>
<p>​    8）配置ssh</p>
<p>​    9）群起并测试集群</p>
<h3 id="虚拟机准备"><a href="#虚拟机准备" class="headerlink" title="虚拟机准备"></a>虚拟机准备</h3><p>自己装</p>
<p>需要关闭全部的防火墙</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line">[root@hadoop100 ~]<span class="comment"># systemctl disable firewalld.service</span></span><br></pre></td></tr></table></figure>



<p>设置atguigu用户的密码，每台虚拟机都要设置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ su</span><br><span class="line">密码：</span><br><span class="line">[root@hadoop102 module]<span class="comment"># passwd atguigu</span></span><br><span class="line">更改用户 atguigu 的密码 。</span><br><span class="line">新的 密码：</span><br><span class="line">重新输入新的 密码：</span><br><span class="line">passwd：所有的身份验证令牌已经成功更新。</span><br><span class="line">[root@hadoop102 module]<span class="comment"># su -atguigu</span></span><br></pre></td></tr></table></figure>



<h3 id="编写集群分发脚本xsync"><a href="#编写集群分发脚本xsync" class="headerlink" title="编写集群分发脚本xsync"></a>编写集群分发脚本xsync</h3><h4 id="scp安全拷贝"><a href="#scp安全拷贝" class="headerlink" title="scp安全拷贝"></a>scp安全拷贝</h4><p>scp（secure copy）安全拷贝</p>
<p>（1）scp定义：</p>
<p>​		scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</p>
<p>（2）基本语法</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp    -r          <span class="variable">$pdir</span>/<span class="variable">$fname</span>              <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$pdir</span>/<span class="variable">$fname</span></span><br><span class="line">命令   递归         要拷贝的文件路径/名称         目的用户@主机:目的路径/名称</span><br></pre></td></tr></table></figure>

<p>（3）实例操作</p>
<p>​	将hadoop100拷贝到hadoop101上的&#x2F;opt&#x2F;moudle</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 opt]$ scp -r module root@hadoop101:/opt/</span><br></pre></td></tr></table></figure>

<p>​	从hadoop100上拉去数据到hadoop102</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]<span class="comment"># scp -r root@hadoop100:/opt/module ./</span></span><br></pre></td></tr></table></figure>

<p>​	在hadoop101上将hadoop100的数据拷贝到hadoop102上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 module]$ scp -r  root@hadoop100:/opt/module/ root@hadoop102:/opt/module/</span><br></pre></td></tr></table></figure>



<p>（4）修改用户权限组，注意两个都要改</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 module]$ sudo <span class="built_in">chown</span> atguigu:atguigu ./ -R</span><br></pre></td></tr></table></figure>



<p>（5）同步配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 opt]$ scp /etc/profile root@hadoop101:/etc/profile </span><br><span class="line">[atguigu@hadoop100 opt]$ scp /etc/profile root@hadoop102:/etc/profile</span><br></pre></td></tr></table></figure>



<p>（6）更新配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>



<h4 id="xsync与jpsall脚本"><a href="#xsync与jpsall脚本" class="headerlink" title="xsync与jpsall脚本"></a>xsync与jpsall脚本</h4><p>​	rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>​	rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p>
<p>​    （1）基本语法</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rsync   -rvl    <span class="variable">$pdir</span>/<span class="variable">$fname</span>        <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$pdir</span>/<span class="variable">$fname</span></span><br><span class="line"></span><br><span class="line">命令  选项参数  要拷贝的文件路径/名称  目的用户@主机:目的路径/名称</span><br></pre></td></tr></table></figure>

<p>​     选项参数说明</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
<tr>
<td>-l</td>
<td>拷贝符号连接</td>
</tr>
</tbody></table>
<p>（2）案例实操</p>
<p>​        （a）把hadoop101机器上的&#x2F;opt&#x2F;software目录同步到hadoop102服务器的root用户下的&#x2F;opt&#x2F;目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 opt]$ rsync -rvl /opt/software/ root@hadoop102:/opt/software</span><br></pre></td></tr></table></figure>



<p><strong>创建jpsall脚本，来观察集群里面的jps信息</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop100 hadoop101 hadoop102</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"> <span class="built_in">echo</span> ---------- jps <span class="variable">$i</span> 启动 ------------</span><br><span class="line">ssh <span class="variable">$i</span> <span class="string">&quot;jps&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>



<p><strong>集群命令脚本</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">$mind</span><br><span class="line"></span><br><span class="line">for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">do</span><br><span class="line"> echo ---------- jps $i 启动 ------------</span><br><span class="line">ssh $i $mind</span><br><span class="line"></span><br><span class="line">done;</span><br></pre></td></tr></table></figure>



<h4 id="xsync集群分发脚本"><a href="#xsync集群分发脚本" class="headerlink" title="xsync集群分发脚本"></a>xsync集群分发脚本</h4><p>（1）需求：循环复制文件到所有节点的相同目录下</p>
<p>（2）需求分析：</p>
<p>（a）rsync命令原始拷贝：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -rvl   /opt/module         root@hadoop103:/opt/</span><br></pre></td></tr></table></figure>

<p>​        （b）期望脚本：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync要同步的文件名称</span><br></pre></td></tr></table></figure>

<p>​        （c）说明：在&#x2F;home&#x2F;atguigu&#x2F;bin这个目录下存放的脚本，atguigu用户可以在系统任何地方直接执行。</p>
<p>（3）脚本的实现</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span>((pcount==<span class="number">0</span>)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2 获取文件名称</span></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`<span class="built_in">basename</span> <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`<span class="built_in">cd</span> -P $(<span class="built_in">dirname</span> <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> pdir=<span class="variable">$pdir</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4 获取当前用户名称</span></span><br><span class="line">user=`<span class="built_in">whoami</span>`</span><br><span class="line"></span><br><span class="line"><span class="comment">#5 循环</span></span><br><span class="line"><span class="keyword">for</span>((host=<span class="number">100</span>; host&lt;<span class="number">103</span>; host++)); <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> ------------------- hadoop<span class="variable">$host</span> --------------</span><br><span class="line">        rsync -rvl <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$pdir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>分发xsync.sh到其他的虚拟机上：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127000516771.png" alt="image-20211127000516771"></p>
<h4 id="ssh密钥免登录配置"><a href="#ssh密钥免登录配置" class="headerlink" title="ssh密钥免登录配置"></a>ssh密钥免登录配置</h4><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127100836308.png" alt="image-20211127100836308"></p>
<p><strong>1.配置ssh</strong></p>
<p>（1）基本语法</p>
<p>​	ssh另一台电脑的ip地址</p>
<p>（2）语法</p>
<p>先进入目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ <span class="built_in">ls</span> -a</span><br><span class="line">.  ..  .bash_history  .bash_logout  .bash_profile  .bashrc  bin  .cache  .config  .ssh</span><br><span class="line">[atguigu@hadoop101 ~]$ <span class="built_in">cd</span> .ssh/</span><br><span class="line">[atguigu@hadoop101 .ssh]$ <span class="built_in">ls</span> -a</span><br><span class="line">.  ..  known_hosts</span><br></pre></td></tr></table></figure>

<p>生成ssh-key，输入命令后，三次回车即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 .ssh]$ ssh-keygen -t rsa</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file <span class="keyword">in</span> <span class="built_in">which</span> to save the key (/home/atguigu/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty <span class="keyword">for</span> no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved <span class="keyword">in</span> /home/atguigu/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved <span class="keyword">in</span> /home/atguigu/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:hDZghwt8jMBGX8i8ptEeNBRF9x7D7gKHT28OgFs5IEQ atguigu@hadoop101</span><br><span class="line">The key<span class="string">&#x27;s randomart image is:</span></span><br><span class="line"><span class="string">+---[RSA 2048]----+</span></span><br><span class="line"><span class="string">|=E+=B=..         |</span></span><br><span class="line"><span class="string">|.o=B=o..o        |</span></span><br><span class="line"><span class="string">|..o=o.+ .=       |</span></span><br><span class="line"><span class="string">| ..=+.ooo o      |</span></span><br><span class="line"><span class="string">|  =..B oSo       |</span></span><br><span class="line"><span class="string">| . .o B o        |</span></span><br><span class="line"><span class="string">|   .   + +       |</span></span><br><span class="line"><span class="string">|        =        |</span></span><br><span class="line"><span class="string">|         .       |</span></span><br><span class="line"><span class="string">+----[SHA256]-----+</span></span><br><span class="line"><span class="string">[atguigu@hadoop101 .ssh]$ </span></span><br></pre></td></tr></table></figure>

<p>然后查看生产的钥匙：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127101434967.png" alt="image-20211127101434967"></p>
<p>然后我们将生产的公钥拷贝到hadoop102中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">/bin/ssh-copy-id: INFO: Source of key(s) to be installed: <span class="string">&quot;/home/atguigu/.ssh/id_rsa.pub&quot;</span></span><br><span class="line">/bin/ssh-copy-id: INFO: attempting to <span class="built_in">log</span> <span class="keyword">in</span> with the new key(s), to filter out any that are already installed</span><br><span class="line">/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- <span class="keyword">if</span> you are prompted now it is to install the new keys</span><br><span class="line">atguigu@hadoop102<span class="string">&#x27;s password: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Number of key(s) added: 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Now try logging into the machine, with:   &quot;ssh &#x27;</span>hadoop102<span class="string">&#x27;&quot;</span></span><br><span class="line"><span class="string">and check to make sure that only the key(s) you wanted were added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[atguigu@hadoop101 .ssh]$ </span></span><br></pre></td></tr></table></figure>

<p>我们可以看到在hadoop102上多了一个authorized_keys</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127102239123.png" alt="image-20211127102239123"></p>
<p>同理，我们将ssh拷贝到hadoop100中，然后我们还需要将公钥拷贝到自己的服务器上：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 .ssh]$ ssh-copy-id hadoop100</span><br><span class="line">[atguigu@hadoop101 .ssh]$ ssh-copy-id hadoop101</span><br></pre></td></tr></table></figure>



<p><strong>同理，我们在hadoop100和hadoop102按照以上进行配置，不要忘记给自己发公钥了</strong></p>
<p><strong>我们同时要注意的是，我们目前配置的话只是atguigu这个用户的免密登录，但是我们的root没有配置，我们需要对root也进行配置</strong></p>
<h4 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h4><p><strong>1.集群规划部署</strong></p>
<p>hadoop102对应我集群中的hadoop100</p>
<p>hadoop103对应我集群中的hadoop101</p>
<p>hadoop104对应我集群中的hadoop102</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127092301334.png" alt="image-20211127092301334"></p>
<p><strong>2.配置集群</strong></p>
<p>（1）核心配置文件</p>
<p>​	配置core-sote.xml</p>
<p>首先进入配置目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop]<span class="comment"># pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[root@hadoop101 hadoop]<span class="comment"># vim core-site.xml </span></span><br></pre></td></tr></table></figure>

<p>在该文件夹编写如下配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop100:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127092827719.png" alt="image-20211127092827719"></p>
<p>（2）HDFS配置文件</p>
<p>配置hadoop-env.sh,同样在上一个目录下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop]<span class="comment"># vim hadoop-env.sh </span></span><br></pre></td></tr></table></figure>

<p>在最下方追加：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_281</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127093208556.png" alt="image-20211127093208556"></p>
<p>配置hdfs-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop]$ vim hdfs-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>（3）YARN配置文件</p>
<p>配置yarn-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop]$ vim yarn-env.sh</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_281</span><br></pre></td></tr></table></figure>

<p>配置yarn-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>（4）MapReduce配置文件</p>
<p>配置mapred-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop]$ vim mapred-env.sh</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_281</span><br></pre></td></tr></table></figure>

<p>配置mapred-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop]$ <span class="built_in">cp</span> mapred-site.xml.template mapred-site.xml</span><br><span class="line">[atguigu@hadoop101 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><strong>3．在集群上分发配置好的Hadoop配置文件</strong></p>
<p>首先要看清你在哪一个目录文件夹，然后将hadoop分发出去</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 etc]$ /home/atguigu/bin/xsync.sh hadoop</span><br></pre></td></tr></table></figure>



<h4 id="集群单点启动"><a href="#集群单点启动" class="headerlink" title="集群单点启动"></a>集群单点启动</h4><p><strong>注意：一定要看好我是在哪台服务器上输入命令的</strong></p>
<p>（1）如果集群是第一次启动，需要<strong>格式化NameNode</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127095423462.png" alt="image-20211127095423462"></p>
<p>（2）在hadoop100上启动NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127095854163.png" alt="image-20211127095854163"></p>
<p>（3）在hadoop100、hadoop101以及hadoop102上分别启动DataNode</p>
<p><strong>Hadoop100</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop]$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127100031117.png" alt="image-20211127100031117"></p>
<p><strong>Hadoop101</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127100224006.png" alt="image-20211127100224006"></p>
<p><strong>Hadoop102</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127100251047.png" alt="image-20211127100251047"></p>
<p><strong>查看启动是否成功</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127100521011.png" alt="image-20211127100521011"></p>
<h4 id="群起集群"><a href="#群起集群" class="headerlink" title="群起集群"></a>群起集群</h4><ol>
<li><strong>配置slaves</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop/slaves</span><br><span class="line">[atguigu@hadoop100 hadoop]$ vim slaves</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下内容,一开始里面有个localhost，先删掉在新增下面的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop100</span><br><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>

<p>配置完成之后，进行分发</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop]$ xsync.sh slaves </span><br></pre></td></tr></table></figure>



<ol start="2">
<li><strong>启动集群</strong></li>
</ol>
<p>​    （1）如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop]$ hadoop-daemon.sh stop datanode</span><br><span class="line">[atguigu@hadoop100 hadoop]$ hadoop-daemon.sh stop namenode</span><br><span class="line"><span class="comment">#其他服务器上的datanode都要停下来</span></span><br></pre></td></tr></table></figure>



<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127103942419.png" alt="image-20211127103942419"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop namenode -format</span><br></pre></td></tr></table></figure>



<p>开始启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">Starting namenodes on [hadoop100]</span><br><span class="line">hadoop100: starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-namenode-hadoop100.out</span><br><span class="line">hadoop102: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-datanode-hadoop102.out</span><br><span class="line">hadoop101: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-datanode-hadoop101.out</span><br><span class="line">hadoop100: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-datanode-hadoop100.out</span><br><span class="line">Starting secondary namenodes [hadoop102]</span><br><span class="line">hadoop102: starting secondarynamenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-secondarynamenode-hadoop102.out</span><br></pre></td></tr></table></figure>



<p><strong>启动Yarn，必须在你配置的Yarn上面，比如我的就在hadoop101</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-atguigu-resourcemanager-hadoop101.out</span><br><span class="line">hadoop100: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-atguigu-nodemanager-hadoop100.out</span><br><span class="line">hadoop102: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-atguigu-nodemanager-hadoop102.out</span><br><span class="line">hadoop101: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-atguigu-nodemanager-hadoop101.out</span><br></pre></td></tr></table></figure>

<p>启动过后，jps中的进行应该和下图一至：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127105320695.png" alt="image-20211127105320695"></p>
<p>查看SecondaryNameNode节点是否启动成功：</p>
<p><a target="_blank" rel="noopener" href="http://192.168.17.43:50090/status.html">SecondaryNamenode information</a></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127105736349.png" alt="image-20211127105736349"></p>
<p><strong>3.集群的一个测试</strong></p>
<p>（1）上传文件到集群</p>
<p>​     上传小文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/atguigu/input</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -put wcinput/wc.input /user/atguigu/input</span><br></pre></td></tr></table></figure>

<p>​	 我们上传一个大文件，我们上穿到根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hdfs dfs -put /home/atguigu/hadoop-2.7.2.tar.gz /</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127161341898.png" alt="image-20211127161341898"></p>
<p><strong>（2）上传文件后查看文件存放在什么位置</strong></p>
<p>​	（a）查看HDFS文件存储路径</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">cd</span> data/tmp/dfs/data/current/BP-846787476-192.168.17.41-1637978270396/current/finalized/subdir0/subdir0/</span><br><span class="line">[atguigu@hadoop100 subdir0]$ ll</span><br><span class="line">总用量 194552</span><br><span class="line">-rw-rw-r-- 1 atguigu atguigu        46 11月 27 16:09 blk_1073741825</span><br><span class="line">-rw-rw-r-- 1 atguigu atguigu        11 11月 27 16:09 blk_1073741825_1001.meta</span><br><span class="line">-rw-rw-r-- 1 atguigu atguigu 134217728 11月 27 16:12 blk_1073741826</span><br><span class="line">-rw-rw-r-- 1 atguigu atguigu   1048583 11月 27 16:12 blk_1073741826_1002.meta</span><br><span class="line">-rw-rw-r-- 1 atguigu atguigu  63439959 11月 27 16:12 blk_1073741827</span><br><span class="line">-rw-rw-r-- 1 atguigu atguigu    495635 11月 27 16:12 blk_1073741827_1003.meta</span><br><span class="line">[atguigu@hadoop100 subdir0]$ </span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127161545146.png" alt="image-20211127161545146"></p>
<h4 id="集群启动-x2F-停止方式总结"><a href="#集群启动-x2F-停止方式总结" class="headerlink" title="集群启动&#x2F;停止方式总结"></a>集群启动&#x2F;停止方式总结</h4><ol>
<li><strong>各个服务组件逐一启动&#x2F;停止</strong></li>
</ol>
<p>​    （1）分别启动&#x2F;停止HDFS组件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start / stop namenode / datanode / secondarynamenode</span><br></pre></td></tr></table></figure>

<p>​    （2）启动&#x2F;停止YARN</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start / stop resourcemanager / nodemanager</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>各个模块分开启动&#x2F;停止（配置ssh是前提）常用</strong></li>
</ol>
<p>​    （1）整体启动&#x2F;停止HDFS</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh  / stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>​    （2）整体启动&#x2F;停止YARN</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh / stop-yarn.sh</span><br></pre></td></tr></table></figure>



<h4 id="crond系统定时任务"><a href="#crond系统定时任务" class="headerlink" title="crond系统定时任务"></a>crond系统定时任务</h4><p><strong>1.crond服务管理</strong></p>
<p>1.重新启动crond服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond restart</span><br></pre></td></tr></table></figure>



<p><strong>2.crontab定时任务设置</strong></p>
<p>1.基本语法</p>
<p>crontab [选项]</p>
<p>2.选项说明</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>编辑crontab定时任务</td>
</tr>
<tr>
<td>-l</td>
<td>查询crontab任务</td>
</tr>
<tr>
<td>-r</td>
<td>删除当前用户所有的crontab任务</td>
</tr>
</tbody></table>
<p>3.参数说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure>

<p>(1)进入crontab编辑界面，会打开vim编辑你的工作</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127163148671.png" alt="image-20211127163148671"></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127163424417.png" alt="image-20211127163424417"></p>
<p>执行特定时间的命令</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127163558145.png" alt="image-20211127163558145"></p>
<p><strong>示例</strong></p>
<p>向banzhang.txt我呢见当中每分钟追加文字</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure>

<p>写入以下内容</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/<span class="number">1</span> * * * * /bin/<span class="keyword">echo</span> <span class="string">&quot;yanjing&quot;</span> &gt;&gt; /home/atguigu/testFIle/banzhang.txt</span><br></pre></td></tr></table></figure>

<p>启动服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond restart</span><br></pre></td></tr></table></figure>

<p>有可能需要输入时间：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127164551852.png" alt="image-20211127164551852"></p>
<p>查看：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127164709807.png" alt="image-20211127164709807"></p>
<p><strong>查询当前的任务</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -l</span><br></pre></td></tr></table></figure>



<p><strong>关闭当前的任务</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -r</span><br></pre></td></tr></table></figure>



<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127164833841.png" alt="image-20211127164833841"></p>
<h4 id="集群时间同步"><a href="#集群时间同步" class="headerlink" title="集群时间同步"></a>集群时间同步</h4><p>​	时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127165111352.png" alt="image-20211127165111352"></p>
<p><strong>1.事件服务器配置(必须root用户)</strong></p>
<p>（1）检查ntp是否安装</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[**root**@hadoop102 ]# rpm -qa|grep ntp</span><br></pre></td></tr></table></figure>

<p>ntp-4.2.6p5-10.el6.centos.x86_64</p>
<p>fontpackages-filesystem-1.41-1.1.el6.noarch</p>
<p>ntpdate-4.2.6p5-10.el6.centos.x86_64</p>
<p>（2）修改ntp的配置文件</p>
<p>​	修改内容如下</p>
<p>a）修改1（授权192.168.17.0-192.168.17.255网段上的所有机器可以从这台机器上查询和同步时间）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为</span><br><span class="line">restrict 192.168.17.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure>

<p> b）修改2（集群在局域网中，不使用其他互联网上的时间）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst为</span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211127170819877.png" alt="image-20211127170819877"></p>
<p>c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步），在文件最下方追加以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>



<p>（3）修改&#x2F;etc&#x2F;sysconfig&#x2F;ntpd 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]<span class="comment"># vim /etc/sysconfig/ntpd</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201160452013.png" alt="image-20211201160452013"></p>
<p>增加内容如下（让硬件时间与系统时间一起同步）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>

<p>（4）重新启动ntpd服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[**root**@hadoop102 桌面]<span class="comment"># service ntpd status</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201160855619.png" alt="image-20211201160855619"></p>
<p>启动：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[**root**@hadoop102 桌面]# service ntpd start</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201160943437.png" alt="image-20211201160943437"></p>
<p>（5）设置ntpd开机启动服务（这个不做要求）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]<span class="comment"># chkconfig ntpd on</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>其他机器配置（必须root用户）</strong></li>
</ol>
<p>（1）在其他机器配置10分钟与时间服务器同步一次</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[**root**@hadoop103桌面]# crontab -e</span><br></pre></td></tr></table></figure>

<p>编写定时任务如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop100</span><br></pre></td></tr></table></figure>

<p>（2）修改任意机器时间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# date -s &quot;2018-11-11 11:11:11&quot;</span><br><span class="line">2018年 11月 11日 星期日 11:11:11 CST</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201161520205.png" alt="image-20211201161520205"></p>
<p>（3）十分钟后查看机器是否与时间服务器同步</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[**root**@hadoop103桌面]# date</span><br></pre></td></tr></table></figure>

<p>说明：测试的时候可以将10分钟调整为1分钟，节省时间。</p>
<h2 id="Hadoop源码编译"><a href="#Hadoop源码编译" class="headerlink" title="Hadoop源码编译"></a>Hadoop源码编译</h2><h3 id="前期准备工作"><a href="#前期准备工作" class="headerlink" title="前期准备工作"></a>前期准备工作</h3><ol>
<li><strong>CentOS联网</strong></li>
</ol>
<p>配置CentOS能连接外网。Linux虚拟机ping <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a> 是畅通的</p>
<p>注意：采用root角色编译，减少文件夹权限出现问题</p>
<ol start="2">
<li><strong>jar包准备(hadoop源码、JDK8、maven、ant 、protobuf)</strong></li>
</ol>
<p>（1）hadoop-2.7.2-src.tar.gz</p>
<p>（2）jdk-8u144-linux-x64.tar.gz</p>
<p>（3）apache-ant-1.9.9-bin.tar.gz（build工具，打包用的）</p>
<p>（4）apache-maven-3.0.5-bin.tar.gz</p>
<p>（5）protobuf-2.5.0.tar.gz（序列化的框架）</p>
<h3 id="上传文件"><a href="#上传文件" class="headerlink" title="上传文件"></a>上传文件</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201163634383.png" alt="image-20211201163634383"></p>
<p>上传资料</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201163915824.png" alt="image-20211201163915824"></p>
<h3 id="jar包安装"><a href="#jar包安装" class="headerlink" title="jar包安装"></a>jar包安装</h3><p>注意：所有操作必须在root用户下完成</p>
<h4 id="JDK解压"><a href="#JDK解压" class="headerlink" title="JDK解压"></a>JDK解压</h4><p>配置环境变量JAVA_HOME和PATH ，验证Java -version</p>
<h4 id="Maven解压"><a href="#Maven解压" class="headerlink" title="Maven解压"></a>Maven解压</h4><p>配置  MAVEN_HOME和PATH。</p>
<p>先解压maven</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 atguigu]<span class="comment"># tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure>

<p>配置maven的下载源</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-maven-3.0.5]<span class="comment"># vim conf/settings.xml </span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201164410197.png" alt="image-20211201164410197"></p>
<p>插入数据：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-maven-3.0.5]<span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201164556338.png" alt="image-20211201164556338"></p>
<p>环境生效：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-maven-3.0.5]<span class="comment"># source /etc/profile</span></span><br></pre></td></tr></table></figure>

<p>验证：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201164650718.png" alt="image-20211201164650718"></p>
<h4 id="ant解压"><a href="#ant解压" class="headerlink" title="ant解压"></a>ant解压</h4><p>配置  ANT _HOME和PATH</p>
<p>1.先解压到module文件夹</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 atguigu]<span class="comment"># tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure>

<p>2.配置环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-ant-1.9.9]<span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure>

<p>追加数据如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ANT_HOME</span></span><br><span class="line"><span class="built_in">export</span> ANT_HOME=/opt/module/apache-ant-1.9.9</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ANT_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>更新配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-ant-1.9.9]<span class="comment"># source /etc/profile</span></span><br></pre></td></tr></table></figure>

<p>验证：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201165448371.png" alt="image-20211201165448371"></p>
<h4 id="安装glibc-headers和g"><a href="#安装glibc-headers和g" class="headerlink" title="安装glibc-headers和g++"></a>安装glibc-headers和g++</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-ant-1.9.9]<span class="comment"># yum install glibc-headers</span></span><br><span class="line">[root@hadoop100 apache-ant-1.9.9]<span class="comment"># yum install gcc-c++</span></span><br></pre></td></tr></table></figure>

<h4 id="安装make和cmake"><a href="#安装make和cmake" class="headerlink" title="安装make和cmake"></a>安装make和cmake</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 apache-ant-1.9.9]<span class="comment"># yum install make</span></span><br><span class="line">[root@hadoop100 apache-ant-1.9.9]<span class="comment"># yum install cmake</span></span><br></pre></td></tr></table></figure>

<h4 id="解压protobuf"><a href="#解压protobuf" class="headerlink" title="解压protobuf"></a>解压protobuf</h4><p>进入到解压后protobuf主目录，&#x2F;opt&#x2F;module&#x2F;protobuf-2.5.0，然后相继执行命令</p>
<p><strong>1.解压</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 atguigu]<span class="comment"># tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure>

<p><strong>2.配置环境配置</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># ./configure </span></span><br><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># make</span></span><br><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># make check</span></span><br><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># make install</span></span><br><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># ldconfig </span></span><br><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># vim /etc/profile</span></span><br><span class="line"><span class="comment">#追加一下内容</span></span><br><span class="line"><span class="comment">#LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line">[root@hadoop100 protobuf-2.5.0]<span class="comment"># source /etc/profile</span></span><br></pre></td></tr></table></figure>

<p><strong>3.验证</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201170613654.png" alt="image-20211201170613654"></p>
<h4 id="安装openssl"><a href="#安装openssl" class="headerlink" title="安装openssl"></a>安装openssl</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openssl-devel</span><br></pre></td></tr></table></figure>

<h4 id="安装ncurses"><a href="#安装ncurses" class="headerlink" title="安装ncurses"></a>安装ncurses</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ncurses-devel</span><br></pre></td></tr></table></figure>

<p><strong>到此，安装编译工具已经完成</strong></p>
<h3 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h3><p>1.解压hadoop-src源码到&#x2F;opt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 atguigu]# tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/</span><br></pre></td></tr></table></figure>

<p>2.进入到hadoop源码主目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 atguigu]<span class="comment"># cd /opt/hadoop-2.7.2-src/</span></span><br><span class="line">[root@hadoop100 hadoop-2.7.2-src]<span class="comment"># pwd</span></span><br><span class="line">/opt/hadoop-2.7.2-src</span><br><span class="line">[root@hadoop100 hadoop-2.7.2-src]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p><strong>3.通过maven执行编译命令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 hadoop-2.7.2-src]# mvn package -Pdist,native -DskipTests -Dtar</span><br></pre></td></tr></table></figure>

<p>如果期间编译失败，再次运行上面命令即可。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201201904327.png" alt="image-20211201201904327"></p>
<p><strong>5.编译源码过程中常见的问题及解决方案</strong></p>
<p>（1）MAVEN install时候JVM内存溢出</p>
<p>处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：<a target="_blank" rel="noopener" href="http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method%EF%BC%89">http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method）</a></p>
<p>（2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,nativeN -DskipTests -Dtar</span><br></pre></td></tr></table></figure>

<p>（3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐</p>
<p>2.7.0版本的问题汇总帖子 <a target="_blank" rel="noopener" href="http://www.tuicool.com/articles/IBn63qf">http://www.tuicool.com/articles/IBn63qf</a></p>
<h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><h2 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h2><h3 id="HDFS概述-1"><a href="#HDFS概述-1" class="headerlink" title="HDFS概述"></a>HDFS概述</h3><h4 id="HDFS产出背景及定义"><a href="#HDFS产出背景及定义" class="headerlink" title="HDFS产出背景及定义"></a>HDFS产出背景及定义</h4><p><strong>1.HDFS产生背景</strong></p>
<p>​	随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切<code>需要一种系统来管理多台机器上的文件</code>，这就是分布式文件管理系统。<code>HDFS只是分布式文件管理系统中的一种</code>。</p>
<p><strong>2.HDFS定义</strong></p>
<p>​	<code>HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件</code>。其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>​	<code>HDFS的使用场景:适合一次写入，多次读出的场景，且不支持文件的修改</code>。适合用来做数据分析，并不适合用来做网盘应用。</p>
<h3 id="HDFS的优缺点"><a href="#HDFS的优缺点" class="headerlink" title="HDFS的优缺点"></a>HDFS的优缺点</h3><p>优点：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201203152388.png" alt="image-20211201203152388"></p>
<p>缺点：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201203250010.png" alt="image-20211201203250010"></p>
<h3 id="HDFS组成架构"><a href="#HDFS组成架构" class="headerlink" title="HDFS组成架构"></a>HDFS组成架构</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201203417179.png" alt="image-20211201203417179"></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201204454920.png" alt="image-20211201204454920"></p>
<h3 id="HDFS文件块大小-重点"><a href="#HDFS文件块大小-重点" class="headerlink" title="HDFS文件块大小(重点)"></a>HDFS文件块大小(重点)</h3><p>​	HDFS中的文件在物理上是分块存储(Block )，块的大小可以通过配置参数dfs.blocksize)来规定，<code>默认大小在Hadoop2.x版本中是128M，老版本中是64M</code>。</p>
<p>具体我们可以打开hadoop的hdfs-defalut.xml</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201211016577.png" alt="image-20211201211016577"></p>
<p>HDFS寻址时间：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201211122697.png" alt="image-20211201211122697"></p>
<p>思考：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211201212406493.png" alt="image-20211201212406493"></p>
<h2 id="HDFS的Shell操作-开发重点"><a href="#HDFS的Shell操作-开发重点" class="headerlink" title="HDFS的Shell操作(开发重点)"></a>HDFS的Shell操作(开发重点)</h2><p><strong>1.基本语法</strong></p>
<p>bin&#x2F;hadoop fs 具体命令  OR bin&#x2F;hdfs dfs 具体命令</p>
<p>dfs是fs的实现类。</p>
<p><strong>2.命令大全</strong></p>
<p>我们可以先看看hadoop中hdfs给我们的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 module]$ hdfs dfs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<p>我们看看hadoop给我们的hadoop命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 module]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<p><strong>hdfs和hadoop fs基本上没啥区别</strong></p>
<h3 id="常用命令实操"><a href="#常用命令实操" class="headerlink" title="常用命令实操"></a>常用命令实操</h3><h4 id="启动Hadoop集群-方便后面的测试"><a href="#启动Hadoop集群-方便后面的测试" class="headerlink" title="启动Hadoop集群(方便后面的测试)"></a>启动Hadoop集群(方便后面的测试)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ sbin/start-dfs.sh </span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ sbin/start-yarn.sh </span><br></pre></td></tr></table></figure>



<h4 id="help-输出这个命令的参数"><a href="#help-输出这个命令的参数" class="headerlink" title="help:输出这个命令的参数"></a>help:输出这个命令的参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">help</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202153229710.png" alt="image-20211202153229710"></p>
<h4 id="ls显示目录信息"><a href="#ls显示目录信息" class="headerlink" title="ls显示目录信息"></a>ls显示目录信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示hadoop更目录下的信息</span></span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup  197657687 2021-11-27 16:12 /hadoop-2.7.2.tar.gz</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-11-27 16:09 /user</span><br></pre></td></tr></table></figure>

<p>递归查询目录文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -lsr /</span><br><span class="line">lsr: DEPRECATED: Please use <span class="string">&#x27;ls -R&#x27;</span> instead.</span><br><span class="line">-rw-r--r--   3 atguigu supergroup  197657687 2021-11-27 16:12 /hadoop-2.7.2.tar.gz</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-11-27 16:09 /user</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-11-27 16:09 /user/atguigu</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-11-27 16:09 /user/atguigu/input</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         46 2021-11-27 16:09 /user/atguigu/input/wc.input</span><br></pre></td></tr></table></figure>

<h4 id="mkdir-在HDFS上创建目录"><a href="#mkdir-在HDFS上创建目录" class="headerlink" title="mkdir:在HDFS上创建目录"></a>mkdir:在HDFS上创建目录</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mkdir</span> -p /sanguo/shuguo</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> /</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup  197657687 2021-11-27 16:12 /hadoop-2.7.2.tar.gz</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-12-02 15:49 /sanguo</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-11-27 16:09 /user</span><br></pre></td></tr></table></figure>

<h4 id="moveFromLocal-从本地剪切粘贴到HDFS"><a href="#moveFromLocal-从本地剪切粘贴到HDFS" class="headerlink" title="moveFromLocal 从本地剪切粘贴到HDFS"></a>moveFromLocal 从本地剪切粘贴到HDFS</h4><p>我们在本地新建一个txt文件夹，随便追加一点内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">touch</span> panjinlian.txt</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ vim panjinlian.txt </span><br></pre></td></tr></table></figure>

<p>我们将panjinlian.txt文件夹移到hadoop目录的shuguo文件夹下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./panjinlian.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>

<p>我们查看最终效果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> /sanguo/shuguo</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         18 2021-12-02 15:54 /sanguo/shuguo/panjinlian.txt</span><br></pre></td></tr></table></figure>

<h4 id="appendToFile-追加一个文件到已经存在的文件末尾"><a href="#appendToFile-追加一个文件到已经存在的文件末尾" class="headerlink" title="appendToFile:追加一个文件到已经存在的文件末尾"></a>appendToFile:追加一个文件到已经存在的文件末尾</h4><p>需求：我们将一个txt文件内容追加到hadoop文件中panjinlian文件中。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">touch</span> liubei.txt   </span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ vim liubei.txt </span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -appendToFile ./liubei.txt /sanguo/shuguo/panjinlian.txt</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">cat</span> /sanguo/shuguo/panjinlian.txt</span><br><span class="line">wo shi panjinlian</span><br><span class="line">wo shi liubei`</span><br></pre></td></tr></table></figure>

<h4 id="chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"><a href="#chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限" class="headerlink" title="chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"></a>chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</h4><p>需求：将panjinlian.txt文件的组名修改为atguigu</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">chgrp</span> atguigu /sanguo/shuguo/panjinlian.txt</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> /sanguo/shuguo/panjinlian.txt</span><br><span class="line">-rw-r--r--   3 atguigu atguigu         33 2021-12-02 16:03 /sanguo/shuguo/panjinlian.txt</span><br></pre></td></tr></table></figure>

<h4 id="copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"><a href="#copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去" class="headerlink" title="copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"></a>copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</h4><p>新建一个ximenqing.txt文件夹，把他放在和panjinlian.txt文件夹同级目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -copyFromLocal ./ximenqing.txt /sanguo/shuguo/</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> /sanguo/shuguo/</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 atguigu atguigu            33 2021-12-02 16:03 /sanguo/shuguo/panjinlian.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         17 2021-12-02 16:12 /sanguo/shuguo/ximenqing.txt</span><br></pre></td></tr></table></figure>

<h4 id="copyToLocal：从HDFS拷贝到本地"><a href="#copyToLocal：从HDFS拷贝到本地" class="headerlink" title="copyToLocal：从HDFS拷贝到本地"></a>copyToLocal：从HDFS拷贝到本地</h4><p>需求：把panjinlian.txt拷贝到本地文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/panjinlian.txt ./</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202161730434.png" alt="image-20211202161730434"></p>
<h4 id="cp-：从HDFS的一个路径拷贝到HDFS的另一个路径"><a href="#cp-：从HDFS的一个路径拷贝到HDFS的另一个路径" class="headerlink" title="cp ：从HDFS的一个路径拷贝到HDFS的另一个路径"></a>cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</h4><p>需求：把panjinlian.txt拷贝到&#x2F;sanguo目录下面</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/panjinlian.txt /sanguo/</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202161930758.png" alt="image-20211202161930758"></p>
<h4 id="mv：在HDFS目录中移动文件"><a href="#mv：在HDFS目录中移动文件" class="headerlink" title="mv：在HDFS目录中移动文件"></a>mv：在HDFS目录中移动文件</h4><p>需求：把&#x2F;sanguo下的panjinlian.txt移动到hadoop根目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mv</span> /sanguo/panjinlian.txt /</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202162134299.png" alt="image-20211202162134299"></p>
<h4 id="get：等同于copyToLocal，就是从HDFS下载文件到本地"><a href="#get：等同于copyToLocal，就是从HDFS下载文件到本地" class="headerlink" title="get：等同于copyToLocal，就是从HDFS下载文件到本地"></a>get：等同于copyToLocal，就是从HDFS下载文件到本地</h4><p>需求：我们需要将panjinglian.txt文件下载到本地目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -get /panjinlian.txt ./</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202162822070.png" alt="image-20211202162822070"></p>
<h4 id="getmerge：合并下载多个文件，比如HDFS的目录-x2F-user-x2F-atguigu-x2F-test下有多个文件-log-1-log-2-log-3-…"><a href="#getmerge：合并下载多个文件，比如HDFS的目录-x2F-user-x2F-atguigu-x2F-test下有多个文件-log-1-log-2-log-3-…" class="headerlink" title="getmerge：合并下载多个文件，比如HDFS的目录 &#x2F;user&#x2F;atguigu&#x2F;test下有多个文件:log.1, log.2,log.3,…"></a>getmerge：合并下载多个文件，比如HDFS的目录 &#x2F;user&#x2F;atguigu&#x2F;test下有多个文件:log.1, log.2,log.3,…</h4><p>需求：我要将&#x2F;sanguo&#x2F;shuguo下的ximenqing.txt和panjinlian.txt文件合并在一起</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -getmerge /sanguo/shuguo/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202163419278.png" alt="image-20211202163419278"></p>
<h4 id="put：等同于copyFromLocal"><a href="#put：等同于copyFromLocal" class="headerlink" title="put：等同于copyFromLocal"></a>put：等同于copyFromLocal</h4><p>需求：我们将LICENSE.txt文件上传到&#x2F;sanguo&#x2F;shuguo&#x2F;文件下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -put ./LICENSE.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202163802816.png" alt="image-20211202163802816"></p>
<h4 id="tail：显示一个文件的末尾"><a href="#tail：显示一个文件的末尾" class="headerlink" title="tail：显示一个文件的末尾"></a>tail：显示一个文件的末尾</h4><p>需求：我们显示LICENSE文件的末尾</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">tail</span> /sanguo/shuguo/LICENSE.txt</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202163951681.png" alt="image-20211202163951681"></p>
<h4 id="rm：删除文件或文件夹"><a href="#rm：删除文件或文件夹" class="headerlink" title="rm：删除文件或文件夹"></a>rm：删除文件或文件夹</h4><p>需求：我们需要将LICENSE删掉</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -<span class="built_in">rm</span> /sanguo/shuguo/LICENSE.txt</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202164142933.png" alt="image-20211202164142933"></p>
<h4 id="rmdir：删除空目录"><a href="#rmdir：删除空目录" class="headerlink" title="rmdir：删除空目录"></a>rmdir：删除空目录</h4><h4 id="du统计文件夹的大小信息"><a href="#du统计文件夹的大小信息" class="headerlink" title="du统计文件夹的大小信息"></a>du统计文件夹的大小信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -du /</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202164300644.png" alt="image-20211202164300644"></p>
<p>将hadoop文件的大小数据进行统计</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -du -h /</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202164335951.png" alt="image-20211202164335951"></p>
<p>查看hadoop总文件的大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -du -h -s /</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202164352189.png" alt="image-20211202164352189"></p>
<h3 id="setrep：设置HDFS中文件的副本数量"><a href="#setrep：设置HDFS中文件的副本数量" class="headerlink" title="setrep：设置HDFS中文件的副本数量"></a>setrep：设置HDFS中文件的副本数量</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202164637732.png" alt="image-20211202164637732"></p>
<p>需求：给panjinlian.txt设备2个副本数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -setrep 2 /panjinlian.txt</span><br><span class="line">Replication 2 <span class="built_in">set</span>: /panjinlian.txt</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202165943382.png" alt="image-20211202165943382"></p>
<p>​	这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<h2 id="HDFS客户端操作-开发重点"><a href="#HDFS客户端操作-开发重点" class="headerlink" title="HDFS客户端操作(开发重点)"></a>HDFS客户端操作(开发重点)</h2><h3 id="HDFS客户端环境准备"><a href="#HDFS客户端环境准备" class="headerlink" title="HDFS客户端环境准备"></a>HDFS客户端环境准备</h3><p>1．根据自己电脑的操作系统拷贝对应的编译后的hadoop jar包到非中文路径（例如：D:\Develop\hadoop-2.7.2），如图3-4所示。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202170711323.png" alt="image-20211202170711323"></p>
<p>2．配置HADOOP_HOME环境变量</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202170907512.png" alt="image-20211202170907512"></p>
<ol start="3">
<li>配置Path环境变量</li>
</ol>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202171032600.png" alt="image-20211202171032600"></p>
<p>4．创建一个Maven工程HdfsClientDemo</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202171859434.png" alt="image-20211202171859434"></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202172043256.png" alt="image-20211202172043256"></p>
<p>填写创建目录信息后，既可完成：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202172157306.png" alt="image-20211202172157306"></p>
<p>导入maven依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202172457522.png" alt="image-20211202172457522"></p>
<p>新建一个package包：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202172819415.png" alt="image-20211202172819415"></p>
<p>创建一个java类：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202172914042.png" alt="image-20211202172914042"></p>
<h4 id="开始进行开发"><a href="#开始进行开发" class="headerlink" title="开始进行开发"></a>开始进行开发</h4><p>先导入log日志</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="attr">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<p><strong>创建java文件，写入以下代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HDFSClient</span> &#123;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException&#123;</span><br><span class="line">		<span class="comment">//1.获取hdfs客户端对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="comment">//设置连接地址，我们在这设置链接hadoop集群的链接地址，该地址要配置namenode的服务器地址</span></span><br><span class="line">		conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(conf);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.在hdfs上创建路径</span></span><br><span class="line">		fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shanguo/dashen&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line">		</span><br><span class="line">		System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行的时候，因为目前我们虚拟机用户还是atguigu，所以我们要指定一下：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202204934481.png" alt="image-20211202204934481"></p>
<p>填写指定信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-DHADOOP_USER_NAME=a</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202205256164.png" alt="image-20211202205256164"></p>
<p>运行：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202205404327.png" alt="image-20211202205404327"></p>
<p><strong>验证答案</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202205436727.png" alt="image-20211202205436727"></p>
<p><strong>代码改良，一次运行</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HDFSClient</span> &#123;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException&#123;</span><br><span class="line">		<span class="comment">//1.获取hdfs客户端对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="comment">//设置连接地址，我们在这设置链接hadoop集群的链接地址，该地址要配置namenode的服务器地址</span></span><br><span class="line">		<span class="comment">//conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://192.168.17.41:9000&quot;);</span></span><br><span class="line">		</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.在hdfs上创建路径</span></span><br><span class="line">		fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/0521/dada&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line">		</span><br><span class="line">		System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211202205950532.png" alt="image-20211202205950532"></p>
<p>​	客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME&#x3D;atguigu，atguigu为用户名称。</p>
<h3 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h3><h4 id="HDFS文件上传（测试参数优先级）"><a href="#HDFS文件上传（测试参数优先级）" class="headerlink" title="HDFS文件上传（测试参数优先级）"></a>HDFS文件上传（测试参数优先级）</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.文件上传</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//1.获取fs对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fsFileSystem</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.执行操作</span></span><br><span class="line">		fsFileSystem.copyFromLocalFile(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;d:/banzhang.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/banzhang.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.关闭资源</span></span><br><span class="line">		fsFileSystem.close();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>运行截图：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205171525548.png" alt="image-20211205171525548"></p>
<p><strong>将hdfs-site.xml拷贝到项目的根目录下</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205171737715.png" alt="image-20211205171737715"></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上传banhua.txt</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205171852125.png" alt="image-20211205171852125"></p>
<p><strong>更具上面的测试，我们可以得出结论</strong></p>
<p>3．参数优先级</p>
<p>参数优先级排序：</p>
<p>（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置</p>
<h4 id="HDFS文件下载"><a href="#HDFS文件下载" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//2.文件下载</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),configuration,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.执行下载任务</span></span><br><span class="line">		<span class="comment">//fs.copyToLocalFile(new Path(&quot;/banhua.txt&quot;), new Path(&quot;d:/banhua1.txt&quot;));</span></span><br><span class="line">		<span class="comment">//使用下面这种方式可以避免出现src文件</span></span><br><span class="line">		fs.copyToLocalFile(<span class="literal">false</span> , <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/banhua.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;d:/banhua11.txt&quot;</span>) , <span class="literal">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205175811790.png" alt="image-20211205175811790"></p>
<h4 id="文件删除"><a href="#文件删除" class="headerlink" title="文件删除"></a>文件删除</h4><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205180317690.png" alt="image-20211205180317690"></p>
<p>代码运行后：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205180340571.png" alt="image-20211205180340571"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//3.文件删除</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fSystem</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),configuration,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.文件的删除</span></span><br><span class="line">		fSystem.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/banhua.txt&quot;</span>),<span class="literal">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.关闭资源</span></span><br><span class="line">		fSystem.close();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<h4 id="文件更名"><a href="#文件更名" class="headerlink" title="文件更名"></a>文件更名</h4><p>把banzhang.txt文件名改成banzhang1.txt</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205180754250.png" alt="image-20211205180754250"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//4.文件更名</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRename</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fSystem</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),configuration,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">			</span><br><span class="line">		<span class="comment">//2.文件的更名</span></span><br><span class="line">		fSystem.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/banzhang.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/banzhnag1.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.关闭资源</span></span><br><span class="line">		fSystem.close();</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>



<h4 id="HDFS文件详情查看"><a href="#HDFS文件详情查看" class="headerlink" title="HDFS文件详情查看"></a>HDFS文件详情查看</h4><p>查看文件名称、权限、长度、块信息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//5.文件详情查看</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testListFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),configuration,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//查看文件详情</span></span><br><span class="line">		RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">			<span class="type">LocatedFileStatus</span> <span class="variable">fileStatus</span> <span class="operator">=</span> listFiles.next();</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//查看文件名称、权限、长度、快信息</span></span><br><span class="line">			System.out.println(fileStatus.getPath().getName());</span><br><span class="line">			System.out.println(fileStatus.getPermission());</span><br><span class="line">			System.out.println(fileStatus.getLen());</span><br><span class="line">			</span><br><span class="line">			BlockLocation[] bLocations = fileStatus.getBlockLocations();</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">for</span>(BlockLocation boLocation : bLocations) &#123;</span><br><span class="line">				String[] hosts = boLocation.getHosts();</span><br><span class="line">				</span><br><span class="line">				<span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">					System.out.println(host);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">			System.out.println(<span class="string">&quot;-----------------------&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205185939106.png" alt="image-20211205185939106"></p>
<h4 id="文件和文件夹的判断"><a href="#文件和文件夹的判断" class="headerlink" title="文件和文件夹的判断"></a>文件和文件夹的判断</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//6.判断是文件还是文件夹</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),configuration,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//判断操作</span></span><br><span class="line">		FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line">			<span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">				<span class="comment">//文件</span></span><br><span class="line">				System.out.println(<span class="string">&quot;f:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">			&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="comment">//文件夹</span></span><br><span class="line">				System.out.println(<span class="string">&quot;d:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205191015800.png" alt="image-20211205191015800"></p>
<h3 id="HDFS的I-x2F-O流操作"><a href="#HDFS的I-x2F-O流操作" class="headerlink" title="HDFS的I&#x2F;O流操作"></a>HDFS的I&#x2F;O流操作</h3><p>​	上面我们学的API操作HDFS系统都是框架封装好的。那么如果我们想自己实现上述API的操作该怎么实现呢？</p>
<p>​	我们可以采用IO流的方式实现数据的上传和下载。</p>
<h4 id="HDFS文件上传"><a href="#HDFS文件上传" class="headerlink" title="HDFS文件上传"></a>HDFS文件上传</h4><p>1.需求：把本地d盘上的banhua.txt文件上传和HDFS根目录。</p>
<p>2.编写代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//把本地d盘的banhua.txt文件上传到HDFS根目录</span></span><br><span class="line">	<span class="meta">@SuppressWarnings(&quot;resource&quot;)</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">putFileToHDFS</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.获取输入流</span></span><br><span class="line">		<span class="type">FileInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;d:/banzhang.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.获取输出流</span></span><br><span class="line">		<span class="type">FSDataOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/banzhang.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//4.流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(fis, fos, conf);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.关闭资源</span></span><br><span class="line">		IOUtils.closeStream(fos);</span><br><span class="line">		IOUtils.closeStream(fis);</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205215633171.png" alt="image-20211205215633171"></p>
<h4 id="HDFS文件下载-1"><a href="#HDFS文件下载-1" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h4><p>1.需求：从HDFS上下载banhua.txt文件到本地e盘上</p>
<p>2.编写代码</p>
<p>下载第一块</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下载第一块</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.获取输入流</span></span><br><span class="line">		<span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.获取输出流</span></span><br><span class="line">		<span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;d:/hadoop-2.7.2.tar.gz.part1&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//4.流的对转</span></span><br><span class="line">		<span class="type">byte</span>[] buf = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt;<span class="number">1024</span>*<span class="number">128</span>; i++) &#123;</span><br><span class="line">			fis.read(buf);</span><br><span class="line">			fos.write(buf);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.关闭</span></span><br><span class="line">		IOUtils.closeStream(fos);</span><br><span class="line">		IOUtils.closeStream(fis);</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//下载第二块</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.获取输入流</span></span><br><span class="line">		<span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.设置指定读取的七点</span></span><br><span class="line">		fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//4.获取输出流</span></span><br><span class="line">		<span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;d:/hadoop-2.7.2.tar.gz.part2&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.流的对拷</span></span><br><span class="line">		<span class="type">byte</span>[] buf = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt;<span class="number">1024</span>*<span class="number">128</span>; i++) &#123;</span><br><span class="line">			fis.read(buf);</span><br><span class="line">			fos.write(buf);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.关闭</span></span><br><span class="line">		IOUtils.closeStream(fos);</span><br><span class="line">		IOUtils.closeStream(fis);</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定位文件下载"><a href="#定位文件下载" class="headerlink" title="定位文件下载"></a>定位文件下载</h4><p>1．需求：分块读取HDFS上的大文件，比如根目录下的&#x2F;hadoop-2.7.2.tar.gz</p>
<p>2．编写代码</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211205222213098.png" alt="image-20211205222213098"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下载第一块</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.获取输入流</span></span><br><span class="line">		<span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.获取输出流</span></span><br><span class="line">		<span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;d:/hadoop-2.7.2.tar.gz.part1&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//4.流的对转</span></span><br><span class="line">		<span class="type">byte</span>[] buf = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt;<span class="number">1024</span>*<span class="number">128</span>; i++) &#123;</span><br><span class="line">			fis.read(buf);</span><br><span class="line">			fos.write(buf);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.关闭</span></span><br><span class="line">		IOUtils.closeStream(fos);</span><br><span class="line">		IOUtils.closeStream(fis);</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//下载第二块</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">		<span class="comment">//1.获取对象</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.17.41:9000&quot;</span>),conf,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//2.获取输入流</span></span><br><span class="line">		<span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//3.设置指定读取的七点</span></span><br><span class="line">		fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//4.获取输出流</span></span><br><span class="line">		<span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;d:/hadoop-2.7.2.tar.gz.part2&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(fis, fos, conf);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//5.关闭</span></span><br><span class="line">		IOUtils.closeStream(fos);</span><br><span class="line">		IOUtils.closeStream(fis);</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>（3）合并文件</p>
<p>在Window命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并</p>
<p>type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</p>
<p>合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。</p>
<h2 id="HDFS的数据流-面试重点）"><a href="#HDFS的数据流-面试重点）" class="headerlink" title="HDFS的数据流(面试重点）"></a>HDFS的数据流(面试重点）</h2><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><h4 id="刨析文件写入"><a href="#刨析文件写入" class="headerlink" title="刨析文件写入"></a>刨析文件写入</h4><p>HDFS写数据流程，如下图所示：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211123546845.png" alt="image-20211211123546845"></p>
<p>1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</p>
<p>2）NameNode返回是否可以上传。</p>
<p>3）客户端请求第一个 Block上传到哪几个DataNode服务器上。</p>
<p>4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</p>
<p>5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</p>
<p>6）dn1、dn2、dn3逐级应答客户端。</p>
<p>7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</p>
<p>8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</p>
<h4 id="网路拓扑-节点距离计算"><a href="#网路拓扑-节点距离计算" class="headerlink" title="网路拓扑-节点距离计算"></a>网路拓扑-节点距离计算</h4><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211124853289.png" alt="image-20211211124853289"></p>
<p>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为&#x2F;d1&#x2F;r1&#x2F;n1。利用这种标记，这里给出四种距离描述，如图3-9所示。</p>
<p>大家算一算每两个节点之间的距离，如图3-10所示。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211125136333.png" alt="image-20211211125136333"></p>
<h4 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h4><p><strong>1.官方ip地址</strong></p>
<p>机架感知说明</p>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</a></p>
<p><strong>此处描述的当前默认副本放置策略正在执行中</strong></p>
<p>副本的放置对于HDFS的可靠性和性能至关重要。优化副本放置使HDFS区别于大多数其他分布式文件系统。这是一个需要大量调整和经验的功能。机架感知副本放置策略的目的是提高数据可靠性、可用性和网络带宽利用率。副本放置策略的当前实现是这方面的第一项工作。实施这一政策的短期目标是在生产系统上验证它，了解它的行为，并为测试和研究更复杂的政策打下基础。</p>
<p>大型HDFS实例运行在通常分布在多个机架上的计算机集群上。不同机架中的两个节点之间的通信必须通过交换机进行。在大多数情况下，同一机架中机器之间的网络带宽大于不同机架中机器之间的网络带宽。</p>
<p>NameNode通过Hadoop rack Aware中概述的过程确定每个DataNode所属的机架id。一种简单但非最佳的策略是将副本放置在唯一的机架上。这样可以防止在整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在群集中均匀分布副本，这使得在组件出现故障时可以轻松平衡负载。但是，此策略会增加写入成本，因为写入需要将块传输到多个机架。</p>
<p>对于常见情况，当复制系数为3时，HDFS的放置策略是将一个副本放置在本地机架的一个节点上，另一个副本放置在本地机架的另一个节点上，最后一个副本放置在不同机架的另一个节点上。此策略减少机架间写入通信量，这通常会提高写入性能。机架故障的概率远小于节点故障的概率；此策略不会影响数据可靠性和可用性保证。但是，它确实减少了读取数据时使用的总网络带宽，因为一个块只放在两个而不是三个唯一的机架中。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本位于一个节点上，三分之二的副本位于一个机架上，另三分之一均匀分布在其余机架上。此策略在不影响数据可靠性或读取性能的情况下提高了写入性能。</p>
<p><strong>2.Hadoop2.7.2副本节点选择</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211125714326.png" alt="image-20211211125714326"></p>
<h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p>HDFS的读数据流程，如图3-13所示。<strong>读取顺序是顺序读，不会并发读</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211125842214.png" alt="image-20211211125842214"></p>
<p>1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</p>
<p>2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</p>
<p>3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</p>
<p>4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<h2 id="NameNode和SecondayNameNode-面试开发"><a href="#NameNode和SecondayNameNode-面试开发" class="headerlink" title="NameNode和SecondayNameNode(面试开发)"></a>NameNode和SecondayNameNode(面试开发)</h2><h3 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h3><p><strong>思考：NameNode中的元数据是存储在哪里的？</strong></p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。<strong>因此产生在磁盘中备份元数据的FsImage</strong>。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，<strong>引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中</strong>。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，<strong>引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并</strong>。</p>
<p>NN和2NN工作机制，如图3-14所示：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211130815780.png" alt="image-20211211130815780"></p>
<ol>
<li><strong>第一阶段：NameNode启动</strong>（黑色的字）</li>
</ol>
<p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存，<strong>编辑日志+镜像文件&#x3D;元数据</strong>。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode记录操作日志，更新滚动日志。</p>
<p>（4）NameNode在内存中对数据进行增删改。</p>
<ol start="2">
<li><strong>第二阶段：Secondary NameNode工作</strong>（紫色的字）检查条件：满足1小时和Edit中有100万条</li>
</ol>
<p>​    （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</p>
<p>​    （2）Secondary NameNode请求执行CheckPoint。</p>
<p>​	（3）NameNode滚动正在写的Edits日志。</p>
<p>​    （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p>
<p>​    （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p>
<p>​    （6）生成新的镜像文件fsimage.chkpoint。</p>
<p>​    （7）拷贝fsimage.chkpoint到NameNode。</p>
<p>​    （8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<p>  <strong>NN和2NN工作机制详解</strong>  </p>
<p>Fsimage：NameNode内存中元数据序列化后形成的文件。  </p>
<p>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。  </p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。  </p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。  </p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。</p>
<p>NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。  </p>
<h3 id="Fsimage和Edits详解"><a href="#Fsimage和Edits详解" class="headerlink" title="Fsimage和Edits详解"></a>Fsimage和Edits详解</h3><p><strong>1.概念</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211161012709.png" alt="image-20211211161012709"></p>
<p><strong>2.oiv查看Fsimage文件</strong></p>
<p>1.查看oiv和oev命令</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211161134292.png" alt="image-20211211161134292"></p>
<p>2.基本语法</p>
<p>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p>
<p>2.实操</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 current]$ <span class="built_in">pwd</span></span><br><span class="line">[atguigu@hadoop100 current]$ hdfs oiv -p XML -i fsimage_0000000000000000000 -o /opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/fsimage.xml</span><br><span class="line"><span class="comment">#把生成的fsimage.xml上传到hdfs中</span></span><br><span class="line">[atguigu@hadoop100 BP-1593717789-192.168.17.41-1639208147086]$ hadoop fs -moveFromLocal fsimage.xml /</span><br></pre></td></tr></table></figure>

<p>下载该文件：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211163041036.png" alt="image-20211211163041036"></p>
<p>将显示的xml文件内容拷贝到Eclipse中创建的xml文件中，并格式化。部分显示结果如下。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fsimage</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">NameSection</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">genstampV1</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV1</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">genstampV2</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV2</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">genstampV1Limit</span>&gt;</span>0<span class="tag">&lt;/<span class="name">genstampV1Limit</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">lastAllocatedBlockId</span>&gt;</span>1073741824<span class="tag">&lt;/<span class="name">lastAllocatedBlockId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">txid</span>&gt;</span>0<span class="tag">&lt;/<span class="name">txid</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">NameSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">lastInodeId</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">lastInodeId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">id</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">name</span>&gt;</span><span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>0<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">INodeReferenceSection</span>&gt;</span><span class="tag">&lt;/<span class="name">INodeReferenceSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">snapshotCounter</span>&gt;</span>0<span class="tag">&lt;/<span class="name">snapshotCounter</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">INodeDirectorySection</span>&gt;</span><span class="tag">&lt;/<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">FileUnderConstructionSection</span>&gt;</span><span class="tag">&lt;/<span class="name">FileUnderConstructionSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">SnapshotDiffSection</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">diff</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">inodeid</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">inodeid</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">diff</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">SnapshotDiffSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">currentId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">currentId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">tokenSequenceNumber</span>&gt;</span>0<span class="tag">&lt;/<span class="name">tokenSequenceNumber</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">nextDirectiveId</span>&gt;</span>1<span class="tag">&lt;/<span class="name">nextDirectiveId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">fsimage</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？</p>
<p>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
<p><strong>3.oev查看Edits文件</strong></p>
<p>（1）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">cat</span> /opt/module/hadoop-2.7.2/edits.xml</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h3><p>（1）通常情况下，SecondaryNameNode每隔一小时执行一次。</p>
<p><strong>[hdfs-default.xml]</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）一分钟检查一次操作次数，如果当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<p><strong>[hdfs-default.xml]</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure>

<h3 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。</p>
<p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</strong></p>
<p>\1. kill -9 NameNode进程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ <span class="built_in">kill</span> -9 1504</span><br></pre></td></tr></table></figure>

<p>\2. 删除NameNode存储的数据（&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line">[atguigu@hadoop100 dfs]$ <span class="built_in">rm</span> -rf name/*</span><br></pre></td></tr></table></figure>

<p>\3. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/current</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop100 name]$  scp -r atguigu@hadoop102:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./</span><br><span class="line">fsimage_0000000000000000000.md5                                                                               100%   62    64.1KB/s   00:00    </span><br><span class="line">fsimage_0000000000000000000                                                                                   100%  354   322.0KB/s   00:00    </span><br><span class="line">edits_0000000000000000001-0000000000000000020                                                                 100% 1516   541.7KB/s   00:00    </span><br><span class="line">fsimage_0000000000000000020.md5                                                                               100%   62    60.4KB/s   00:00    </span><br><span class="line">fsimage_0000000000000000020                                                                                   100%  654   225.7KB/s   00:00    </span><br><span class="line">VERSION                                                                                                       100%  205   151.8KB/s   00:00    </span><br><span class="line">in_use.lock                                                                                                   100%   14     7.4KB/s   00:00    </span><br><span class="line">[atguigu@hadoop100 name]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/name</span><br><span class="line">[atguigu@hadoop100 name]$ </span><br></pre></td></tr></table></figure>

<p>\4. 重新启动NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>成功恢复：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211164918588.png" alt="image-20211211164918588"></p>
<p><strong>方法二：使用-import Checkpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p>
<p>\1.   修改hdfs-site.xml中的</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>\2. kill -9 NameNode进程</p>
<p>\3.  删除NameNode存储的数据（&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>

<p>\4.  如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line">[atguigu@hadoop102 namesecondary]$ <span class="built_in">rm</span> -rf in_use.lock</span><br><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">ls</span></span><br><span class="line">data name namesecondary</span><br></pre></td></tr></table></figure>

<p>\5.  导入检查点数据（等待一会ctrl+c结束掉）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure>

<p>6.  启动NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>



<h3 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211165312832.png" alt="image-20211211165312832"></p>
<ol>
<li>基本语法</li>
</ol>
<p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<p>（1）bin&#x2F;hdfs dfsadmin -safemode get     （功能描述：查看安全模式状态）</p>
<p>（2）bin&#x2F;hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）</p>
<p>（3）bin&#x2F;hdfs dfsadmin -safemode leave   （功能描述：离开安全模式状态）</p>
<p>（4）bin&#x2F;hdfs dfsadmin -safemode wait    （功能描述：等待安全模式状态）</p>
<ol start="2">
<li>案例</li>
</ol>
<p>​    模拟等待安全模式</p>
<p>（1）查看当前模式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure>

<p>（2）先进入安全模式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter </span><br><span class="line">Safe mode is ON</span><br></pre></td></tr></table></figure>

<p><strong>在安全模式下，上传文件是不允许的</strong></p>
<p>（3）创建并执行下面的脚本（先让hadoop进入安全模式）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line">hdfs dfs -put /opt/module/hadoop-2.7.2/README.txt /</span><br></pre></td></tr></table></figure>

<p>（4）再打开一个窗口，执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfsadmin -safemode leave </span><br></pre></td></tr></table></figure>

<p>（5）观察</p>
<p>​	（a）再观察上一个窗口</p>
<p>​	（b）HDFS集群上已经有上传的数据了。</p>
<h3 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h3><ol>
<li><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，<strong>增加了可靠性</strong></p>
</li>
<li><p>具体配置如下</p>
</li>
</ol>
<p>​    （1）在hdfs-site.xml文件中增加如下内容(记得分发配置文件)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）停止集群，删除data和logs中所有数据。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data/ logs/</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data/ logs/</span><br><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data/ logs/</span><br></pre></td></tr></table></figure>

<p>（3）格式化集群并启动。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>（4）查看结果</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure>



<h2 id="DataNode-面试开发重点"><a href="#DataNode-面试开发重点" class="headerlink" title="DataNode(面试开发重点)"></a>DataNode(面试开发重点)</h2><h3 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h3><p>DataNode工作机制，如图3-15所示。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211174055598.png" alt="image-20211211174055598"></p>
<p>1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
<p>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</p>
<p>3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</p>
<p>4）集群运行中可以安全加入和退出一些机器。</p>
<h3 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h3><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是DataNode节点保证数据完整性的方法。</p>
<p>1）当DataNode读取Block的时候，它会计算CheckSum。</p>
<p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p>
<p>3）Client读取其他DataNode上的Block。</p>
<p>4）DataNode在其文件创建后周期验证CheckSum，如图3-16所示。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211175942966.png" alt="image-20211211175942966"></p>
<h3 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211180043881.png" alt="image-20211211180043881"></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="服役新数据节点"><a href="#服役新数据节点" class="headerlink" title="服役新数据节点"></a>服役新数据节点</h3><p>0.需求：</p>
<p>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。</p>
<ol>
<li>环境准备</li>
</ol>
<p>​    （1）在hadoop100主机上再克隆一台hadoop103主机</p>
<p>​    （2）修改IP地址和主机名称</p>
<p>​    （3）<strong>删除原来<strong><strong>HDFS</strong></strong>文件系统留存的文件（&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data<strong><strong>和log</strong></strong>）</strong></p>
<p>​    （4）source一下配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211195820803.png" alt="image-20211211195820803"></p>
<ol start="2">
<li>服役新节点具体步骤</li>
</ol>
<p>（1）直接启动DataNode，即可关联到集群</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 module]$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p>我们查看web可视化页面：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211200150162.png" alt="image-20211211200150162"></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211200202622.png" alt="image-20211211200202622"></p>
<p><strong>发现100和103轮流调，原来是我们刚开是分发的是100的hadoop数据，所以我们格式化hadoop103的数据</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf logs</span><br></pre></td></tr></table></figure>

<p>重新启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p>可以发现四台机器全部都起来了：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211200405413.png" alt="image-20211211200405413"></p>
<p><strong>在hadoop103上传个数据：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ hadoop fs -put /opt/module/hadoop-2.7.2/LICENSE.txt /</span><br></pre></td></tr></table></figure>

<p>上传成功：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211200537343.png" alt="image-20211211200537343"></p>
<p><strong>对于新上的主机，我们需要对他的资源进行一个平衡：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-balancer.sh </span><br></pre></td></tr></table></figure>



<h3 id="退役旧数据节点"><a href="#退役旧数据节点" class="headerlink" title="退役旧数据节点"></a>退役旧数据节点</h3><h4 id="添加白名单"><a href="#添加白名单" class="headerlink" title="添加白名单"></a>添加白名单</h4><p>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</p>
<p>配置白名单的具体步骤如下：</p>
<p>（1）在NameNode的&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop目录下创建dfs.hosts文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#hadoop2.7.2/etc/hadoop</span><br><span class="line">[atguigu@hadoop100 hadoop]$ touch dfs.hosts</span><br><span class="line">[atguigu@hadoop100 hadoop]$ vim dfs.hosts </span><br></pre></td></tr></table></figure>

<p>添加如下主机名称（不添加hadoop105）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop100</span><br><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>

<p>（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop]$ vim hdfs-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211201541771.png" alt="image-20211211201541771"></p>
<p>（3）配置文件分发</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 etc]$ xsync.sh hadoop</span><br></pre></td></tr></table></figure>

<p>（4）刷新NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 etc]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line">[atguigu@hadoop100 etc]$ </span><br></pre></td></tr></table></figure>

<p>Refresh nodes successf</p>
<p>（5）更新ResourceManager节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p>（6）在web浏览器上查看</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211211201956236.png" alt="image-20211211201956236"></p>
<p><strong>如果数据不均衡，可以用命令实现集群的再平衡</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>



<h4 id="黑名单退役"><a href="#黑名单退役" class="headerlink" title="黑名单退役"></a>黑名单退役</h4><p><strong>注意：白名单和黑名单不能同时使用，同时使用会导致集群宕机</strong></p>
<p>在黑名单上面的主机都会被强制退出。</p>
<p>1.在NameNode的&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop目录下创建dfs.hosts.exclude文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ <span class="built_in">touch</span> dfs.hosts.exclude</span><br><span class="line">[atguigu@hadoop102 hadoop]$ vi dfs.hosts.exclude</span><br></pre></td></tr></table></figure>

<p>添加如下主机名称（要退役的节点）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop105</span><br></pre></td></tr></table></figure>

<p>2．在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3．刷新NameNode、刷新ResourceManager</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br><span class="line">17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033</span><br></pre></td></tr></table></figure>

<p>\4.   检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点，如图3-17所示</p>
<p>\5.   等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役，如图3-18所示:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure>

<p>\6.  如果数据不均衡，可以用命令实现集群的再平衡</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh </span><br><span class="line">starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span><br></pre></td></tr></table></figure>

<h3 id="Datanode多目录配置"><a href="#Datanode多目录配置" class="headerlink" title="Datanode多目录配置"></a>Datanode多目录配置</h3><ol>
<li>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li>
<li>具体配置如下（直接复制下面内容即可，然后再分发）</li>
</ol>
<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h2 id="HDFS-2-X新特性"><a href="#HDFS-2-X新特性" class="headerlink" title="HDFS 2.X新特性"></a>HDFS 2.X新特性</h2><h3 id="集群间数据拷贝"><a href="#集群间数据拷贝" class="headerlink" title="集群间数据拷贝"></a>集群间数据拷贝</h3><p>1．scp实现两个远程主机之间的文件复制</p>
<p>​    scp -r hello.txt <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt</a>        &#x2F;&#x2F; 推 push</p>
<p>​    scp -r [root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt hello.txt](mailto:root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt  hello.txt)       &#x2F;&#x2F; 拉 pull</p>
<p>​    scp -r <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt</a> root@hadoop104:&#x2F;user&#x2F;atguigu  &#x2F;&#x2F;是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</p>
<p>2．采用distcp命令实现两个Hadoop集群之间的递归数据复制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp</span><br><span class="line">hdfs://haoop102:9000/user/atguigu/hello.txt </span><br><span class="line">hdfs://hadoop103:9000/user/atguigu/hello.txt</span><br></pre></td></tr></table></figure>



<h3 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211212075423675.png" alt="image-20211212075423675"></p>
<p><strong>3.案例实操</strong></p>
<p>（1）需要启动YARN进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>（2）归档文件</p>
<p>首先创建新的文件夹，存入三个txt文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$  hadoop fs -mkdir -p /usr/atguigu/input</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -put LICENSE.txt /usr/atguigu/input</span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -put README.txt /usr/atguigu/input </span><br><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -put NOTICE.txt /usr/atguigu/input</span><br></pre></td></tr></table></figure>

<p>​    <img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211212081258828.png" alt="image-20211212081258828"></p>
<p>把&#x2F;user&#x2F;atguigu&#x2F;input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到&#x2F;user&#x2F;atguigu&#x2F;output路径下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop archive  -archiveName input.har -p /usr/atguigu/input /usr/atguigu/output</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211212081949453.png" alt="image-20211212081949453"></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211212082011037.png" alt="image-20211212082011037"></p>
<p>3.解除归档文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hadoop-2.7.2]$ hadoop fs -ls -R har:///usr/atguigu/output/input.har</span><br><span class="line">-rw-r--r--   3 atguigu supergroup      15429 2021-12-12 08:11 har:///usr/atguigu/output/input.har/LICENSE.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup        101 2021-12-12 08:11 har:///usr/atguigu/output/input.har/NOTICE.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       1366 2021-12-12 08:11 har:///usr/atguigu/output/input.har/README.txt</span><br></pre></td></tr></table></figure>



<h3 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h3><p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。</p>
<p>1．回收站参数设置及工作机制（这里单位是分钟）</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211212082242097.png" alt="image-20211212082242097"></p>
<p>2．启用回收站</p>
<p>修改core-site.xml，配置垃圾回收时间为1分钟。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3．查看回收站</p>
<p><strong>在执行删除操作后，我们会得到一行提示：可以查看回收站的地址</strong></p>
<p>回收站在集群中的路径：&#x2F;user&#x2F;atguigu&#x2F;.Trash&#x2F;….</p>
<p>4．修改访问垃圾回收站用户名称(需要进行分发)</p>
<p>​    进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户</p>
<p>​    [core-site.xml]</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</li>
</ol>
<p>Trash trash &#x3D; New Trash(conf);</p>
<p>trash.moveToTrash(path)</p>
<ol start="6">
<li>恢复回收站数据</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -mv</span><br><span class="line">/user/atguigu/.Trash/Current/user/atguigu/input  /user/atguigu/input</span><br></pre></td></tr></table></figure>



<h3 id="快照管理"><a href="#快照管理" class="headerlink" title="快照管理"></a>快照管理</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211212083811250.png" alt="image-20211212083811250"></p>
<p>2．案例实操</p>
<p>​    （1）开启&#x2F;禁用指定目录的快照功能</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/atguigu/input</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/atguigu/input</span><br></pre></td></tr></table></figure>

<p>​    （2）对目录创建快照</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input</span><br></pre></td></tr></table></figure>

<p>通过web访问hdfs:&#x2F;&#x2F;hadoop102:50070&#x2F;user&#x2F;atguigu&#x2F;input&#x2F;.snapshot&#x2F;s…..&#x2F;&#x2F; 快照和源文件使用相同数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/atguigu/input/.snapshot/</span><br></pre></td></tr></table></figure>

<p>​    （3）指定名称创建快照</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input miao170508</span><br></pre></td></tr></table></figure>

<p>​    （4）重命名快照</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/atguigu/input/  miao170508 atguigu170508</span><br></pre></td></tr></table></figure>

<p>​    （5）列出当前用户所有可快照目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir</span><br></pre></td></tr></table></figure>

<p>​    （6）比较两个快照目录的不同之处</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff</span><br><span class="line">/user/atguigu/input/  .  .snapshot/atguigu170508	</span><br></pre></td></tr></table></figure>

<p>（7）恢复快照</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp</span><br><span class="line">/user/atguigu/input/.snapshot/s20170708-134303.027 /user</span><br></pre></td></tr></table></figure>



</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">DingQuan Zuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/">http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">ccbigs blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="/img/ahead.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/28/SSM%E2%80%94%E2%80%94Spring/" title="SSM——Spring"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SSM——Spring</div></div></a></div><div class="next-post pull-right"><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/" title="大数据——Hive调优"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据——Hive调优</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94ClickHouse/" title="大数据——ClickHouse"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——ClickHouse</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Flume/" title="大数据——Flume"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Flume</div></div></a></div><div><a href="/2023/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hbase/" title="大数据——Hbase"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">大数据——Hbase</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/" title="大数据——Hive调优"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Hive调优</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/" title="大数据——Hive"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Hive</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Scala/" title="大数据——Scala"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Scala</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/ahead.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">DingQuan Zuo</div><div class="author-info__description">技术路上少不了自我怀疑，往往你的决定，会让你看到不同的风景</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccbigs" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1692062014@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%AE%BA"><span class="toc-number">1.1.</span> <span class="toc-text">大数据概论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.1.</span> <span class="toc-text">大数据概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%89%B9%E7%82%B9-4V"><span class="toc-number">1.1.2.</span> <span class="toc-text">大数据特点(4V)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.1.3.</span> <span class="toc-text">大数据应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8F%91%E5%B1%95%E5%89%8D%E6%99%AF"><span class="toc-number">1.1.4.</span> <span class="toc-text">大数据发展前景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.5.</span> <span class="toc-text">大数据业务流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%83%A8%E9%97%A8%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84-%E9%87%8D%E7%82%B9"><span class="toc-number">1.1.6.</span> <span class="toc-text">大数据部门组织结构(重点)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E8%AE%A8%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81"><span class="toc-number">1.2.</span> <span class="toc-text">从Hadoop框架讨论大数据生态</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.2.1.</span> <span class="toc-text">Hadoop是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="toc-number">1.2.2.</span> <span class="toc-text">Hadoop发展历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E7%9A%84%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC"><span class="toc-number">1.2.3.</span> <span class="toc-text">Hadoop的发行版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E7%9A%84%E4%BC%98%E5%8A%BF%EF%BC%884%E9%AB%98%EF%BC%89"><span class="toc-number">1.2.4.</span> <span class="toc-text">Hadoop的优势（4高）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop1-x%E5%92%8Chadoop2-x%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.2.5.</span> <span class="toc-text">Hadoop1.x和hadoop2.x的区别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-number">1.2.5.1.</span> <span class="toc-text">HDFS架构概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Yarn%E6%9E%B6%E6%9E%84"><span class="toc-number">1.2.5.2.</span> <span class="toc-text">Yarn架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MapReduce%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-number">1.2.5.3.</span> <span class="toc-text">MapReduce架构概述</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB"><span class="toc-number">1.2.6.</span> <span class="toc-text">大数据技术生态体系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%A1%86%E6%9E%B6%E5%9B%BE"><span class="toc-number">1.2.7.</span> <span class="toc-text">推荐系统框架图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">1.3.</span> <span class="toc-text">Hadoop环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.1.</span> <span class="toc-text">准备操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A2%98%EF%BC%9A%E9%85%8D%E7%BD%AE%E7%94%A8%E6%88%B7%E7%AD%89%E4%BF%A1%E6%81%AF"><span class="toc-number">1.3.2.</span> <span class="toc-text">前题：配置用户等信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%B3%BB%E7%BB%9F%E7%94%A8%E6%88%B7"><span class="toc-number">1.3.3.</span> <span class="toc-text">配置系统用户</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%85%8D%E7%BD%AE%E7%9B%AE%E5%BD%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">创建配置目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E5%BA%94%E7%94%A8%E6%96%87%E4%BB%B6"><span class="toc-number">1.3.5.</span> <span class="toc-text">上传应用文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%BE%91%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.3.6.</span> <span class="toc-text">编辑配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E7%9A%84%E7%9B%AE%E5%BD%95"><span class="toc-number">1.3.7.</span> <span class="toc-text">Hadoop的目录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.4.</span> <span class="toc-text">Hadoop运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">本地运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%98%E6%96%B9Grep%E6%A1%88%E4%BE%8B"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">官方Grep案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%98%E6%96%B9WordCount%E6%A1%88%E4%BE%8B"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">官方WordCount案例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E8%BF%90%E8%A1%8C"><span class="toc-number">1.4.2.</span> <span class="toc-text">伪分布式的运行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8HDFS%E5%B9%B6%E8%BF%90%E8%A1%8CMapReduce%E7%A8%8B%E5%BA%8F"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">启动HDFS并运行MapReduce程序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.5.</span> <span class="toc-text">完全分布式运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%87%86%E5%A4%87"><span class="toc-number">1.5.1.</span> <span class="toc-text">虚拟机准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E9%9B%86%E7%BE%A4%E5%88%86%E5%8F%91%E8%84%9A%E6%9C%ACxsync"><span class="toc-number">1.5.2.</span> <span class="toc-text">编写集群分发脚本xsync</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#scp%E5%AE%89%E5%85%A8%E6%8B%B7%E8%B4%9D"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">scp安全拷贝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#xsync%E4%B8%8Ejpsall%E8%84%9A%E6%9C%AC"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">xsync与jpsall脚本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#xsync%E9%9B%86%E7%BE%A4%E5%88%86%E5%8F%91%E8%84%9A%E6%9C%AC"><span class="toc-number">1.5.2.3.</span> <span class="toc-text">xsync集群分发脚本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ssh%E5%AF%86%E9%92%A5%E5%85%8D%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.2.4.</span> <span class="toc-text">ssh密钥免登录配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.2.5.</span> <span class="toc-text">集群配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E5%8D%95%E7%82%B9%E5%90%AF%E5%8A%A8"><span class="toc-number">1.5.2.6.</span> <span class="toc-text">集群单点启动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BE%A4%E8%B5%B7%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.2.7.</span> <span class="toc-text">群起集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8-x2F-%E5%81%9C%E6%AD%A2%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.2.8.</span> <span class="toc-text">集群启动&#x2F;停止方式总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#crond%E7%B3%BB%E7%BB%9F%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.5.2.9.</span> <span class="toc-text">crond系统定时任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="toc-number">1.5.2.10.</span> <span class="toc-text">集群时间同步</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91"><span class="toc-number">1.6.</span> <span class="toc-text">Hadoop源码编译</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.6.1.</span> <span class="toc-text">前期准备工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6"><span class="toc-number">1.6.2.</span> <span class="toc-text">上传文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#jar%E5%8C%85%E5%AE%89%E8%A3%85"><span class="toc-number">1.6.3.</span> <span class="toc-text">jar包安装</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#JDK%E8%A7%A3%E5%8E%8B"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">JDK解压</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Maven%E8%A7%A3%E5%8E%8B"><span class="toc-number">1.6.3.2.</span> <span class="toc-text">Maven解压</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ant%E8%A7%A3%E5%8E%8B"><span class="toc-number">1.6.3.3.</span> <span class="toc-text">ant解压</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85glibc-headers%E5%92%8Cg"><span class="toc-number">1.6.3.4.</span> <span class="toc-text">安装glibc-headers和g++</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85make%E5%92%8Ccmake"><span class="toc-number">1.6.3.5.</span> <span class="toc-text">安装make和cmake</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8Bprotobuf"><span class="toc-number">1.6.3.6.</span> <span class="toc-text">解压protobuf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85openssl"><span class="toc-number">1.6.3.7.</span> <span class="toc-text">安装openssl</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85ncurses"><span class="toc-number">1.6.3.8.</span> <span class="toc-text">安装ncurses</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81"><span class="toc-number">1.6.4.</span> <span class="toc-text">编译源码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS"><span class="toc-number">2.</span> <span class="toc-text">HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">HDFS概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E6%A6%82%E8%BF%B0-1"><span class="toc-number">2.1.1.</span> <span class="toc-text">HDFS概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E4%BA%A7%E5%87%BA%E8%83%8C%E6%99%AF%E5%8F%8A%E5%AE%9A%E4%B9%89"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">HDFS产出背景及定义</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">2.1.2.</span> <span class="toc-text">HDFS的优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84"><span class="toc-number">2.1.3.</span> <span class="toc-text">HDFS组成架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F-%E9%87%8D%E7%82%B9"><span class="toc-number">2.1.4.</span> <span class="toc-text">HDFS文件块大小(重点)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E7%9A%84Shell%E6%93%8D%E4%BD%9C-%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9"><span class="toc-number">2.2.</span> <span class="toc-text">HDFS的Shell操作(开发重点)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%93%8D"><span class="toc-number">2.2.1.</span> <span class="toc-text">常用命令实操</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Hadoop%E9%9B%86%E7%BE%A4-%E6%96%B9%E4%BE%BF%E5%90%8E%E9%9D%A2%E7%9A%84%E6%B5%8B%E8%AF%95"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">启动Hadoop集群(方便后面的测试)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#help-%E8%BE%93%E5%87%BA%E8%BF%99%E4%B8%AA%E5%91%BD%E4%BB%A4%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">help:输出这个命令的参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ls%E6%98%BE%E7%A4%BA%E7%9B%AE%E5%BD%95%E4%BF%A1%E6%81%AF"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">ls显示目录信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mkdir-%E5%9C%A8HDFS%E4%B8%8A%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95"><span class="toc-number">2.2.1.4.</span> <span class="toc-text">mkdir:在HDFS上创建目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#moveFromLocal-%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%89%AA%E5%88%87%E7%B2%98%E8%B4%B4%E5%88%B0HDFS"><span class="toc-number">2.2.1.5.</span> <span class="toc-text">moveFromLocal 从本地剪切粘贴到HDFS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#appendToFile-%E8%BF%BD%E5%8A%A0%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E5%88%B0%E5%B7%B2%E7%BB%8F%E5%AD%98%E5%9C%A8%E7%9A%84%E6%96%87%E4%BB%B6%E6%9C%AB%E5%B0%BE"><span class="toc-number">2.2.1.6.</span> <span class="toc-text">appendToFile:追加一个文件到已经存在的文件末尾</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#chgrp-%E3%80%81-chmod%E3%80%81-chown%EF%BC%9ALinux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E7%94%A8%E6%B3%95%E4%B8%80%E6%A0%B7%EF%BC%8C%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E6%89%80%E5%B1%9E%E6%9D%83%E9%99%90"><span class="toc-number">2.2.1.7.</span> <span class="toc-text">chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#copyFromLocal%EF%BC%9A%E4%BB%8E%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%AD%E6%8B%B7%E8%B4%9D%E6%96%87%E4%BB%B6%E5%88%B0HDFS%E8%B7%AF%E5%BE%84%E5%8E%BB"><span class="toc-number">2.2.1.8.</span> <span class="toc-text">copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#copyToLocal%EF%BC%9A%E4%BB%8EHDFS%E6%8B%B7%E8%B4%9D%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-number">2.2.1.9.</span> <span class="toc-text">copyToLocal：从HDFS拷贝到本地</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cp-%EF%BC%9A%E4%BB%8EHDFS%E7%9A%84%E4%B8%80%E4%B8%AA%E8%B7%AF%E5%BE%84%E6%8B%B7%E8%B4%9D%E5%88%B0HDFS%E7%9A%84%E5%8F%A6%E4%B8%80%E4%B8%AA%E8%B7%AF%E5%BE%84"><span class="toc-number">2.2.1.10.</span> <span class="toc-text">cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mv%EF%BC%9A%E5%9C%A8HDFS%E7%9B%AE%E5%BD%95%E4%B8%AD%E7%A7%BB%E5%8A%A8%E6%96%87%E4%BB%B6"><span class="toc-number">2.2.1.11.</span> <span class="toc-text">mv：在HDFS目录中移动文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get%EF%BC%9A%E7%AD%89%E5%90%8C%E4%BA%8EcopyToLocal%EF%BC%8C%E5%B0%B1%E6%98%AF%E4%BB%8EHDFS%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-number">2.2.1.12.</span> <span class="toc-text">get：等同于copyToLocal，就是从HDFS下载文件到本地</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#getmerge%EF%BC%9A%E5%90%88%E5%B9%B6%E4%B8%8B%E8%BD%BD%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%EF%BC%8C%E6%AF%94%E5%A6%82HDFS%E7%9A%84%E7%9B%AE%E5%BD%95-x2F-user-x2F-atguigu-x2F-test%E4%B8%8B%E6%9C%89%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6-log-1-log-2-log-3-%E2%80%A6"><span class="toc-number">2.2.1.13.</span> <span class="toc-text">getmerge：合并下载多个文件，比如HDFS的目录 &#x2F;user&#x2F;atguigu&#x2F;test下有多个文件:log.1, log.2,log.3,…</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#put%EF%BC%9A%E7%AD%89%E5%90%8C%E4%BA%8EcopyFromLocal"><span class="toc-number">2.2.1.14.</span> <span class="toc-text">put：等同于copyFromLocal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tail%EF%BC%9A%E6%98%BE%E7%A4%BA%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E6%9C%AB%E5%B0%BE"><span class="toc-number">2.2.1.15.</span> <span class="toc-text">tail：显示一个文件的末尾</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rm%EF%BC%9A%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E6%88%96%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">2.2.1.16.</span> <span class="toc-text">rm：删除文件或文件夹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rmdir%EF%BC%9A%E5%88%A0%E9%99%A4%E7%A9%BA%E7%9B%AE%E5%BD%95"><span class="toc-number">2.2.1.17.</span> <span class="toc-text">rmdir：删除空目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#du%E7%BB%9F%E8%AE%A1%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E5%A4%A7%E5%B0%8F%E4%BF%A1%E6%81%AF"><span class="toc-number">2.2.1.18.</span> <span class="toc-text">du统计文件夹的大小信息</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#setrep%EF%BC%9A%E8%AE%BE%E7%BD%AEHDFS%E4%B8%AD%E6%96%87%E4%BB%B6%E7%9A%84%E5%89%AF%E6%9C%AC%E6%95%B0%E9%87%8F"><span class="toc-number">2.2.2.</span> <span class="toc-text">setrep：设置HDFS中文件的副本数量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C-%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9"><span class="toc-number">2.3.</span> <span class="toc-text">HDFS客户端操作(开发重点)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">2.3.1.</span> <span class="toc-text">HDFS客户端环境准备</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%BF%9B%E8%A1%8C%E5%BC%80%E5%8F%91"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">开始进行开发</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%9A%84API%E6%93%8D%E4%BD%9C"><span class="toc-number">2.3.2.</span> <span class="toc-text">HDFS的API操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%EF%BC%88%E6%B5%8B%E8%AF%95%E5%8F%82%E6%95%B0%E4%BC%98%E5%85%88%E7%BA%A7%EF%BC%89"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">HDFS文件上传（测试参数优先级）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">HDFS文件下载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%88%A0%E9%99%A4"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">文件删除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%9B%B4%E5%90%8D"><span class="toc-number">2.3.2.4.</span> <span class="toc-text">文件更名</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E8%AF%A6%E6%83%85%E6%9F%A5%E7%9C%8B"><span class="toc-number">2.3.2.5.</span> <span class="toc-text">HDFS文件详情查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E5%88%A4%E6%96%AD"><span class="toc-number">2.3.2.6.</span> <span class="toc-text">文件和文件夹的判断</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%9A%84I-x2F-O%E6%B5%81%E6%93%8D%E4%BD%9C"><span class="toc-number">2.3.3.</span> <span class="toc-text">HDFS的I&#x2F;O流操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0"><span class="toc-number">2.3.3.1.</span> <span class="toc-text">HDFS文件上传</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD-1"><span class="toc-number">2.3.3.2.</span> <span class="toc-text">HDFS文件下载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%BD%8D%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="toc-number">2.3.3.3.</span> <span class="toc-text">定位文件下载</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81-%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.4.</span> <span class="toc-text">HDFS的数据流(面试重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.1.</span> <span class="toc-text">HDFS写数据流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A8%E6%9E%90%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">刨析文件写入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E8%B7%AF%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">网路拓扑-节点距离计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">机架感知</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.2.</span> <span class="toc-text">HDFS读数据流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NameNode%E5%92%8CSecondayNameNode-%E9%9D%A2%E8%AF%95%E5%BC%80%E5%8F%91"><span class="toc-number">2.5.</span> <span class="toc-text">NameNode和SecondayNameNode(面试开发)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.1.</span> <span class="toc-text">NN和2NN工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fsimage%E5%92%8CEdits%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.5.2.</span> <span class="toc-text">Fsimage和Edits详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CheckPoint%E6%97%B6%E9%97%B4%E8%AE%BE%E7%BD%AE"><span class="toc-number">2.5.3.</span> <span class="toc-text">CheckPoint时间设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NameNode%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-number">2.5.4.</span> <span class="toc-text">NameNode故障处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.5.5.</span> <span class="toc-text">集群安全模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NameNode%E5%A4%9A%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE"><span class="toc-number">2.5.6.</span> <span class="toc-text">NameNode多目录配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataNode-%E9%9D%A2%E8%AF%95%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9"><span class="toc-number">2.6.</span> <span class="toc-text">DataNode(面试开发重点)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.6.1.</span> <span class="toc-text">DataNode工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="toc-number">2.6.2.</span> <span class="toc-text">数据完整性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">2.6.3.</span> <span class="toc-text">掉线时限参数设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%8D%E5%BD%B9%E6%96%B0%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9"><span class="toc-number">2.6.4.</span> <span class="toc-text">服役新数据节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%80%E5%BD%B9%E6%97%A7%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9"><span class="toc-number">2.6.5.</span> <span class="toc-text">退役旧数据节点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E7%99%BD%E5%90%8D%E5%8D%95"><span class="toc-number">2.6.5.1.</span> <span class="toc-text">添加白名单</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%BB%91%E5%90%8D%E5%8D%95%E9%80%80%E5%BD%B9"><span class="toc-number">2.6.5.2.</span> <span class="toc-text">黑名单退役</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Datanode%E5%A4%9A%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE"><span class="toc-number">2.6.6.</span> <span class="toc-text">Datanode多目录配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7"><span class="toc-number">2.7.</span> <span class="toc-text">HDFS 2.X新特性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E9%97%B4%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D"><span class="toc-number">2.7.1.</span> <span class="toc-text">集群间数据拷贝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E5%AD%98%E6%A1%A3"><span class="toc-number">2.7.2.</span> <span class="toc-text">小文件存档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E6%94%B6%E7%AB%99"><span class="toc-number">2.7.3.</span> <span class="toc-text">回收站</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86"><span class="toc-number">2.7.4.</span> <span class="toc-text">快照管理</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E2%80%94%E2%80%94Mybatis/" title="底层原理——Mybatis">底层原理——Mybatis</a><time datetime="2023-06-28T10:00:13.000Z" title="发表于 2023-06-28 18:00:13">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/SSM%E2%80%94%E2%80%94Mybatis/" title="SSM——Mybatis">SSM——Mybatis</a><time datetime="2023-06-28T07:35:16.000Z" title="发表于 2023-06-28 15:35:16">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/SSM%E2%80%94%E2%80%94Spring/" title="SSM——Spring">SSM——Spring</a><time datetime="2023-06-28T07:22:40.000Z" title="发表于 2023-06-28 15:22:40">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/" title="大数据——Hadoop.2.x">大数据——Hadoop.2.x</a><time datetime="2023-06-27T10:09:04.000Z" title="发表于 2023-06-27 18:09:04">2023-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/" title="大数据——Hive调优">大数据——Hive调优</a><time datetime="2023-06-27T10:01:35.000Z" title="发表于 2023-06-27 18:01:35">2023-06-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By DingQuan Zuo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>