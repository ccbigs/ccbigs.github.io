<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据——Hive | ccbigs blog</title><meta name="author" content="DingQuan Zuo"><meta name="copyright" content="DingQuan Zuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第1 章 Hive 基本概念什么是 Hive 1） hive 简介  Hive：由 Facebook 开源用于解决海量、结构化日志的数据统计工具。 Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。  2） Hive 本质：将 HQL 转化成 MapReduce 程序   （1）Hive 处理的数据存储在 HDFS  （2）Hive">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据——Hive">
<meta property="og:url" content="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/index.html">
<meta property="og:site_name" content="ccbigs blog">
<meta property="og:description" content="第1 章 Hive 基本概念什么是 Hive 1） hive 简介  Hive：由 Facebook 开源用于解决海量、结构化日志的数据统计工具。 Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。  2） Hive 本质：将 HQL 转化成 MapReduce 程序   （1）Hive 处理的数据存储在 HDFS  （2）Hive">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/ahead.jpg">
<meta property="article:published_time" content="2023-06-27T04:21:43.000Z">
<meta property="article:modified_time" content="2023-06-27T06:02:13.596Z">
<meta property="article:author" content="DingQuan Zuo">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/ahead.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据——Hive',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-27 14:02:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/ahead.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/1558.png')"><nav id="nav"><span id="blog-info"><a href="/" title="ccbigs blog"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">ccbigs blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据——Hive</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-27T04:21:43.000Z" title="发表于 2023-06-27 12:21:43">2023-06-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-27T06:02:13.596Z" title="更新于 2023-06-27 14:02:13">2023-06-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">30.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>138分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据——Hive"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="第1-章-Hive-基本概念"><a href="#第1-章-Hive-基本概念" class="headerlink" title="第1 章 Hive 基本概念"></a>第1 章 Hive 基本概念</h1><h2 id="什么是-Hive"><a href="#什么是-Hive" class="headerlink" title="什么是 Hive"></a>什么是 Hive</h2><blockquote>
<p>1） hive 简介</p>
</blockquote>
<p>Hive：由 Facebook 开源用于解决<strong>海量</strong>、<strong>结构化日志</strong>的数据统计工具。 Hive 是基于 Hadoop 的一个<strong>数据仓库</strong>工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。</p>
<blockquote>
<p>2） Hive 本质：将 HQL 转化成 MapReduce 程序</p>
</blockquote>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211215154116187.png" alt="image-20211215154116187"></p>
<p>（1）Hive 处理的数据存储在 HDFS </p>
<p>（2）Hive 分析数据底层的实现是 MapReduce </p>
<p>（3）执行程序运行在 Yarn 上</p>
<h2 id="Hive-的优缺点"><a href="#Hive-的优缺点" class="headerlink" title="Hive 的优缺点"></a>Hive 的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>（1）操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）。</p>
<p>（2）避免了去写 MapReduce，减少开发人员的学习成本。</p>
<p>（3）Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。</p>
<p>（4）Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。</p>
<p>（5）Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><blockquote>
<p>1）Hive 的 HQL 表达能力有限</p>
</blockquote>
<p>（1）迭代式算法无法表达</p>
<p>（2）<strong>数据挖掘方面不擅长</strong>，由于 MapReduce 数据处理流程的限制，效率更高的算法却无法实现。</p>
<blockquote>
<p>2）Hive 的效率比较低</p>
</blockquote>
<p>（1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化.</p>
<p>（2）Hive 调优比较困难，粒度较粗.</p>
<h2 id="Hive-架构原理"><a href="#Hive-架构原理" class="headerlink" title="Hive 架构原理"></a>Hive 架构原理</h2><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211215155440362.png" alt="image-20211215155440362"></p>
<p>1）用户接口：Client CLI（command-line interface）、JDBC&#x2F;ODBC(jdbc 访问 hive)、WEBUI（浏览器访问 hive）</p>
<p>2）元数据：Metastore 元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p>
<p><strong>默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore 。</strong></p>
<p>3）Hadoop：使用 HDFS 进行存储，使用 MapReduce 进行计算。</p>
<p>4）驱动器：Driver </p>
<p>​	（1）解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。</p>
<p>​	（2）编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。</p>
<p>​	（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p>
<p>​	（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR&#x2F;Spark。</p>
<blockquote>
<p>Hive的运行机制</p>
</blockquote>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211215160250922.png" alt="image-20211215160250922"></p>
<p>Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的 Driver，结合元数据(MetaStore)，将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。</p>
<h2 id="Hive-和数据库比较"><a href="#Hive-和数据库比较" class="headerlink" title="Hive 和数据库比较"></a>Hive 和数据库比较</h2><p>由于 Hive 采用了类似 SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是 Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p>
<h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。</p>
<h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于 Hive 是针对数据仓库应用设计的，而<strong>数据仓库的内容是读多写少的</strong>。因此，<strong>Hive 中不建议对数据的改写，所有的数据都是在加载的时候确定好的</strong>。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET 修改数据。</p>
<h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候， Hive 的并行计算显然能体现出优势。</p>
<h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p>
<h1 id="第2-章-Hive-安装"><a href="#第2-章-Hive-安装" class="headerlink" title="第2 章 Hive 安装"></a>第2 章 Hive 安装</h1><h2 id="Hive-安装地址"><a href="#Hive-安装地址" class="headerlink" title="Hive 安装地址"></a>Hive 安装地址</h2><p>1）Hive 官网地址 <a target="_blank" rel="noopener" href="http://hive.apache.org/">http://hive.apache.org/</a></p>
<p>2）文档查看地址 <a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a> </p>
<p>3）下载地址 <a target="_blank" rel="noopener" href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p>
<p>4）github 地址 <a target="_blank" rel="noopener" href="https://github.com/apache/hive">https://github.com/apache/hive</a></p>
<h2 id="Hive-安装部署"><a href="#Hive-安装部署" class="headerlink" title="Hive 安装部署"></a>Hive 安装部署</h2><h3 id="安装-Hive"><a href="#安装-Hive" class="headerlink" title="安装 Hive"></a>安装 Hive</h3><p>1）把 apache-hive-3.1.2-bin.tar.gz 上传到 linux 的&#x2F;opt&#x2F;software 目录下</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211215163322937.png" alt="image-20211215163322937"></p>
<p>2）解压 apache-hive-3.1.2-bin.tar.gz 到&#x2F;opt&#x2F;module&#x2F;目录下面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 ~]$ tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment">#更改名字</span></span><br><span class="line">[atguigu@hadoop100 module]$ <span class="built_in">mv</span> apache-hive-3.1.2-bin hive</span><br><span class="line"><span class="comment">#查看当前目录</span></span><br><span class="line">[atguigu@hadoop100 hive]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hive</span><br><span class="line"><span class="comment">#修改环境变量</span></span><br><span class="line">[atguigu@hadoop100 hive]$ sudo vim /etc/profile</span><br><span class="line"><span class="comment">#-----追加内容------</span></span><br><span class="line"><span class="comment">#HIVE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="comment">#刷新</span></span><br><span class="line">[atguigu@hadoop100 hive]$ <span class="built_in">source</span> /etc/profile </span><br></pre></td></tr></table></figure>

<p>3）解决日志 Jar 包冲突</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 software]$ <span class="built_in">mv</span> <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl2.10.0.jar <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.bak</span><br></pre></td></tr></table></figure>

<p>4）初始化元数据库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/schematool -dbType derby -initSchema</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211215164156418.png" alt="image-20211215164156418"></p>
<p><strong>初始化完毕，最后启动hadoop集群</strong></p>
<h3 id="启动并使用-Hive"><a href="#启动并使用-Hive" class="headerlink" title="启动并使用 Hive"></a>启动并使用 Hive</h3><p>1）启动 Hive</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/hive</span><br></pre></td></tr></table></figure>

<p>2）创建一个数据库，并插入数据以及查看</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">hive&gt; create table <span class="built_in">test</span>(<span class="built_in">id</span> string);</span><br><span class="line">hive&gt; insert into <span class="built_in">test</span> values(<span class="string">&quot;1001&quot;</span>);</span><br><span class="line">hive&gt; <span class="keyword">select</span> * from <span class="built_in">test</span>;</span><br><span class="line">OK</span><br><span class="line">1001</span><br></pre></td></tr></table></figure>

<p>3）在前端页面查看</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211216113546779.png" alt="image-20211216113546779"></p>
<p>下载并打开</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211216113809909.png" alt="image-20211216113809909"></p>
<blockquote>
<p>存在缺陷</p>
</blockquote>
<p>在新会话中使用会报错，原因在于 Hive 默认使用的元数据库为 derby，开启 Hive 之后就会占用元数据库，且不与其他客户端共享数据，所以我们需要将 Hive 的元数据地址改为 MySQL。</p>
<h2 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h2><blockquote>
<p>1）检查当前系统是否安装过 MySQL</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ rpm -qa|grep mariadb</span><br><span class="line">mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line"><span class="comment">#如果存在，就用下面这条命令卸载</span></span><br><span class="line">[atguigu@hadoop100 hive]$ sudo rpm -e --nodeps mariadb-libs</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）解压 MySQL 安装包</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 ~]$ tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C /opt/software/</span><br><span class="line">[atguigu@hadoop100 software]$ ll</span><br><span class="line">总用量595272</span><br><span class="line">drwxr-xr-x 11 atguigu atguigu 233 12月16 08:26 hadoop-2.7.2</span><br><span class="line">-rw-r--r--1 atguigu atguigu 45109364 9月30 2019 mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 318768 9月30 2019 mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 7037096 9月30 2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 49329100 9月30 2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 23354908 9月30 2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 136837816 9月30 2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 4374364 9月30 2019 mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 1353312 9月30 2019 mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 208694824 9月30 2019 mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--1 atguigu atguigu 133129992 9月30 2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）在安装目录下执行 rpm 安装</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y libaio</span><br><span class="line">sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">云服务器安装</span><br><span class="line">sudo yum install -y libaio</span><br><span class="line">sudo yum -y install numactl</span><br><span class="line">sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm --force --nodeps</span><br><span class="line">sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm --force --nodeps</span><br><span class="line">sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm --force --nodeps</span><br><span class="line">sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm --force --nodeps</span><br><span class="line">sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm --force --nodeps</span><br></pre></td></tr></table></figure>

<p>注意:按照顺序依次执行如果 Linux 是最小化安装的，在安装 mysql-community-server-5.7.28-1.el7.x86_64.rpm 时可能会出现如下错误</p>
<blockquote>
<p>5）删除&#x2F;etc&#x2F;my.cnf 文件中 datadir 指向的目录下的所有内容,如果有内容的情况下:</p>
</blockquote>
<p>查看 datadir 的值：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datadir=/var/lib/mysql</span><br></pre></td></tr></table></figure>

<p>删除&#x2F;var&#x2F;lib&#x2F;mysql 目录下的所有内容: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu @hadoop102 mysql]<span class="comment"># cd /var/lib/mysql </span></span><br><span class="line">[atguigu @hadoop102 mysql]<span class="comment"># sudo rm -rf ./*//注意执行命令的位置</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>6）初始化数据库</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 software]$ sudo mysqld --initialize --user=mysql</span><br></pre></td></tr></table></figure>

<blockquote>
<p>7）查看数据生成的临时密码</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 software]$ sudo <span class="built_in">cat</span> /var/log/mysqld.log</span><br><span class="line">2021-12-16T04:00:41.379764Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation <span class="keyword">for</span> more details).</span><br><span class="line">2021-12-16T04:00:41.555275Z 0 [Warning] InnoDB: New <span class="built_in">log</span> files created, LSN=45790</span><br><span class="line">2021-12-16T04:00:41.585421Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.</span><br><span class="line">2021-12-16T04:00:41.644727Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: bc871a78-5e24-11ec-8bb9-000c291b1228.</span><br><span class="line">2021-12-16T04:00:41.655476Z 0 [Warning] Gtid table is not ready to be used. Table <span class="string">&#x27;mysql.gtid_executed&#x27;</span> cannot be opened.</span><br><span class="line">2021-12-16T04:00:42.193117Z 0 [Warning] CA certificate ca.pem is self signed.</span><br><span class="line">2021-12-16T04:00:42.380448Z 1 [Note] A temporary password is generated <span class="keyword">for</span> root@localhost: cuF/svfFG6:<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>8）启动 MySQL 服务</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu @hadoop102 opt]$ sudo systemctl start mysqld</span><br></pre></td></tr></table></figure>

<blockquote>
<p>9）登录 MySQL 数据库</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu @hadoop102 opt]$ mysql -uroot -p </span><br><span class="line">Enter password: 输入临时生成的密码</span><br></pre></td></tr></table></figure>



<blockquote>
<p>10）必须先修改 root 用户的密码,否则执行其他的操作会报错</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> password = password(<span class="string">&quot;新密码&quot;</span>); </span><br></pre></td></tr></table></figure>

<blockquote>
<p>11）修改 mysql 库下的 user 表中的 root 用户允许任意 ip 连接</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update mysql.user <span class="built_in">set</span> host=<span class="string">&#x27;%&#x27;</span> <span class="built_in">where</span> user=<span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK,1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK,0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>



<h2 id="Hive-元数据配置到-MySQL"><a href="#Hive-元数据配置到-MySQL" class="headerlink" title="Hive 元数据配置到 MySQL"></a>Hive 元数据配置到 MySQL</h2><h3 id="拷贝驱动"><a href="#拷贝驱动" class="headerlink" title="拷贝驱动"></a>拷贝驱动</h3><p>将 MySQL 的 JDBC 驱动拷贝到 Hive 的 lib 目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 software]$ <span class="built_in">cp</span> /home/atguigu/mysql-connector-java-5.1.37.jar <span class="variable">$HIVE_HOME</span>/lib</span><br></pre></td></tr></table></figure>



<h3 id="配置-Metastore-到-MySQL"><a href="#配置-Metastore-到-MySQL" class="headerlink" title="配置 Metastore 到 MySQL"></a>配置 Metastore 到 MySQL</h3><blockquote>
<p>1）在$HIVE_HOME&#x2F;conf 目录下新建 hive-site.xml 文件</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 software]$ vim <span class="variable">$HIVE_HOME</span>/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>追加以下内容：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- jdbc 连接的 URL --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop100:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- jdbc 连接的 Driver--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- jdbc 连接的 username--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- jdbc 连接的 password --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>7760608<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Hive 元数据存储版本的验证--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--元数据存储授权--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Hive 默认在 HDFS 的工作目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）登陆 MySQL </p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 software]$ mysql -uroot -p7760608 </span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）新建 Hive 元数据库</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database metastore; </span><br><span class="line">mysql&gt; quit; </span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）初始化 Hive 元数据库</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 conf]$ schematool -initSchema -dbType mysql -verbose</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211216123337867.png" alt="image-20211216123337867"></p>
<h3 id="再次启动-Hive"><a href="#再次启动-Hive" class="headerlink" title="再次启动 Hive"></a>再次启动 Hive</h3><blockquote>
<p>1）启动 Hive</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/hive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）使用 Hive</p>
</blockquote>
<h2 id="使用元数据服务的方式访问-Hive"><a href="#使用元数据服务的方式访问-Hive" class="headerlink" title="使用元数据服务的方式访问 Hive"></a>使用元数据服务的方式访问 Hive</h2><p><strong>原因：方便其他接口调用hive</strong></p>
<blockquote>
<p> 1）在 hive-site.xml 文件中添加如下配置信息</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--指定存储元数据要连接的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop100:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）启动 metastore</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ hive --service metastore</span><br><span class="line">2021-12-16 14:15:28: Starting Hive Metastore Server</span><br></pre></td></tr></table></figure>

<p>注意: 启动后窗口不能再操作，需打开一个新的 shell 窗口做别的操作</p>
<p>3）启动 hive</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/hive</span><br></pre></td></tr></table></figure>



<h2 id="使用-JDBC-方式访问-Hive"><a href="#使用-JDBC-方式访问-Hive" class="headerlink" title="使用 JDBC 方式访问 Hive"></a>使用 JDBC 方式访问 Hive</h2><blockquote>
<p>1）在 hive-site.xml 文件中添加如下配置信息</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--指定 hiveserver2 连接的 host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定 hiveserver2 连接的端口号--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）启动 hiveserver2 </p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/hive --service hiveserver2</span><br></pre></td></tr></table></figure>

<p><strong>配置hadoop中的core-site.xml文件，允许hive访问</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--配置访问hadoop的权限，能够让hive访问到--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.atguigu.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.atguigu.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>配置完成之后，需要重启整个hadoop集群。</strong></p>
<blockquote>
<p>3）启动 beeline 客户端（需要多等待一会）</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/beeline -u jdbc:hive2://hadoop100:10000 -n atguigu</span><br></pre></td></tr></table></figure>



<blockquote>
<p>4）看到如下界面</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/beeline -u jdbc:hive2://hadoop100:10000 -n atguigu</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/opt/module/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Connecting to jdbc:hive2://hadoop100:10000</span><br><span class="line">Connected to: Apache Hive (version 3.1.2)</span><br><span class="line">Driver: Hive JDBC (version 3.1.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 3.1.2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop100:10000&gt;</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>5）编写 hive 服务启动脚本（了解）</strong></p>
</blockquote>
<p>（1）前台启动的方式导致需要打开多个 shell 窗口，可以使用如下方式后台方式启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span>: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态</span><br><span class="line">/dev/null：是 Linux 文件系统中的一个文件，被称为黑洞，所有写入改文件的内容都会被自动丢弃</span><br><span class="line">2&gt;&amp;1 : 表示将错误重定向到标准输出上</span><br><span class="line">&amp;: 放在命令结尾,表示后台运行</span><br></pre></td></tr></table></figure>

<p>一般会组合使用: nohup [xxx 命令操作]&gt; file 2&gt;&amp;1 &amp;，表示将 xxx 命令运行的结果输出到 file 中，并保持命令启动的进程在后台运行。</p>
<p>如上命令不要求掌握。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop202 hive]$ nohup hive --service metastore 2&gt;&amp;1 &amp;</span><br><span class="line">[atguigu@hadoop202 hive]$ nohup hive --service hiveserver2 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>（2）为了方便使用，可以直接编写脚本来管理服务的启动和关闭</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ vim <span class="variable">$HIVE_HOME</span>/bin/hiveservices.sh </span><br></pre></td></tr></table></figure>

<p>内容如下：此脚本的编写不要求掌握。直接拿来使用即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=$HIVE_HOME/logs</span><br><span class="line">if [ !-d $HIVE_LOG_DIR ]</span><br><span class="line">then</span><br><span class="line">	mkdir -p $HIVE_LOG_DIR</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">检查进程是否运行正常，参数1 为进程名，参数2 为进程端口</span></span><br><span class="line">function check_process()</span><br><span class="line">	&#123;</span><br><span class="line">		pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line">		ppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk &#x27;&#123;print $7&#125;&#x27;| cut -d &#x27;/&#x27;-f 1)</span><br><span class="line">		echo $pid</span><br><span class="line">		[[ &quot;$pid&quot;=~&quot;$ppid&quot;]]&amp;&amp;[ &quot;$ppid&quot;]&amp;&amp; return 0 || return 1</span><br><span class="line">	&#125;</span><br><span class="line">	function hive_start()</span><br><span class="line">		&#123;</span><br><span class="line">			metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">			cmd=&quot;nohup hive --service metastore &gt;$HIVE_LOG_DIR/metastore.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">			[ -z &quot;$metapid&quot;]&amp;&amp; eval $cmd || echo &quot;Metastroe 服务已启动&quot;</span><br><span class="line">			server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">			cmd=&quot;nohup hiveserver2 &gt;$HIVE_LOG_DIR/hiveServer2.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">			[ -z &quot;$server2pid&quot;]&amp;&amp; eval $cmd || echo &quot;HiveServer2 服务已启动&quot;</span><br><span class="line">		&#125;</span><br><span class="line">		function hive_stop()</span><br><span class="line">			&#123;</span><br><span class="line">				metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">				[ &quot;$metapid&quot;]&amp;&amp; kill $metapid || echo &quot;Metastore 服务未启动&quot;</span><br><span class="line">				server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">				[ &quot;$server2pid&quot;]&amp;&amp; kill $server2pid || echo &quot;HiveServer2 服务未启动&quot;</span><br><span class="line">			&#125;</span><br><span class="line">			case $1 in</span><br><span class="line">			&quot;start&quot;)</span><br><span class="line">			hive_start</span><br><span class="line">		;;</span><br><span class="line">		&quot;stop&quot;)</span><br><span class="line">		hive_stop</span><br><span class="line">	;;</span><br><span class="line">	&quot;restart&quot;)</span><br><span class="line">	hive_stop</span><br><span class="line">	sleep 2</span><br><span class="line">	hive_start</span><br><span class="line">;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; echo &quot;Metastore 服务运行正常&quot;|| echo &quot;Metastore 服务运行异常&quot;</span><br><span class="line">check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; echo &quot;HiveServer2 服务运行正常&quot;|| echo &quot;HiveServer2 服务运行异常&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo Invalid Args!</span><br><span class="line">echo &#x27;Usage: &#x27;$(basename $0)&#x27; start|stop|restart|status&#x27;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>3）添加执行权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ <span class="built_in">chmod</span> +x <span class="variable">$HIVE_HOME</span>/bin/hiveservices.sh </span><br></pre></td></tr></table></figure>

<p>4）启动 Hive 后台服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ hiveservices.sh start </span><br><span class="line">Metastroe 服务已启动</span><br><span class="line">HiveServer2 服务已启动</span><br></pre></td></tr></table></figure>

<p><strong>查看状态</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ hiveservices.sh status</span><br><span class="line">Metastore 服务运行正常</span><br><span class="line">HiveServer2 服务运行正常</span><br></pre></td></tr></table></figure>



<h2 id="Hive-常用交互命令"><a href="#Hive-常用交互命令" class="headerlink" title="Hive 常用交互命令"></a>Hive 常用交互命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ bin/hive -<span class="built_in">help</span></span><br><span class="line">Hive Session ID = 2da2c156-d8a5-4ece-9979-970d645cfe83</span><br><span class="line">usage: hive</span><br><span class="line">-d,--define &lt;key=value&gt;  Variable substitution to apply to Hive</span><br><span class="line"> commands. e.g.-d A=B or --define A=B</span><br><span class="line">--database &lt;databasename&gt;  Specify the database to use</span><br><span class="line">-e &lt;quoted-query-string&gt;  SQL from <span class="built_in">command</span> line</span><br><span class="line">-f &lt;filename&gt;  SQL from files</span><br><span class="line">-H,--<span class="built_in">help</span>  Print <span class="built_in">help</span> information</span><br><span class="line">--hiveconf &lt;property=value&gt;  Use value <span class="keyword">for</span> given property</span><br><span class="line">--hivevar &lt;key=value&gt;  Variable substitution to apply to Hive</span><br><span class="line"> commands. e.g.--hivevar A=B</span><br><span class="line">-i &lt;filename&gt;  Initialization SQL file</span><br><span class="line">-S,--silent  Silent mode <span class="keyword">in</span> interactive shell</span><br><span class="line">-v,--verbose  Verbose mode (<span class="built_in">echo</span> executed SQL to the</span><br><span class="line"> console)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）“-e”不进入 hive 的交互窗口执行 sql 语句</p>
</blockquote>
<p>[atguigu@hadoop102 hive]$ bin&#x2F;hive -e “select id from student;”</p>
<blockquote>
<p>2）“-f”执行脚本中 sql 语句</p>
</blockquote>
<p>（1）在&#x2F;opt&#x2F;module&#x2F;hive&#x2F;下创建 datas 目录并在 datas 目录下创建 hivef.sql 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ <span class="built_in">touch</span> hivef.sql </span><br></pre></td></tr></table></figure>

<p>（2）文件中写入正确的 sql 语句</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> *from student; </span><br></pre></td></tr></table></figure>

<p>（3）执行文件中的 sql 语句</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql </span><br></pre></td></tr></table></figure>

<p>（4）执行文件中的 sql 语句并将结果写入文件中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure>



<h2 id="Hive-其他命令操作"><a href="#Hive-其他命令操作" class="headerlink" title="Hive 其他命令操作"></a>Hive 其他命令操作</h2><blockquote>
<p>1）退出 hive 窗口：</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;<span class="built_in">exit</span>; </span><br><span class="line">hive(default)&gt;quit; </span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）在 hive cli 命令窗口中如何查看 hdfs 文件系统</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -<span class="built_in">ls</span> /; </span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）查看在 hive 中输入的所有历史命令</p>
</blockquote>
<p>（1）进入到当前用户的根目录&#x2F;root 或&#x2F;home&#x2F;atguigu </p>
<p>（2）查看. hivehistory 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguig2u@hadoop102 ~]$ <span class="built_in">cat</span> .hivehistory</span><br></pre></td></tr></table></figure>



<h2 id="Hive-常见属性配置"><a href="#Hive-常见属性配置" class="headerlink" title="Hive 常见属性配置"></a>Hive 常见属性配置</h2><h3 id="Hive-运行日志信息配置"><a href="#Hive-运行日志信息配置" class="headerlink" title="Hive 运行日志信息配置"></a>Hive 运行日志信息配置</h3><p>1）<strong>Hive 的 log 默认存放在&#x2F;tmp&#x2F;atguigu&#x2F;hive.log 目录下</strong>（当前用户名下）</p>
<p>2）<strong>修改 hive 的 log 存放日志到&#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs</strong></p>
<blockquote>
<p>（1）修改&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;hive-log4j2.properties.template </p>
</blockquote>
<p>文件名称为 hive-log4j2.properties</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 conf]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop100 conf]$ <span class="built_in">mv</span>  hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line">[atguigu@hadoop100 conf]$ vim hive-log4j2.properties </span><br></pre></td></tr></table></figure>

<p>默认参数：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211216160612192.png" alt="image-20211216160612192"></p>
<p>改为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">property.hive.log.dir = /opt/module/hive/logs</span><br></pre></td></tr></table></figure>



<h3 id="打印当前库和表头"><a href="#打印当前库和表头" class="headerlink" title="打印当前库和表头"></a>打印当前库和表头</h3><p>在 hive-site.xml 中加入如下两个配置: </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h3><p>1）查看当前所有的配置信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure>

<p>2）参数的配置三种方式</p>
<blockquote>
<p>（1）配置文件方式默认配置文件：hive-default.xml </p>
</blockquote>
<p>用户自定义配置文件：hive-site.xml </p>
<p>注意：<strong>用户自定义配置会覆盖默认配置</strong>。另外，Hive 也会读入 Hadoop 的配置，因为<strong>Hive 是作为 Hadoop 的客户端启动的</strong>，Hive 的配置会覆盖 Hadoop 的配置。配置文件的设定对本机启动的所有 Hive 进程都有效。</p>
<blockquote>
<p>（2）命令行参数方式</p>
</blockquote>
<p>启动 Hive 时，可以在命令行添加-hiveconf param&#x3D;value 来设定参数。</p>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10; </span><br></pre></td></tr></table></figure>

<p>注意：仅对本次 hive 启动有效查看参数设置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>（3）参数声明方式</p>
</blockquote>
<p>可以在 HQL 中使用 SET 关键字设定参数</p>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100; </span><br></pre></td></tr></table></figure>

<p><strong>注意：仅对本次 hive 启动有效。</strong></p>
<p>查看参数设置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks; </span><br></pre></td></tr></table></figure>

<p>上述三种设定方式的优先级依次递增。即<strong>配置文件&lt;命令行参数&lt;参数声明</strong>。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
<h1 id="第3-章-Hive-数据类型"><a href="#第3-章-Hive-数据类型" class="headerlink" title="第3 章 Hive 数据类型"></a>第3 章 Hive 数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211216170731289.png" alt="image-20211216170731289"></p>
<p>对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB 的字符数。</p>
<h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211216170816622.png" alt="image-20211216170816622"></p>
<p>Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的 Array 和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<blockquote>
<p>1）案例实操</p>
</blockquote>
<p>（1）假设某表有如下一行，我们用 JSON 格式来表示其数据结构。在 Hive 下访问的格式为</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;songsong&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;bingbing&quot;</span><span class="punctuation">,</span><span class="string">&quot;lili&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="comment">//列表 Array,</span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="comment">//键值 Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span><span class="punctuation">:</span> <span class="number">18</span> <span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span><span class="punctuation">:</span> <span class="number">19</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="comment">//结构 Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hui long guan&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;city&quot;</span><span class="punctuation">:</span> <span class="string">&quot;beijing&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>（2）基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。</p>
</blockquote>
<p>创建本地测试文件 test.txt</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long </span><br><span class="line">guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
<blockquote>
<p>（3）Hive 上创建测试表 test</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test3(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string, city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p>row format delimited fields terminated by ‘,’–列分隔符</p>
<p>collection items terminated by ‘_’–MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p>
<p>map keys terminated by ‘:’– MAP 中的 key 与 value 的分隔符</p>
<p>lines terminated by ‘\n’; –行分隔符</p>
<blockquote>
<p>4.上传text.txt文件到hadoop上</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ hadoop fs -put text.txt /user/hive/warehouse/test3</span><br></pre></td></tr></table></figure>



<blockquote>
<p>（5）访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; <span class="keyword">select</span> friends[1],children[<span class="string">&#x27;xiao song&#x27;</span>],address.city from test3;</span><br><span class="line">OK</span><br><span class="line">_c0  _c1  city</span><br><span class="line">lili 18  NULL</span><br><span class="line">NULL  NULL  NULL</span><br><span class="line">susu  NULL  beijing</span><br><span class="line">Time taken: 0.573 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>



<h2 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h2><p>Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如，某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST 操作。</p>
<blockquote>
<p>1）隐式类型转换规则如下</p>
</blockquote>
<p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT。</p>
<p>（2）所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。</p>
<p>（3）TINYINT、SMALLINT、INT 都可以转换为 FLOAT。</p>
<p>（4）BOOLEAN 类型不可以转换为任何其它的类型。</p>
<blockquote>
<p>2）可以使用 CAST 操作显示进行数据类型转换</p>
</blockquote>
<p>例如 CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行 CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop102:10000&gt; select &#x27;1&#x27;+2, cast(&#x27;1&#x27;as int)+2;</span><br><span class="line">+------+------+--+</span><br><span class="line">| _c0 | _c1 |</span><br><span class="line">+------+------+--+</span><br><span class="line">|3.0 |3 |</span><br><span class="line">+------+------+--+</span><br></pre></td></tr></table></figure>





<h1 id="第4-章-DDL-数据定义"><a href="#第4-章-DDL-数据定义" class="headerlink" title="第4 章 DDL 数据定义"></a>第4 章 DDL 数据定义</h1><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE [IF NOT EXISTS] database_name</span><br><span class="line">[COMMENT database_comment]#注释</span><br><span class="line">[LOCATION hdfs_path]#指定当前库的hdfs目录</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value,...)]; #备注创建作者和创建时间</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）创建一个数据库，数据库在 HDFS 上的默认存储路径是&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;*.db。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法）</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from </span><br><span class="line">org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）创建一个数据库，指定数据库在 HDFS 上存放的位置</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database if not exists hive3 location &#x27;/hive3&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217090532691.png" alt="image-20211217090532691"></p>
<h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><h3 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h3><blockquote>
<p>1）显示数据库</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">default</span><br><span class="line">hive3</span><br><span class="line">Time taken: 0.41 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）过滤显示查询的数据库</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show databases like &#x27;hi*&#x27;;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">hive3</span><br><span class="line">Time taken: 0.142 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>



<h3 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h3><blockquote>
<p>1）显示数据库信息</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc database hive3;</span><br><span class="line">OK</span><br><span class="line">db_name comment location  owner_name  owner_type  parameters</span><br><span class="line">hive3  hdfs://hadoop100:8020/hive3  atguigu USER</span><br><span class="line">Time taken: 0.187 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）显示数据库详细信息，extended</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;  desc database extended hive3;</span><br><span class="line">OK</span><br><span class="line">db_name comment location  owner_name  owner_type  parameters</span><br><span class="line">hive3  hdfs://hadoop100:8020/hive3  atguigu USER</span><br></pre></td></tr></table></figure>



<h3 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use hive3;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>



<h2 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h2><p>用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; alter database hive3 set dbproperties(&quot;createTime&quot;=&quot;202-10-26&quot;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.17 seconds</span><br></pre></td></tr></table></figure>

<p><strong>在 hive 中查看修改结果</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; desc database extended hive3;</span><br><span class="line">OK</span><br><span class="line">db_name comment location  owner_name  owner_type  parameters</span><br><span class="line">hive3  hdfs://hadoop100:8020/hive3  atguigu USER &#123;createTime=202-10-26&#125;</span><br><span class="line">Time taken: 0.065 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>



<h2 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h2><blockquote>
<p>1）删除空数据库(drop命令只能删除空的数据库)</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）如果删除的数据库不存在，最好采用 if exists 判断数据库是否存在</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）如果数据库不为空，可以采用 cascade 命令，强制删除</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from </span><br><span class="line">org.apache.hadoop.hive.ql.exec.DDLTask.</span><br><span class="line">InvalidOperationException(message:Database db_hive is not empty. One or </span><br><span class="line">more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure>



<h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><blockquote>
<p>1）建表语法</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name #EXTERNAL：外部的</span><br><span class="line">[(col_name data_type [COMMENT col_comment],...)]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment],...)]#PARTITIONED BY：分区表</span><br><span class="line">[CLUSTERED BY (col_name, col_name,...)#CLUSTERED BY：分桶表</span><br><span class="line">[SORTED BY (col_name [ASC|DESC],...)] INTO num_buckets BUCKETS]#分桶表</span><br><span class="line">[ROW FORMAT row_format]#行格式</span><br><span class="line">[STORED AS file_format]#指定文件格式</span><br><span class="line">[LOCATION hdfs_path]	#指定表的位置信息</span><br><span class="line">[TBLPROPERTIES (property_name=property_value,...)]#额外属性</span><br><span class="line">[AS select_statement]	#</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）字段解释说明</p>
</blockquote>
<p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
<p>（2）EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
<p>（3）COMMENT：为表和列添加注释。</p>
<p>（4）PARTITIONED BY 创建分区表</p>
<p>（5）CLUSTERED BY 创建分桶表</p>
<p>（6）SORTED BY 不常用，对桶中的一个或多个列另外排序</p>
<p>（7）ROW FORMAT  DELIMITED [FIELDS TERMINATED BY char][COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char][LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name&#x3D;property_value, property_name&#x3D;property_value,…)]用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW  FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，<strong>Hive 通过 SerDe 确定表的具体的列的数据。 SerDe 是 Serialize&#x2F;Deserilize 的简称， hive 使用 Serde 进行行对象的序列与反序列化</strong>。</p>
<p>（8）STORED AS 指定存储文件类型常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED  AS SEQUENCEFILE。</p>
<p>（9）LOCATION ：指定表在 HDFS 上的存储位置。</p>
<p>（10）AS：后跟查询语句，根据查询结果创建表。</p>
<p>（11）LIKE 允许用户复制现有的表结构，但是不复制数据。</p>
<h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><blockquote>
<p>1）理论</p>
</blockquote>
<p>默认创建的表都是所谓的<strong>管理表</strong>，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，&#x2F;user&#x2F;hive&#x2F;warehouse)所定义的目录的子目录下。</p>
<p><strong>当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。</strong></p>
<blockquote>
<p>2）案例实操</p>
</blockquote>
<p>（0）原始数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1001 ss1</span><br><span class="line">1002 ss2</span><br><span class="line">1003 ss3</span><br><span class="line">1004 ss4</span><br><span class="line">1005 ss5</span><br><span class="line">1006 ss6</span><br><span class="line">1007 ss7</span><br><span class="line">1008 ss8</span><br><span class="line">1009 ss9</span><br><span class="line">1010 ss10</span><br><span class="line">1011 ss11</span><br><span class="line">1012 ss12</span><br><span class="line">1013 ss13</span><br><span class="line">1014 ss14</span><br><span class="line">1015 ss15</span><br><span class="line">1016 ss16</span><br></pre></td></tr></table></figure>

<p>上传到hadoop</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 ~]$ hadoop fs -put student.txt /user/hive/warehouse/student </span><br></pre></td></tr></table></figure>



<p>（1）普通创建表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; create table if not exists student(id int,name string)</span><br><span class="line">&gt; row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">&gt; stored as textfile</span><br><span class="line">&gt; location &#x27;/user/hive/warehouse/student&#x27;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.417 seconds</span><br></pre></td></tr></table></figure>

<p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; create table if not exists student2 as select id,name from student;</span><br><span class="line">Query ID = atguigu_20211217102911_8a4f1a46-8e34-4a33-8f49-801c72a41aa7</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&#x27;s no reduce operator</span><br><span class="line">Starting Job = job_1639638039622_0002, Tracking URL = http://hadoop101:8088/proxy/application_1639638039622_0002/</span><br><span class="line">Kill Command = /opt/module/hadoop-3.1.3/bin/mapred job -kill job_1639638039622_0002</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2021-12-17 10:29:37,602 Stage-1 map = 0%, reduce = 0%</span><br><span class="line">2021-12-17 10:29:51,642 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.52 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 520 msec</span><br><span class="line">Ended Job = job_1639638039622_0002</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to directory hdfs://hadoop100:8020/hive3/.hive-staging_hive_2021-12-17_10-29-11_615_1154320876271755629-1/-ext-10002</span><br><span class="line">Moving data to directory hdfs://hadoop100:8020/hive3/student2</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Cumulative CPU: 1.52 sec  HDFS Read: 4868 HDFS Write: 166 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 520 msec</span><br><span class="line">OK</span><br><span class="line">id  name</span><br><span class="line">Time taken: 42.57 seconds</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217103155520.png" alt="image-20211217103155520"></p>
<p>（3）根据已经存在的表结构创建表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; create table if not exists student3 like student;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.31 seconds</span><br></pre></td></tr></table></figure>



<p>（4）查询表的类型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; desc formatted student;</span><br><span class="line">OK</span><br><span class="line">col_name  data_type  comment</span><br><span class="line"># col_name  data_type  comment </span><br><span class="line">id  int </span><br><span class="line">name  string </span><br><span class="line"></span><br><span class="line"># Detailed Table Information </span><br><span class="line">Database:  hive3 </span><br><span class="line">OwnerType:  USER </span><br><span class="line">Owner:  atguigu </span><br><span class="line">CreateTime:  Fri Dec 17 10:20:47 CST 2021 </span><br><span class="line">LastAccessTime:  UNKNOWN </span><br><span class="line">Retention: 0 </span><br><span class="line">Location:  hdfs://hadoop100:8020/user/hive/warehouse/student </span><br><span class="line">Table Type:  MANAGED_TABLE </span><br><span class="line">Table Parameters: </span><br><span class="line"> COLUMN_STATS_ACCURATE &#123;\&quot;BASIC_STATS\&quot;:\&quot;true\&quot;,\&quot;COLUMN_STATS\&quot;:&#123;\&quot;id\&quot;:\&quot;true\&quot;,\&quot;name\&quot;:\&quot;true\&quot;&#125;&#125;</span><br><span class="line"> bucketing_version 2 </span><br><span class="line"> numFiles 0 </span><br><span class="line"> numRows 0 </span><br><span class="line"> rawDataSize 0 </span><br><span class="line"> totalSize 0 </span><br><span class="line"> transient_lastDdlTime 1639707647 </span><br><span class="line"></span><br><span class="line"># Storage Information </span><br><span class="line">SerDe Library:  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe </span><br><span class="line">InputFormat:  org.apache.hadoop.mapred.TextInputFormat </span><br><span class="line">OutputFormat:  org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat </span><br><span class="line">Compressed:  No </span><br><span class="line">Num Buckets: -1 </span><br><span class="line">Bucket Columns: []</span><br><span class="line">Sort Columns: []</span><br><span class="line">Storage Desc Params: </span><br><span class="line"> field.delim \t </span><br><span class="line"> serialization.format \t </span><br><span class="line">Time taken: 0.305 seconds, Fetched: 33 row(s)</span><br></pre></td></tr></table></figure>



<h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><blockquote>
<p>1）理论</p>
</blockquote>
<p>因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p>
<blockquote>
<p>2）管理表和外部表的使用场景</p>
</blockquote>
<p>每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT 进入内部表。</p>
<blockquote>
<p>3）案例实操</p>
</blockquote>
<p>分别创建部门和员工外部表，并向表中导入数据。</p>
<p>（0）原始数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dept:</span><br><span class="line">10 ACCOUNTING 1700</span><br><span class="line">20 RESEARCH 1800</span><br><span class="line">30 SALES 1900</span><br><span class="line">40 OPERATIONS 1700</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">emp：</span><br><span class="line">7369 SMITH CLERK 7902 1980-12-17 800.00 20</span><br><span class="line">7499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 30</span><br><span class="line">7521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 30</span><br><span class="line">7566 JONES MANAGER 7839 1981-4-2 2975.00 20</span><br><span class="line">7654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 30</span><br><span class="line">7698 BLAKE MANAGER 7839 1981-5-1 2850.00 30</span><br><span class="line">7782 CLARK MANAGER 7839 1981-6-9 2450.00 10</span><br><span class="line">7788 SCOTT ANALYST 7566 1987-4-19 3000.00 20</span><br><span class="line">7839 KING PRESIDENT 1981-11-17 5000.00 10</span><br><span class="line">7844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 30</span><br><span class="line">7876 ADAMS CLERK 7788 1987-5-23 1100.00 20</span><br><span class="line">7900 JAMES CLERK 7698 1981-12-3 950.00 30</span><br><span class="line">7902 FORD ANALYST 7566 1981-12-3 3000.00 20</span><br><span class="line">7934 MILLER CLERK 7782 1982-1-23 1300.00 10</span><br></pre></td></tr></table></figure>

<p>（1）上传数据到 HDFS</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 ~]$ hadoop fs -put emp.txt /hive3/emp</span><br><span class="line">2021-12-17 10:47:20,774 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop100 ~]$ hadoop fs -put dept /hive3/dept</span><br><span class="line">2021-12-17 10:47:47,312 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>（2）建表语句，创建外部表</p>
<p>创建部门表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; create external table if not exists dept(</span><br><span class="line"> deptno int,</span><br><span class="line"> dname string,</span><br><span class="line"> loc int</span><br><span class="line">)</span><br><span class="line"> row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.148 seconds</span><br></pre></td></tr></table></figure>

<p>创建员工表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; create external table if not exists emp(</span><br><span class="line"> empno int,</span><br><span class="line"> ename string,</span><br><span class="line"> job string,</span><br><span class="line"> mgr int,</span><br><span class="line"> hiredate string,</span><br><span class="line"> sal double,</span><br><span class="line"> comm double,</span><br><span class="line"> deptno int)</span><br><span class="line"> row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.167 seconds</span><br></pre></td></tr></table></figure>

<p>（3）查看创建的表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">dept</span><br><span class="line">emp</span><br><span class="line">student</span><br><span class="line">student2</span><br><span class="line">student3</span><br><span class="line">Time taken: 0.06 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure>

<p>（4）查看表格式化数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; desc formatted emp;</span><br></pre></td></tr></table></figure>

<p>（5）删除外部表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept;</span><br></pre></td></tr></table></figure>

<p>外部表删除后，hdfs 中的数据还在，但是 metadata 中 dept 的元数据已被删除。</p>
<h3 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h3><p>（1）查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted student2;</span><br><span class="line"><span class="keyword">Table</span> Type:  MANAGED_TABLE </span><br></pre></td></tr></table></figure>

<p>（2）修改内部表 student2 为外部表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;TRUE&#x27;</span>);</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.163</span> seconds</span><br></pre></td></tr></table></figure>

<p>（3）查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted student2;</span><br><span class="line"><span class="keyword">Table</span> Type:  EXTERNAL_TABLE </span><br></pre></td></tr></table></figure>

<p>（4）修改外部表 student2 为内部表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;FALSE&#x27;</span>);</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.189</span> seconds</span><br></pre></td></tr></table></figure>

<p>（5）查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted student2;</span><br><span class="line"><span class="keyword">Table</span> Type:  MANAGED_TABLE </span><br></pre></td></tr></table></figure>

<p><strong>注意：(‘EXTERNAL’&#x3D;’TRUE’)和(‘EXTERNAL’&#x3D;’FALSE’)为固定写法，区分大小写！</strong></p>
<p><strong>如果需要修改分隔符</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> location <span class="keyword">SET</span> SERDEPROPERTIES (<span class="string">&#x27;field.delim&#x27;</span><span class="operator">=</span> <span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;serialization.format&#x27;</span><span class="operator">=</span><span class="string">&#x27;&#x27;</span>);</span><br></pre></td></tr></table></figure>



<h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><p>修改表的语法基本是 alter table name (你要对表的操作)</p>
<h3 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h3><blockquote>
<p>1）语法</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）实操案例</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition2 rename <span class="keyword">to</span> dept_partition3;</span><br></pre></td></tr></table></figure>



<h3 id="增加-x2F-修改-x2F-替换列信息"><a href="#增加-x2F-修改-x2F-替换列信息" class="headerlink" title="增加&#x2F;修改&#x2F;替换列信息"></a>增加&#x2F;修改&#x2F;替换列信息</h3><blockquote>
<p>1）语法</p>
</blockquote>
<p>（1）更新列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name </span><br><span class="line">column_type [COMMENT col_comment][<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name]</span><br></pre></td></tr></table></figure>

<p>（2）增加和替换列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span><span class="operator">|</span>REPLACE COLUMNS (col_name data_type [COMMENT col_comment],...)</span><br></pre></td></tr></table></figure>

<p>注：<strong>ADD</strong>是代表新增一字段，字段位置在所有列后面(partition 列前)，<strong>REPLACE</strong>则是表示替换表中所有字段。</p>
<blockquote>
<p>2）实操案例</p>
</blockquote>
<p>（1）查询表结构</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">desc</span> dept; </span><br></pre></td></tr></table></figure>

<p>（2）添加列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept <span class="keyword">add</span> columns(deptdesc string);</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.359</span> seconds</span><br></pre></td></tr></table></figure>



<p>（3）查询表结构</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">desc</span> dept;</span><br></pre></td></tr></table></figure>



<p>（4）更新列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept change <span class="keyword">column</span> deptdesc <span class="keyword">desc</span> string;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.265</span> seconds</span><br></pre></td></tr></table></figure>



<p>（5）查询表结构</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">desc</span> dept; </span><br></pre></td></tr></table></figure>

<p>（6）替换列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept replace columns(deptno string, dname string, loc string); </span><br></pre></td></tr></table></figure>

<p>（7）查询表结构</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">desc</span> dept;</span><br><span class="line">OK</span><br><span class="line">col_name  data_type  comment</span><br><span class="line">deptno  string </span><br><span class="line">dname  string </span><br><span class="line">loc  string </span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.08</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> dept;</span><br></pre></td></tr></table></figure>



<h1 id="第5-章-DML-数据操作"><a href="#第5-章-DML-数据操作" class="headerlink" title="第5 章 DML 数据操作"></a>第5 章 DML 数据操作</h1><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><h3 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h3><blockquote>
<p>1）语法</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> load data [<span class="keyword">local</span>] inpath <span class="string">&#x27;数据的 path&#x27;</span>[overwrite] <span class="keyword">into</span> <span class="keyword">table</span> student [<span class="keyword">partition</span> (partcol1<span class="operator">=</span>val1,…)];</span><br></pre></td></tr></table></figure>

<p>（1）load data:表示加载数据</p>
<p>（2）local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表</p>
<p>（3）inpath:表示加载数据的路径</p>
<p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p>
<p>（5）into table:表示加载到哪张表</p>
<p>（6）student:表示具体的表</p>
<p>（7）partition:表示上传到指定分区</p>
<blockquote>
<p>2）实操案例</p>
</blockquote>
<p>（0）创建一张表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(id string, name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>（1）加载本地文件到 hive</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> hive3.student;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.student</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.007</span> seconds</span><br></pre></td></tr></table></figure>

<p><strong>查询结果：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br><span class="line">OK</span><br><span class="line">student.id  student.name</span><br><span class="line"><span class="number">1001</span>  ss1</span><br><span class="line"><span class="number">1002</span>  ss2</span><br><span class="line"><span class="number">1003</span>  ss3</span><br><span class="line"><span class="number">1004</span>  ss4</span><br><span class="line"><span class="number">1005</span>  ss5</span><br><span class="line"><span class="number">1006</span>  ss6</span><br><span class="line"><span class="number">1007</span>  ss7</span><br><span class="line"><span class="number">1008</span>  ss8</span><br><span class="line"><span class="number">1009</span>  ss9</span><br><span class="line"><span class="number">1010</span>  ss10</span><br><span class="line"><span class="number">1011</span>  ss11</span><br><span class="line"><span class="number">1012</span>  ss12</span><br><span class="line"><span class="number">1013</span>  ss13</span><br><span class="line"><span class="number">1014</span>  ss14</span><br><span class="line"><span class="number">1015</span>  ss15</span><br><span class="line"><span class="number">1016</span>  ss16</span><br><span class="line"><span class="number">1001</span>  zzz</span><br><span class="line"><span class="number">1002</span>  ddd</span><br><span class="line"><span class="number">1111</span>  ccc</span><br></pre></td></tr></table></figure>

<p>（2）加载 HDFS 文件到 hive 中</p>
<p>上传文件到 HDFS </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hive<span class="operator">/</span>data<span class="operator">/</span>student.txt <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>atguigu<span class="operator">/</span>hive; </span><br></pre></td></tr></table></figure>

<p>加载 HDFS 上数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data inpath <span class="string">&#x27;/user/atguigu/hive/student.txt&#x27;</span> <span class="keyword">into</span>  <span class="keyword">table</span> default.student;</span><br></pre></td></tr></table></figure>

<p>（3）加载数据覆盖表中已有的数据</p>
<p>上传文件到 HDFS </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>data<span class="operator">/</span>student.txt <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>atguigu<span class="operator">/</span>hive; </span><br></pre></td></tr></table></figure>

<p>加载数据覆盖表中已有的数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/student.txt&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> hive3.student;</span><br></pre></td></tr></table></figure>

<p>查询覆盖后的信息：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br><span class="line">OK</span><br><span class="line">student.id  student.name</span><br><span class="line"><span class="number">1001</span>  zzz</span><br><span class="line"><span class="number">1002</span>  ddd</span><br><span class="line"><span class="number">1111</span>  ccc</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.747</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h3><p>1）创建一张表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> student_par(id <span class="type">int</span>, name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">3.124</span> seconds</span><br></pre></td></tr></table></figure>

<p>2）基本插入数据，<strong>插入数据会生成MR任务，这样插入的数据会放在最前面</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span>  <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_par  <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;wangwu&#x27;</span>),(<span class="number">2</span>,<span class="string">&#x27;zhaoliu&#x27;</span>);</span><br><span class="line">Automatically selecting <span class="keyword">local</span> <span class="keyword">only</span> mode <span class="keyword">for</span> query</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211217144118_3d15bc1c<span class="operator">-</span>c822<span class="number">-41</span>f3<span class="operator">-</span>a62b<span class="number">-62e1</span>e57fa3a2</span><br><span class="line">Total jobs <span class="operator">=</span> <span class="number">3</span></span><br><span class="line">Launching Job <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">3</span></span><br><span class="line">Number <span class="keyword">of</span> reduce tasks determined <span class="keyword">at</span> compile <span class="type">time</span>: <span class="number">1</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</span><br><span class="line"> <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> limit the maximum number <span class="keyword">of</span> reducers:</span><br><span class="line"> <span class="keyword">set</span> hive.exec.reducers.max<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">set</span> a constant number <span class="keyword">of</span> reducers:</span><br><span class="line"> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line">Job <span class="keyword">running</span> <span class="keyword">in</span><span class="operator">-</span>process (<span class="keyword">local</span> Hadoop)</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-17</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">21</span>,<span class="number">668</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-17</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">22</span>,<span class="number">688</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-17</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">23</span>,<span class="number">694</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span></span><br><span class="line">Ended Job <span class="operator">=</span> job_local1983525660_0001</span><br><span class="line">Stage<span class="number">-4</span> <span class="keyword">is</span> selected <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Stage<span class="number">-3</span> <span class="keyword">is</span> filtered <span class="keyword">out</span> <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Stage<span class="number">-5</span> <span class="keyword">is</span> filtered <span class="keyword">out</span> <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Moving data <span class="keyword">to</span> directory hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>student_par<span class="operator">/</span>.hive<span class="operator">-</span>staging_hive_2021<span class="number">-12</span><span class="number">-17</span>_14<span class="number">-41</span><span class="number">-18</span>_153_8125208710986864882<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>ext<span class="number">-10000</span></span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> default.student_par</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>:  HDFS Read: <span class="number">0</span> HDFS Write: <span class="number">82823431</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">0</span> msec</span><br><span class="line">OK</span><br><span class="line">col1  col2</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">8.727</span> seconds</span><br></pre></td></tr></table></figure>

<p>3）基本模式插入（根据单张表查询结果）</p>
<p>下面将hive3里面的student表的内容overwrite到student_par表格里面</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_par <span class="keyword">select</span> id, name <span class="keyword">from</span> hive3.student;</span><br><span class="line">OK</span><br><span class="line">id  name</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">5.391</span> seconds</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student_par;</span><br><span class="line">OK</span><br><span class="line">student_par.id  student_par.name</span><br><span class="line"><span class="number">1001</span>  zzz</span><br><span class="line"><span class="number">1002</span>  ddd</span><br><span class="line"><span class="number">1111</span>  ccc</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.275</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p>
<p>insert overwrite：会覆盖表中已存在的数据</p>
<p>注意：insert 不支持插入部分字段</p>
<p>4）多表（多分区）插入模式（根据多张表查询结果）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line"> insert overwrite table student partition(month=<span class="string">&#x27;201707&#x27;</span>)</span><br><span class="line"> <span class="keyword">select</span> <span class="built_in">id</span>, name <span class="built_in">where</span> month=<span class="string">&#x27;201709&#x27;</span></span><br><span class="line"> insert overwrite table student partition(month=<span class="string">&#x27;201706&#x27;</span>)</span><br><span class="line"> <span class="keyword">select</span> <span class="built_in">id</span>, name <span class="built_in">where</span> month=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="查询语句中创建表并加载数据（As-Select）"><a href="#查询语句中创建表并加载数据（As-Select）" class="headerlink" title="查询语句中创建表并加载数据（As Select）"></a>查询语句中创建表并加载数据（As Select）</h3><p>详见4.5.1 章创建表。根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student1 <span class="keyword">as</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> hive3.student;</span><br><span class="line">OK</span><br><span class="line">id  name</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">3.2</span> seconds</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student1;</span><br><span class="line">OK</span><br><span class="line">student1.id  student1.name</span><br><span class="line"><span class="number">1001</span>  zzz</span><br><span class="line"><span class="number">1002</span>  ddd</span><br><span class="line"><span class="number">1111</span>  ccc</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.231</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>





<h3 id="创建表时通过-Location-指定加载数据路径"><a href="#创建表时通过-Location-指定加载数据路径" class="headerlink" title="创建表时通过 Location 指定加载数据路径"></a>创建表时通过 Location 指定加载数据路径</h3><blockquote>
<p>  1）上传数据到 hdfs 上</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">/</span>student;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>student.txt <span class="operator">/</span>student;</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217152033994.png" alt="image-20211217152033994"></p>
<blockquote>
<p>2）创建表，并指定在 hdfs 上的位置(最好使用外部表)</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student5(id <span class="type">int</span>, name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;&quot; location <span class="string">&#x27;/student&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.482</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）查询数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student5;</span><br><span class="line">OK</span><br><span class="line">student5.id  student5.name</span><br><span class="line"><span class="number">1001</span>  zzz</span><br><span class="line"><span class="number">1002</span>  ddd</span><br><span class="line"><span class="number">1111</span>  ccc</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.45</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="Import-数据到指定-Hive-表中"><a href="#Import-数据到指定-Hive-表中" class="headerlink" title="Import 数据到指定 Hive 表中"></a>Import 数据到指定 Hive 表中</h3><p>注意：先用 export 导出后，再将数据导入（不然会报非法路径的错误）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student5 from &#x27;/user/hive/warehouse/student/student.txt&#x27;;</span><br></pre></td></tr></table></figure>



<h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><h3 id="Insert-导出"><a href="#Insert-导出" class="headerlink" title="Insert 导出"></a>Insert 导出</h3><blockquote>
<p>1）将查询的结果导出到本地</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/hive/data/export/student&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student5;</span><br><span class="line">Automatically selecting <span class="keyword">local</span> <span class="keyword">only</span> mode <span class="keyword">for</span> query</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211217153118_31119102<span class="operator">-</span>f06a<span class="number">-4313</span><span class="operator">-</span>a1c7<span class="operator">-</span>c99c89d5f549</span><br><span class="line">Total jobs <span class="operator">=</span> <span class="number">1</span></span><br><span class="line">Launching Job <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">1</span></span><br><span class="line">Number <span class="keyword">of</span> reduce tasks <span class="keyword">is</span> <span class="keyword">set</span> <span class="keyword">to</span> <span class="number">0</span> since there<span class="string">&#x27;s no reduce operator</span></span><br><span class="line"><span class="string">Job running in-process (local Hadoop)</span></span><br><span class="line"><span class="string">2021-12-17 15:31:21,767 Stage-1 map = 100%, reduce = 0%</span></span><br><span class="line"><span class="string">Ended Job = job_local2085445374_0004</span></span><br><span class="line"><span class="string">Moving data to local directory /opt/module/hive/data/export/student</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1:  HDFS Read: 767 HDFS Write: 41412249 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 0 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">student5.id  student5.name</span></span><br><span class="line"><span class="string">Time taken: 2.922 seconds</span></span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217153255317.png" alt="image-20211217153255317"></p>
<blockquote>
<p>2）将查询的结果格式化导出到本地（加上一个以“,”隔开数据的格式）</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/hive/data/export/student1&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）将查询的结果导出到 HDFS 上(没有 local)</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/user/atguigu/student2&#x27;</span></span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>



<h3 id="Hadoop-命令导出到本地"><a href="#Hadoop-命令导出到本地" class="headerlink" title="Hadoop 命令导出到本地"></a>Hadoop 命令导出到本地</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span><span class="keyword">get</span> <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>student<span class="operator">/</span>student.txt <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>data<span class="operator">/</span>export<span class="operator">/</span>student3.txt;</span><br></pre></td></tr></table></figure>



<h3 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h3><p>基本语法：（hive -f&#x2F;-e 执行语句或者脚本&gt; file）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu<span class="variable">@hadoop102</span> hive]$ bin<span class="operator">/</span>hive <span class="operator">-</span>e <span class="string">&#x27;select * from default.student;&#x27;</span> opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hive<span class="operator">/</span>data<span class="operator">/</span>export<span class="operator">/</span>student4.txt;</span><br></pre></td></tr></table></figure>



<h3 id="Export-导出到-HDFS-上"><a href="#Export-导出到-HDFS-上" class="headerlink" title="Export 导出到 HDFS 上"></a>Export 导出到 HDFS 上</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> export <span class="keyword">table</span> default.student5 <span class="keyword">to</span> <span class="string">&#x27;/student/export/student&#x27;</span>;</span><br><span class="line">FAILED: Execution Error, <span class="keyword">return</span> code <span class="number">1</span> <span class="keyword">from</span> org.apache.hadoop.hive.ql.exec.ExportTask. Cannot <span class="keyword">copy</span> hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span>student<span class="operator">/</span>export <span class="keyword">to</span> its subdirectory hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span>student<span class="operator">/</span>export<span class="operator">/</span>student<span class="operator">/</span>data<span class="operator">/</span>export</span><br></pre></td></tr></table></figure>

<p><strong>导出的数据中有两个数据源，其中除了主信息之外，还包括记录主数据信息的元数据</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217154617218.png" alt="image-20211217154617218"></p>
<p>export 和 import 主要用于两个 Hadoop 平台集群之间 Hive 表迁移。</p>
<p><strong>我们尝试使用import导入上面所产生的信息，但是导入已存在的表发现报错了</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> import <span class="keyword">table</span> student5 <span class="keyword">from</span> <span class="string">&#x27;/student/export/student&#x27;</span>;</span><br><span class="line">FAILED: SemanticException [Error <span class="number">10119</span>]: <span class="keyword">Table</span> <span class="keyword">exists</span> <span class="keyword">and</span> <span class="keyword">contains</span> data files</span><br></pre></td></tr></table></figure>

<p>我们创建一个新表来进行导入</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student6 from <span class="string">&#x27;/student/export/student&#x27;</span>;</span><br><span class="line">Copying data from hdfs://hadoop100:8020/student/export/student/data</span><br><span class="line">Copying file: hdfs://hadoop100:8020/student/export/student/data/student.txt</span><br><span class="line">Loading data to table default.student6</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.984 seconds</span><br></pre></td></tr></table></figure>

<p>我们查询数据：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; <span class="keyword">select</span> * from student6;</span><br><span class="line">OK</span><br><span class="line">student6.id  student6.name</span><br><span class="line">1001  zzz</span><br><span class="line">1002  ddd</span><br><span class="line">1111  ccc</span><br><span class="line">Time taken: 0.351 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>

<p><strong>结论：导入的表需要是一张没收数据的表，也就是说该表要么不存在，要么就是一张空表。</strong></p>
<h3 id="Sqoop-导出"><a href="#Sqoop-导出" class="headerlink" title="Sqoop 导出"></a>Sqoop 导出</h3><p>后续课程专门讲。主要作用就是将数据导入mysql</p>
<h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate 只能删除管理表，不能删除外部表中数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">truncate</span> <span class="keyword">table</span> student6;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.943</span> seconds</span><br></pre></td></tr></table></figure>



<h1 id="第6-章查询"><a href="#第6-章查询" class="headerlink" title="第6 章查询"></a>第6 章查询</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a> </p>
<p>查询语句语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr,...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line"><span class="operator">|</span>[DISTRIBUTE <span class="keyword">BY</span> col_list][SORT <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[LIMIT number]</span><br></pre></td></tr></table></figure>



<h2 id="基本查询（Select…From）"><a href="#基本查询（Select…From）" class="headerlink" title="基本查询（Select…From）"></a>基本查询（Select…From）</h2><h3 id="全表和特定列查询"><a href="#全表和特定列查询" class="headerlink" title="全表和特定列查询"></a>全表和特定列查询</h3><blockquote>
<p>0）数据准备</p>
</blockquote>
<p>（0）原始数据</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dept:</span><br><span class="line">10 ACCOUNTING 1700</span><br><span class="line">20 RESEARCH 1800</span><br><span class="line">30 SALES 1900</span><br><span class="line">40 OPERATIONS 1700</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">emp：</span><br><span class="line">7369 SMITH CLERK 7902 1980-12-17 800.00 20</span><br><span class="line">7499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 30</span><br><span class="line">7521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 30</span><br><span class="line">7566 JONES MANAGER 7839 1981-4-2 2975.00  20</span><br><span class="line">7654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 30</span><br><span class="line">7698 BLAKE MANAGER 7839 1981-5-1 2850.00 30</span><br><span class="line">7782 CLARK MANAGER 7839 1981-6-9 2450.00 10</span><br><span class="line">7788 SCOTT ANALYST 7566 1987-4-19 3000.00 20</span><br><span class="line">7839 KING PRESIDENT 8888 1981-11-17 5000.00 10</span><br><span class="line">7844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 30</span><br><span class="line">7876 ADAMS CLERK 7788 1987-5-23 1100.00 20</span><br><span class="line">7900 JAMES CLERK 7698 1981-12-3 950.00 30</span><br><span class="line">7902 FORD ANALYST 7566 1981-12-3 3000.00 20</span><br><span class="line">7934 MILLER CLERK 7782 1982-1-23 1300.00 10</span><br></pre></td></tr></table></figure>

<p>（1）创建部门表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">deptno <span class="type">int</span>,</span><br><span class="line">dname string,</span><br><span class="line">loc <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>（2）创建员工表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">empno <span class="type">int</span>,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr <span class="type">int</span>,</span><br><span class="line">hiredate string,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>（3）导入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/dept.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/emp.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）全表查询</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">from</span>  emp ;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）选择特定列查询</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>（1）SQL 语言大小写不敏感。</p>
<p>（2）SQL 可以写在一行或者多行</p>
<p>（3）关键字不能被缩写也不能分行</p>
<p>（4）各子句一般要分行写。</p>
<p>（5）使用缩进提高语句的可读性。</p>
<h3 id="列别名"><a href="#列别名" class="headerlink" title="列别名"></a>列别名</h3><p>1）重命名一个列</p>
<p>2）便于计算</p>
<p>3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’</p>
<p>4）案例实操</p>
<p>查询名称和部门</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">as</span> name,deptno <span class="keyword">as</span> dept <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217170700326.png" alt="image-20211217170700326"></p>
<p>案例实操：查询出所有员工的薪水后加1 显示。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> mgr<span class="operator">+</span><span class="number">100</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><blockquote>
<p>1）求总行数（count），这个有MR生成</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) cnt <span class="keyword">from</span> emp; </span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）求工资的最大值（max），这个有MR生成</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">max</span>(mgr) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）求工资的最小值（min）这个有MR生成</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">min</span>(mgr) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>4）求工资的总和（sum）这个有MR生成</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">sum</span>(mgr) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>5）求工资的平均值（avg）这个有MR生成</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">avg</span>(mgr) avg_sql <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<h3 id="Limit-语句"><a href="#Limit-语句" class="headerlink" title="Limit 语句"></a>Limit 语句</h3><p>典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">5</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">2</span>;</span><br></pre></td></tr></table></figure>



<h3 id="Where-语句"><a href="#Where-语句" class="headerlink" title="Where 语句"></a>Where 语句</h3><blockquote>
<p>1）使用 WHERE 子句，将不满足条件的行过滤掉</p>
<p>2）WHERE 子句紧随 FROM 子句</p>
<p>3）案例实操</p>
</blockquote>
<p>查询出薪水大于1000 的所有员工</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="operator">&gt;</span><span class="number">1000</span>; </span><br></pre></td></tr></table></figure>

<p>注意：where 子句中不能使用字段别名。</p>
<h3 id="比较运算符（Between-x2F-In-x2F-Is-Null）"><a href="#比较运算符（Between-x2F-In-x2F-Is-Null）" class="headerlink" title="比较运算符（Between&#x2F;In&#x2F; Is Null）"></a>比较运算符（Between&#x2F;In&#x2F; Is Null）</h3><blockquote>
<p>1）下面表中描述了谓词操作符，这些操作符同样可以用于 JOIN…ON 和 HAVING 语句中。</p>
</blockquote>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A&#x3D;B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE,反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;&#x3D;&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，如果一边为NULL,返回False</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!&#x3D;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL;如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL;如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;&#x3D;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL,则返回NULL;如果A小于等于B,则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL;如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;&#x3D;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL,则返回NULL;如果A大于等于B,则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。<br/>如果A的值大于等于B而且小于或等于C,则结果为TRUE,反之为FALSE。<br/>如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>IN(数值1,数值2)</td>
<td>所有数据类型</td>
<td>使用IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>string类型</td>
<td>B是一个SQL下的简单正则表达式，也叫通配符模式，<br/>如果A与其匹配的话，则返回TRUE;反之返回FALSE。<br/>B的表达式说明如下: ‘x%’表示A必须以字母’x’开头，<br/>‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,<br/>可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>string类型</td>
<td>B 是基于 java 的正则表达式，如果 A 与其匹配，<br/>则返回 TRUE；反之返回 FALSE。<br/>匹配使用的是 JDK 中的正则表达式接口实现的，<br/>因为正则也依据其中的规则。<br/>例如，正则表达式必须和整个字符串 A 相匹配，<br/>而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
<blockquote>
<p>2）案例实操</p>
</blockquote>
<p>（1）查询出薪水等于5000 的所有员工</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> mgr <span class="operator">=</span> <span class="number">7902</span>;</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7369</span>  SMITH  CLERK <span class="number">7902</span> <span class="number">1980</span><span class="number">-12</span><span class="number">-17</span> <span class="number">800.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.285</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（2）查询工资在500 到1000 的员工信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> mgr <span class="keyword">between</span> <span class="number">7500</span> <span class="keyword">and</span> <span class="number">7700</span>;</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7499</span>  ALLEN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span> <span class="number">1600.0</span> <span class="number">300.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7521</span>  WARD  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span> <span class="number">1250.0</span> <span class="number">500.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7654</span>  MARTIN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span> <span class="number">1250.0</span> <span class="number">1400.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7788</span>  SCOTT  ANALYST <span class="number">7566</span> <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span> <span class="number">3000.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7844</span>  TURNER  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span> <span class="number">1500.0</span> <span class="number">0.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7900</span>  JAMES  CLERK <span class="number">7698</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span> <span class="number">950.0</span> <span class="number">30.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7902</span>  FORD  ANALYST <span class="number">7566</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span> <span class="number">3000.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.576</span> seconds, Fetched: <span class="number">7</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<p>（3）查询 comm 为空的所有员工信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="keyword">null</span>;</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7839</span>  KING  PRESIDENT  <span class="keyword">NULL</span> <span class="number">5000.00</span> <span class="number">10.0</span>  <span class="keyword">NULL</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.229</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<p>（4）查询工资是1500 或5000 的员工信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">in</span> (<span class="number">1500</span>,<span class="number">2600</span>);</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7844</span>  TURNER  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span> <span class="number">1500.0</span> <span class="number">0.0</span> <span class="number">30</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.176</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="Like-和-RLike"><a href="#Like-和-RLike" class="headerlink" title="Like 和 RLike"></a>Like 和 RLike</h3><blockquote>
<p>1）<strong>使用 LIKE 运算选择类似的值</strong></p>
<p>2）<strong>选择条件可以包含字符或数字</strong>: %代表零个或多个字符(任意个字符)。 _ 代表一个字符。</p>
<p>3）<strong>RLIKE</strong>子句 RLIKE 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件。</p>
</blockquote>
<p>4）案例实操</p>
<p>（1）查找名字以 A 开头的员工信息 hive (default)&gt; select * from emp where ename LIKE ‘A%’; </p>
<p>（2）查找名字中第二个字母为 A 的员工信息 hive (default)&gt; select * from emp where ename LIKE ‘_A%’; </p>
<p>（3）查找名字中带有 A 的员工信息 hive (default)&gt; select * from emp where ename RLIKE ‘[A]’;</p>
<h3 id="逻辑运算符（And-x2F-Or-x2F-Not）"><a href="#逻辑运算符（And-x2F-Or-x2F-Not）" class="headerlink" title="逻辑运算符（And&#x2F;Or&#x2F;Not）"></a>逻辑运算符（And&#x2F;Or&#x2F;Not）</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211217181838668.png" alt="image-20211217181838668"></p>
<blockquote>
<p>1）案例实操</p>
</blockquote>
<p>（1）查询薪水大于1000，部门是30</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">and</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7499</span>  ALLEN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span> <span class="number">1600.0</span> <span class="number">300.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7521</span>  WARD  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span> <span class="number">1250.0</span> <span class="number">500.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7654</span>  MARTIN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span> <span class="number">1250.0</span> <span class="number">1400.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7844</span>  TURNER  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span> <span class="number">1500.0</span> <span class="number">0.0</span> <span class="number">30</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.696</span> seconds, Fetched: <span class="number">4</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（2）查询薪水大于1000，或者部门是30</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">or</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7499</span>  ALLEN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span> <span class="number">1600.0</span> <span class="number">300.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7521</span>  WARD  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span> <span class="number">1250.0</span> <span class="number">500.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7566</span>  JONES  MANAGER <span class="number">7839</span> <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span> <span class="number">2975.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7654</span>  MARTIN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span> <span class="number">1250.0</span> <span class="number">1400.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7698</span>  BLAKE  MANAGER <span class="number">7839</span> <span class="number">1981</span><span class="number">-5</span><span class="number">-1</span> <span class="number">2850.0</span> <span class="number">30.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7782</span>  CLARK  MANAGER <span class="number">7839</span> <span class="number">1981</span><span class="number">-6</span><span class="number">-9</span> <span class="number">2450.0</span> <span class="number">10.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7788</span>  SCOTT  ANALYST <span class="number">7566</span> <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span> <span class="number">3000.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7844</span>  TURNER  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span> <span class="number">1500.0</span> <span class="number">0.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7876</span>  ADAMS  CLERK <span class="number">7788</span> <span class="number">1987</span><span class="number">-5</span><span class="number">-23</span> <span class="number">1100.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7902</span>  FORD  ANALYST <span class="number">7566</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span> <span class="number">3000.0</span> <span class="number">20.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">7934</span>  MILLER  CLERK <span class="number">7782</span> <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span> <span class="number">1300.0</span> <span class="number">10.0</span>  <span class="keyword">NULL</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.22</span> seconds, Fetched: <span class="number">11</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（3）查询除了20 部门和30 部门以外的员工信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> <span class="keyword">not</span> deptno<span class="operator">=</span><span class="number">20</span> <span class="keyword">or</span> <span class="keyword">not</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br><span class="line">OK</span><br><span class="line">emp.empno  emp.ename  emp.job emp.mgr emp.hiredate  emp.sal emp.comm  emp.deptno</span><br><span class="line"><span class="number">7499</span>  ALLEN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span> <span class="number">1600.0</span> <span class="number">300.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7521</span>  WARD  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span> <span class="number">1250.0</span> <span class="number">500.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7654</span>  MARTIN  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span> <span class="number">1250.0</span> <span class="number">1400.0</span> <span class="number">30</span></span><br><span class="line"><span class="number">7844</span>  TURNER  SALESMAN <span class="number">7698</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span> <span class="number">1500.0</span> <span class="number">0.0</span> <span class="number">30</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.193</span> seconds, Fetched: <span class="number">4</span> <span class="type">row</span>(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h3 id="Group-By-语句"><a href="#Group-By-语句" class="headerlink" title="Group By 语句"></a>Group By 语句</h3><p>GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<blockquote>
<p>1）案例实操：</p>
</blockquote>
<p>（1）计算 emp 表每个部门的平均工资</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno,<span class="built_in">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>



<p>（2）计算 emp 每个部门中平均薪资大于2000，及部门平均薪资。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno,<span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal <span class="operator">&gt;</span> <span class="number">1500</span>; </span><br><span class="line">#另一种低效的写法</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno,avg_sal <span class="keyword">from</span>(<span class="keyword">select</span> deptno,<span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno) t1 <span class="keyword">where</span> avg_sal<span class="operator">&gt;</span><span class="number">1500</span>;</span><br></pre></td></tr></table></figure>



<p>（2）计算 emp 每个部门中每个岗位的最高薪水</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno, t.job, <span class="built_in">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno, t.job;</span><br></pre></td></tr></table></figure>



<h3 id="Having-语句"><a href="#Having-语句" class="headerlink" title="Having 语句"></a>Having 语句</h3><blockquote>
<p>1）having 与 where 不同点</p>
</blockquote>
<p>（1）where 后面不能写分组函数，而 having 后面可以使用分组函数。</p>
<p>（2）having 只用于 group by 分组统计语句。</p>
<p>2）案例实操</p>
<p>（1）求每个部门的平均薪水大于2000 的部门</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno  <span class="keyword">having</span> avg_sal <span class="operator">&gt;</span> <span class="number">2000</span>;</span><br></pre></td></tr></table></figure>



<h2 id="Join-语句"><a href="#Join-语句" class="headerlink" title="Join 语句"></a>Join 语句</h2><h3 id="等值-Join"><a href="#等值-Join" class="headerlink" title="等值 Join"></a>等值 Join</h3><p>Hive 支持通常的 SQL JOIN 语句。</p>
<blockquote>
<p>1）案例实操</p>
</blockquote>
<p>（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.empno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<h3 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h3><blockquote>
<p>1）好处</p>
</blockquote>
<p>（1）使用别名可以简化查询。</p>
<p>（2）使用表名前缀可以提高执行效率。</p>
<blockquote>
<p>2）案例实操</p>
</blockquote>
<p>合并员工表和部门表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> e.ename,e.empno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<h3 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h3><p>左外连接：JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> t.dname,e.ename,e.empno,e.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept t <span class="keyword">on</span> e.deptno <span class="operator">=</span> t.deptno;</span><br></pre></td></tr></table></figure>



<h3 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h3><p>右外连接：JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<h3 id="满外连接"><a href="#满外连接" class="headerlink" title="满外连接"></a>满外连接</h3><p>满外连接：将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>注意：连接 n 个表，至少需要 n-1 个连接条件。例如：连接三个表，至少需要两个连接条件。数据准备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1700  Beijing</span><br><span class="line">1800  London</span><br><span class="line">1900  Tokyo</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）创建位置表</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> location(</span><br><span class="line">loc <span class="type">int</span>,</span><br><span class="line">loc_name string</span><br><span class="line">)<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）多表连接查询（查询员工姓名，部门名称，部门所在地）</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,d.dname,l.loc_name <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno <span class="keyword">join</span> location l <span class="keyword">on</span> d.loc <span class="operator">=</span> l.loc;</span><br></pre></td></tr></table></figure>



<p>大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。本例中会首先启动一个 MapReduce job 对表 e 和表 d 进行连接操作，然后会再启动一个 MapReduce job 将第一个 MapReduce job 的输出和表 l;进行连接操作。</p>
<p>注意：为什么不是表 d 和表 l 先进行连接操作呢？</p>
<p>这是因为 Hive 总是按照从左到右的顺序执行的。优化：当对3 个或者更多表进行 join 连接时，如果每个 on 子句都使用相同的连接键的话，那么只会产生一个 MapReduce job。</p>
<h3 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><blockquote>
<p>1）笛卡尔集会在下面条件下产生</p>
</blockquote>
<p>（1）省略连接条件</p>
<p>（2）连接条件无效</p>
<p>（3）所有表中的所有行互相连接</p>
<blockquote>
<p>2）案例实操</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> e.ename,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">26.495</span> seconds, Fetched: <span class="number">56</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h3><p>Order By：全局排序，只有一个 Reducer</p>
<blockquote>
<p>1）使用 ORDER BY 子句排序</p>
</blockquote>
<p>ASC（ascend）: 升序（默认）</p>
<p>DESC（descend）: 降序</p>
<blockquote>
<p>2）ORDER BY 子句在 SELECT 语句的结尾</p>
</blockquote>
<blockquote>
<p>3）案例实操</p>
</blockquote>
<p>（1）查询员工信息按工资升序排列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> e.ename,e.sal <span class="keyword">from</span> emp e <span class="keyword">order</span> <span class="keyword">by</span> e.sal;</span><br><span class="line">e.ename e.sal</span><br><span class="line">KING  <span class="keyword">NULL</span></span><br><span class="line">SMITH <span class="number">800.0</span></span><br><span class="line">JAMES <span class="number">950.0</span></span><br><span class="line">ADAMS <span class="number">1100.0</span></span><br><span class="line">WARD <span class="number">1250.0</span></span><br><span class="line">MARTIN <span class="number">1250.0</span></span><br><span class="line">MILLER <span class="number">1300.0</span></span><br><span class="line">TURNER <span class="number">1500.0</span></span><br><span class="line">ALLEN <span class="number">1600.0</span></span><br><span class="line">CLARK <span class="number">2450.0</span></span><br><span class="line">BLAKE <span class="number">2850.0</span></span><br><span class="line">JONES <span class="number">2975.0</span></span><br><span class="line">SCOTT <span class="number">3000.0</span></span><br><span class="line">FORD <span class="number">3000.0</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">4.87</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（2）查询员工信息按工资降序排列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> e.ename,e.sal <span class="keyword">from</span> emp e <span class="keyword">order</span> <span class="keyword">by</span> e.sal <span class="keyword">desc</span>;</span><br><span class="line">e.ename e.sal</span><br><span class="line">FORD <span class="number">3000.0</span></span><br><span class="line">SCOTT <span class="number">3000.0</span></span><br><span class="line">JONES <span class="number">2975.0</span></span><br><span class="line">BLAKE <span class="number">2850.0</span></span><br><span class="line">CLARK <span class="number">2450.0</span></span><br><span class="line">ALLEN <span class="number">1600.0</span></span><br><span class="line">TURNER <span class="number">1500.0</span></span><br><span class="line">MILLER <span class="number">1300.0</span></span><br><span class="line">MARTIN <span class="number">1250.0</span></span><br><span class="line">WARD <span class="number">1250.0</span></span><br><span class="line">ADAMS <span class="number">1100.0</span></span><br><span class="line">JAMES <span class="number">950.0</span></span><br><span class="line">SMITH <span class="number">800.0</span></span><br><span class="line">KING  <span class="keyword">NULL</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">2.08</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h3><p>按照员工薪水的2 倍排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">select</span> ename, sal<span class="operator">*</span><span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal;</span><br></pre></td></tr></table></figure>



<h3 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h3><p>按照部门和工资升序排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> e.deptno,e.sal,e.ename <span class="keyword">from</span> emp e <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal;</span><br></pre></td></tr></table></figure>





<h3 id="每个-Reduce-内部排序（Sort-By）重点"><a href="#每个-Reduce-内部排序（Sort-By）重点" class="headerlink" title="每个 Reduce 内部排序（Sort By）重点"></a>每个 Reduce 内部排序（Sort By）重点</h3><p>Sort By：对于大规模的数据集 order by 的效率非常低。在很多情况下，并不需要全局排序，此时可以使用 sort by。</p>
<p>Sort by 为每个 reducer 产生一个排序文件。每个 Reducer 内部进行排序，对全局结果集来说不是排序。</p>
<blockquote>
<p>1）设置 reduce 个数</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）查看设置 reduce 个数</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）根据部门编号降序查看员工信息</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> e.ename,e.deptno <span class="keyword">from</span> emp e sort <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br><span class="line">e.ename e.deptno</span><br><span class="line">TURNER <span class="number">30</span></span><br><span class="line">BLAKE <span class="number">30</span></span><br><span class="line">MARTIN <span class="number">30</span></span><br><span class="line">SCOTT <span class="number">20</span></span><br><span class="line">CLARK <span class="number">10</span></span><br><span class="line">KING  <span class="keyword">NULL</span></span><br><span class="line">WARD <span class="number">30</span></span><br><span class="line">ALLEN <span class="number">30</span></span><br><span class="line">JAMES <span class="number">30</span></span><br><span class="line">ADAMS <span class="number">20</span></span><br><span class="line">JONES <span class="number">20</span></span><br><span class="line">MILLER <span class="number">10</span></span><br><span class="line">FORD <span class="number">20</span></span><br><span class="line">SMITH <span class="number">20</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">34.805</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>上面的数据整体上看上去不是按照排序来排序的，是因为他们是在3个MR中进行的内部排序，当全部合拢在一起时又不是排序的了。</p>
<blockquote>
<p>4）将查询结果导入到文件中（按照部门编号降序排序）</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/sortby-result&#x27;</span> <span class="keyword">select</span> e.ename,e.deptno <span class="keyword">from</span> emp e sort <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211219114405_5c926e59<span class="number">-440</span>c<span class="number">-4</span>eea<span class="operator">-</span>b11d<span class="number">-51</span>df3b88c7ba</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">3</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">44</span>:<span class="number">16</span>,<span class="number">622</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">44</span>:<span class="number">24</span>,<span class="number">137</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, Cumulative CPU <span class="number">2.03</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">44</span>:<span class="number">33</span>,<span class="number">876</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">33</span><span class="operator">%</span>, Cumulative CPU <span class="number">4.64</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">44</span>:<span class="number">34</span>,<span class="number">975</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">67</span><span class="operator">%</span>, Cumulative CPU <span class="number">6.98</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">44</span>:<span class="number">38</span>,<span class="number">148</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, Cumulative CPU <span class="number">8.97</span> sec</span><br><span class="line">MapReduce Total cumulative CPU <span class="type">time</span>: <span class="number">8</span> seconds <span class="number">970</span> msec</span><br><span class="line">Ended Job <span class="operator">=</span> job_1639880318289_0004</span><br><span class="line">Moving data <span class="keyword">to</span> <span class="keyword">local</span> directory <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>sortby<span class="operator">-</span><span class="keyword">result</span></span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">3</span>  Cumulative CPU: <span class="number">8.97</span> sec  HDFS Read: <span class="number">21354</span> HDFS Write: <span class="number">126</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">8</span> seconds <span class="number">970</span> msec</span><br><span class="line">OK</span><br><span class="line">e.ename e.deptno</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">34.73</span> seconds</span><br></pre></td></tr></table></figure>



<p><strong>查看生成的文件：</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211219114538431.png" alt="image-20211219114538431"></p>
<p><strong>查看文件：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 sortby-result]$ <span class="built_in">cat</span> 000000_0 -n</span><br><span class="line">1  TURNER30</span><br><span class="line">2  BLAKE30</span><br><span class="line">3  MARTIN30</span><br><span class="line">4  SCOTT20</span><br><span class="line">5  CLARK10</span><br><span class="line">6  KING\N</span><br><span class="line">[atguigu@hadoop100 sortby-result]$ <span class="built_in">cat</span> 000001_0 -n </span><br><span class="line">1  WARD30</span><br><span class="line">2  ALLEN30</span><br><span class="line">3  JAMES30</span><br><span class="line">4  ADAMS20</span><br><span class="line">5  JONES20</span><br><span class="line">6  MILLER10</span><br><span class="line">[atguigu@hadoop100 sortby-result]$ <span class="built_in">cat</span> 000002_0 -n </span><br><span class="line">1  FORD20</span><br><span class="line">2  SMITH20</span><br><span class="line">[atguigu@hadoop100 sortby-result]$</span><br></pre></td></tr></table></figure>

<p>可以看到，在文件内部的数据是有序的，也就是进行过排序了。排序过程中，数据是随机拿出来排序的，这是为了<strong>防止数据倾斜</strong>。</p>
<h3 id="分区（Distribute-By）"><a href="#分区（Distribute-By）" class="headerlink" title="分区（Distribute By）"></a>分区（Distribute By）</h3><p>Distribute By：在有些情况下，<strong>我们需要控制某个特定行应该到哪个 reducer</strong>，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by 类似 MR 中 partition （自定义分区），进行分区，结合 sort by 使用。</p>
<p>对于 distribute by 进行测试，一定要分配多 reduce 进行处理，否则无法看到 distribute  by 的效果。</p>
<blockquote>
<p>1）案例实操：</p>
</blockquote>
<p>（1）先按照部门编号分区，再按照员工编号降序排序。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno,ename,sal <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">53</span>:<span class="number">46</span>,<span class="number">343</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">53</span>:<span class="number">56</span>,<span class="number">877</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, Cumulative CPU <span class="number">3.35</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">54</span>:<span class="number">07</span>,<span class="number">429</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">33</span><span class="operator">%</span>, Cumulative CPU <span class="number">5.83</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">54</span>:<span class="number">08</span>,<span class="number">484</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">67</span><span class="operator">%</span>, Cumulative CPU <span class="number">8.31</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">11</span>:<span class="number">54</span>:<span class="number">09</span>,<span class="number">519</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, Cumulative CPU <span class="number">11.05</span> sec</span><br><span class="line">MapReduce Total cumulative CPU <span class="type">time</span>: <span class="number">11</span> seconds <span class="number">50</span> msec</span><br><span class="line">Ended Job <span class="operator">=</span> job_1639880318289_0005</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">3</span>  Cumulative CPU: <span class="number">11.05</span> sec  HDFS Read: <span class="number">23443</span> HDFS Write: <span class="number">647</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">11</span> seconds <span class="number">50</span> msec</span><br><span class="line">OK</span><br><span class="line">deptno  ename  sal</span><br><span class="line"><span class="number">30</span>  BLAKE <span class="number">2850.0</span></span><br><span class="line"><span class="number">30</span>  ALLEN <span class="number">1600.0</span></span><br><span class="line"><span class="number">30</span>  TURNER <span class="number">1500.0</span></span><br><span class="line"><span class="number">30</span>  WARD <span class="number">1250.0</span></span><br><span class="line"><span class="number">30</span>  MARTIN <span class="number">1250.0</span></span><br><span class="line"><span class="number">30</span>  JAMES <span class="number">950.0</span></span><br><span class="line"><span class="keyword">NULL</span>  KING  <span class="keyword">NULL</span></span><br><span class="line"><span class="number">10</span>  CLARK <span class="number">2450.0</span></span><br><span class="line"><span class="number">10</span>  MILLER <span class="number">1300.0</span></span><br><span class="line"><span class="number">20</span>  SCOTT <span class="number">3000.0</span></span><br><span class="line"><span class="number">20</span>  FORD <span class="number">3000.0</span></span><br><span class="line"><span class="number">20</span>  JONES <span class="number">2975.0</span></span><br><span class="line"><span class="number">20</span>  ADAMS <span class="number">1100.0</span></span><br><span class="line"><span class="number">20</span>  SMITH <span class="number">800.0</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">36.518</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p><strong>同样可以输出到本地来进行查看</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/distribute-result&#x27;</span> <span class="keyword">select</span> deptno,ename,sal <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p><strong>使用多个reduce时，hive会退出本地模式</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot run job locally: Number of reducers (= 3) is more than 1</span><br></pre></td></tr></table></figure>



<blockquote>
<p>注意：</p>
<p>➢ distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。</p>
<p>➢ Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。</p>
</blockquote>
<h3 id="Cluster-By（上面两个结合）"><a href="#Cluster-By（上面两个结合）" class="headerlink" title="Cluster By（上面两个结合）"></a>Cluster By（上面两个结合）</h3><p>当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。</p>
<p>cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。<strong>缺点：</strong>但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC。</p>
<blockquote>
<p>（1）以下两种写法等价</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>

<p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20 号和30 号部门分到一个分区里面去。</p>
<h1 id="第7-章分区表和分桶表"><a href="#第7-章分区表和分桶表" class="headerlink" title="第7 章分区表和分桶表"></a>第7 章分区表和分桶表</h1><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<strong>Hive 中的分区就是分目录</strong>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的<strong>查询效率会提高很多</strong>。</p>
<h3 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h3><blockquote>
<p>1）引入分区表（需要根据日期对日志进行管理,通过部门信息模拟）</p>
</blockquote>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211219121205946.png" alt="image-20211219121205946"></p>
<blockquote>
<p>2）创建分区表语法</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_par(deptno <span class="type">int</span> , dname string, loc string) partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span><span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">2.547</span> seconds</span><br></pre></td></tr></table></figure>

<p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p>
<blockquote>
<p>3）加载数据到分区表中</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_par <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.763</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/dept_20200402.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_par <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.962</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/dept_20200403.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_par <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.869</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>4）查找全部数据（可以看到多了个分区字段，但这个字段不是放在表中，而是放在目录上，所以条件查分区的效率会高很多）</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span>  dept_par;</span><br><span class="line">OK</span><br><span class="line">dept_par.deptno dept_par.dname  dept_par.loc  dept_par.day</span><br><span class="line"><span class="number">10</span>  ACCOUNTING <span class="number">1700</span> <span class="number">20200401</span></span><br><span class="line"><span class="number">20</span>  RESEARCH <span class="number">1800</span> <span class="number">20200401</span></span><br><span class="line"><span class="number">30</span>  SALES <span class="number">1900</span> <span class="number">20200402</span></span><br><span class="line"><span class="number">40</span>  OPERATIONS <span class="number">1700</span> <span class="number">20200402</span></span><br><span class="line"><span class="number">50</span>  TEST <span class="number">2000</span> <span class="number">20200403</span></span><br><span class="line"><span class="number">60</span>  DEV <span class="number">1900</span> <span class="number">20200403</span></span><br><span class="line"><span class="keyword">NULL</span>  <span class="keyword">NULL</span>  <span class="keyword">NULL</span> <span class="number">20200403</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">3.684</span> seconds, Fetched: <span class="number">7</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>5）条件查询</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span>  dept_par <span class="keyword">where</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="number">20200401</span>;</span><br><span class="line">OK</span><br><span class="line">dept_par.deptno dept_par.dname  dept_par.loc  dept_par.day</span><br><span class="line"><span class="number">10</span>  ACCOUNTING <span class="number">1700</span> <span class="number">20200401</span></span><br><span class="line"><span class="number">20</span>  RESEARCH <span class="number">1800</span> <span class="number">20200401</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">2.749</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p><strong>另外：分区的信息也存放在mysql的partition表中。</strong></p>
<h3 id="分区的增删改查"><a href="#分区的增删改查" class="headerlink" title="分区的增删改查"></a>分区的增删改查</h3><p>单分区查询</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>多分区联合查询</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span></span><br><span class="line"> <span class="keyword">union</span></span><br><span class="line"> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span></span><br><span class="line"> <span class="keyword">union</span></span><br><span class="line"> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>5）增加分区</p>
</blockquote>
<p>创建单个分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>创建多个分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_par <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.844</span> seconds</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211219141020269.png" alt="image-20211219141020269"></p>
<blockquote>
<p>6）删除分区</p>
</blockquote>
<p>删除单个分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>同时删除多个分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_par <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>),<span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>),<span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br><span class="line">Dropped the <span class="keyword">partition</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200404</span></span><br><span class="line">Dropped the <span class="keyword">partition</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200405</span></span><br><span class="line">Dropped the <span class="keyword">partition</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200406</span></span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.981</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>7）查看分区表有多少分区</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">show</span> partitions dept_par;</span><br><span class="line">OK</span><br><span class="line"><span class="keyword">partition</span></span><br><span class="line"><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span></span><br><span class="line"><span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span></span><br><span class="line"><span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.296</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<blockquote>
<p>8）查看分区表结构</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted dept_par;</span><br><span class="line"># <span class="keyword">Partition</span> Information </span><br><span class="line"># col_name  data_type  comment </span><br><span class="line"><span class="keyword">day</span>  string </span><br></pre></td></tr></table></figure>



<h3 id="二级分区"><a href="#二级分区" class="headerlink" title="二级分区"></a>二级分区</h3><p>思考: 如何一天的日志数据量也很大，如何再将数据拆分?</p>
<blockquote>
<p>1）创建二级分区表</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_par2(deptno <span class="type">int</span> , dname string , loc string) partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string , <span class="keyword">hour</span> string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.172</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）正常的加载数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath &quot;/home/atguigu/hive/dept_20200401.log&quot; <span class="keyword">into</span> <span class="keyword">table</span> dept_par2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;001&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;401&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par2 <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">001</span>, <span class="keyword">hour</span><span class="operator">=</span><span class="number">401</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.221</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath &quot;/home/atguigu/hive/dept_20200402.log&quot; <span class="keyword">into</span> <span class="keyword">table</span> dept_par2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;001&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;402&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par2 <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">001</span>, <span class="keyword">hour</span><span class="operator">=</span><span class="number">402</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.899</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath &quot;/home/atguigu/hive/dept_20200403.log&quot; <span class="keyword">into</span> <span class="keyword">table</span> dept_par2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;001&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;403&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par2 <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">001</span>, <span class="keyword">hour</span><span class="operator">=</span><span class="number">403</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.011</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath &quot;/home/atguigu/hive/dept_20200403.log&quot; <span class="keyword">into</span> <span class="keyword">table</span> dept_par2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;002&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;404&#x27;</span>);</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.dept_par2 <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="number">001</span>, <span class="keyword">hour</span><span class="operator">=</span><span class="number">404</span>)</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.878</span> seconds</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211219142209731.png" alt="image-20211219142209731"></p>
<blockquote>
<p>查询</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_par2 <span class="keyword">where</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;001&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span> <span class="operator">=</span> <span class="string">&#x27;404&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line">dept_par2.deptno  dept_par2.dname dept_par2.loc  dept_par2.day  dept_par2.hour</span><br><span class="line"><span class="number">50</span>  TEST <span class="number">2000</span> <span class="number">001</span> <span class="number">404</span></span><br><span class="line"><span class="number">60</span>  DEV <span class="number">1900</span> <span class="number">001</span> <span class="number">404</span></span><br><span class="line"><span class="keyword">NULL</span>  <span class="keyword">NULL</span>  <span class="keyword">NULL</span> <span class="number">001</span> <span class="number">404</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.692</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>





<blockquote>
<p>3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p>
</blockquote>
<p>（1）方式一：上传数据后修复</p>
<p>上传数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">13</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>dept1.txt <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">13</span>;</span><br></pre></td></tr></table></figure>

<p>查询数据（查询不到刚上传的数据）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;13&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p><strong>让分区信息产生关联的操作：执行修复命令</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> msck repair <span class="keyword">table</span> dept_partition2;</span><br></pre></td></tr></table></figure>

<p>再次查询数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;13&#x27;</span>;</span><br></pre></td></tr></table></figure>



<p>（2）方式二：上传数据后添加分区</p>
<p>上传数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">14</span>; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hive<span class="operator">/</span>datas<span class="operator">/</span>dept_20200401.log <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">14</span>; </span><br></pre></td></tr></table></figure>

<p>执行添加分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition2 <span class="keyword">add</span>  <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;201709&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;14&#x27;</span>); </span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span>  <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;14&#x27;</span>;</span><br></pre></td></tr></table></figure>



<p>（3）方式三：创建文件夹后,使用load </p>
<p>load 数据到分区创建目录</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">15</span>; </span><br></pre></td></tr></table></figure>

<p>上传数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;15&#x27;</span>); </span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span>  <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;15&#x27;</span>;</span><br></pre></td></tr></table></figure>



<h3 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h3><p>关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用 Hive 的动态分区，需要进行相应的配置。</p>
<blockquote>
<p>1）开启动态分区参数设置</p>
</blockquote>
<p>（1）开启动态分区功能（默认 true，开启）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br></pre></td></tr></table></figure>

<p>（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认1000</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions<span class="operator">=</span><span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p>（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有365 个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有365 个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认100000</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files<span class="operator">=</span><span class="number">100000</span></span><br></pre></td></tr></table></figure>

<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition<span class="operator">=</span><span class="literal">false</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）案例实操</p>
</blockquote>
<p>需求：将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分区中。</p>
<p>（1）创建目标分区表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_no_par(dname string, loc string) partitioned <span class="keyword">by</span> (deptno <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.185</span> seconds</span><br></pre></td></tr></table></figure>

<p>（2）设置动态分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)&gt; set hive.exec.dynamic.partition.mode=nonstrict;</span><br></pre></td></tr></table></figure>

<p>(3)导入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_no_par <span class="keyword">partition</span>(deptno) <span class="keyword">select</span> dname,loc,deptno <span class="keyword">from</span> dept;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.062</span> seconds</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211219145313_ad874c4a<span class="operator">-</span>da69<span class="number">-43</span>f2<span class="operator">-</span>a0bb<span class="number">-6e0</span>e9ea0ce6c</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">14</span>:<span class="number">53</span>:<span class="number">33</span>,<span class="number">924</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">14</span>:<span class="number">53</span>:<span class="number">41</span>,<span class="number">638</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, Cumulative CPU <span class="number">2.93</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-12</span><span class="number">-19</span> <span class="number">14</span>:<span class="number">53</span>:<span class="number">51</span>,<span class="number">240</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, reduce <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, Cumulative CPU <span class="number">4.7</span> sec</span><br><span class="line">MapReduce Total cumulative CPU <span class="type">time</span>: <span class="number">4</span> seconds <span class="number">700</span> msec</span><br><span class="line">Ended Job <span class="operator">=</span> job_1639880318289_0007</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>  Cumulative CPU: <span class="number">4.7</span> sec  HDFS Read: <span class="number">14680</span> HDFS Write: <span class="number">773</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">4</span> seconds <span class="number">700</span> msec</span><br><span class="line">OK</span><br><span class="line">dname  loc  deptno</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">42.654</span> seconds</span><br></pre></td></tr></table></figure>

<p>（4）查看目标分区表的分区情况</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">show</span> partitions dept_no_par;</span><br><span class="line">OK</span><br><span class="line"><span class="keyword">partition</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">10</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">20</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">30</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">40</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">70</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.39</span> seconds, Fetched: <span class="number">5</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p><strong>动态分区也可以这么写（3.0新增）：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_no_par2(dname string, loc string) partitioned <span class="keyword">by</span> (deptno <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.188</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_no_par2 <span class="keyword">select</span> dname,loc,deptno <span class="keyword">from</span> dept;</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">show</span> partitions dept_no_par2;</span><br><span class="line">OK</span><br><span class="line"><span class="keyword">partition</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">10</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">20</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">30</span></span><br><span class="line">deptno<span class="operator">=</span><span class="number">40</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.137</span> seconds, Fetched: <span class="number">4</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h2><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p><strong>分区针对的是数据的存储路径；分桶针对的是数据文件。</strong></p>
<blockquote>
<p>1）先创建分桶表‘</p>
</blockquote>
<p>（1）数据准备</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1001</span> ss1</span><br><span class="line"><span class="number">1002</span> ss2</span><br><span class="line"><span class="number">1003</span> ss3</span><br><span class="line"><span class="number">1004</span> ss4</span><br><span class="line"><span class="number">1005</span> ss5</span><br><span class="line"><span class="number">1006</span> ss6</span><br><span class="line"><span class="number">1007</span> ss7</span><br><span class="line"><span class="number">1008</span> ss8</span><br><span class="line"><span class="number">1009</span> ss9</span><br><span class="line"><span class="number">1010</span> ss10</span><br><span class="line"><span class="number">1011</span> ss11</span><br><span class="line"><span class="number">1012</span> ss12</span><br><span class="line"><span class="number">1013</span> ss13</span><br><span class="line"><span class="number">1014</span> ss14</span><br><span class="line"><span class="number">1015</span> ss15</span><br><span class="line"><span class="number">1016</span> ss16</span><br></pre></td></tr></table></figure>

<p>（2）创建分桶表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> stu_buck(id <span class="type">int</span> , name string) clustered <span class="keyword">by</span>(id) <span class="keyword">into</span> <span class="number">4</span> buckets <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.671</span> seconds</span><br></pre></td></tr></table></figure>

<p>（4）导入数据到分桶表中，load 的方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br></pre></td></tr></table></figure>

<p>（5）查看创建的分桶表中是否分成4 个桶</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211219154420597.png" alt="image-20211219154420597"></p>
<p>（6）查询分桶的数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu_buck;</span><br><span class="line">OK</span><br><span class="line">stu_buck.id  stu_buck.name</span><br><span class="line"><span class="number">1001</span>  zzz</span><br><span class="line"><span class="number">1002</span>  ddd</span><br><span class="line"><span class="number">1111</span>  ccc</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.549</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（7）分桶规则：</p>
<p>根据结果可知：Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中</p>
<blockquote>
<p>2）分桶表操作需要注意的事项:</p>
</blockquote>
<p>（1）reduce 的个数设置为-1,让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个数设置为大于等于分桶表的桶数。</p>
<p>（2）从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题。</p>
<p>（3）不要使用本地模式。</p>
<blockquote>
<p>3）insert 方式将数据导入分桶表</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student_insert;</span><br></pre></td></tr></table></figure>



<h2 id="抽样查询"><a href="#抽样查询" class="headerlink" title="抽样查询"></a>抽样查询</h2><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求。</p>
<p>语法: TABLESAMPLE(BUCKET x OUT OF y)</p>
<p>查询表 stu_buck 中的数据。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> id);</span><br></pre></td></tr></table></figure>

<p><strong>注意：x 的值必须小于等于 y 的值，否则</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error <span class="number">10061</span>]: Numerator should <span class="keyword">not</span> be bigger </span><br><span class="line">than denominator <span class="keyword">in</span> sample clause <span class="keyword">for</span> <span class="keyword">table</span> stu_buck</span><br></pre></td></tr></table></figure>



<h1 id="第8-章-函数"><a href="#第8-章-函数" class="headerlink" title="第8 章 函数"></a>第8 章 函数</h1><h2 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h2><blockquote>
<p>1）查看系统自带的函数</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">show</span> functions;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.085</span> seconds, Fetched: <span class="number">289</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）显示自带的函数的用法</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">desc</span> <span class="keyword">function</span> upper;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"><span class="built_in">upper</span>(str)<span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.107</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）详细显示自带的函数的用法</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">desc</span> <span class="keyword">function</span> upper;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"><span class="built_in">upper</span>(str)<span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.107</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">function</span> extended upper;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"><span class="built_in">upper</span>(str)<span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase</span><br><span class="line">Synonyms: ucase</span><br><span class="line">Example:</span><br><span class="line"><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="built_in">upper</span>(<span class="string">&#x27;Facebook&#x27;</span>) <span class="keyword">FROM</span> src LIMIT <span class="number">1</span>;</span><br><span class="line"><span class="string">&#x27;FACEBOOK&#x27;</span></span><br><span class="line"><span class="keyword">Function</span> class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper</span><br><span class="line"><span class="keyword">Function</span> type:BUILTIN</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.049</span> seconds, Fetched: <span class="number">7</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h2><h3 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h3><blockquote>
<p>1）函数说明</p>
</blockquote>
<p>NVL：给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数都为 NULL ，则返回 NULL。</p>
<blockquote>
<p>2）数据准备：采用员工表</p>
</blockquote>
<blockquote>
<p>3）查询：如果员工的 comm 为 NULL，则用-1 代替</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> ename,comm,nvl(comm,<span class="number">-1</span>) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">ename  comm  _c2</span><br><span class="line">SMITH  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">ALLEN <span class="number">300.0</span> <span class="number">300.0</span></span><br><span class="line">WARD <span class="number">500.0</span> <span class="number">500.0</span></span><br><span class="line">JONES  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">MARTIN <span class="number">1400.0</span> <span class="number">1400.0</span></span><br><span class="line">BLAKE  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">CLARK  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">SCOTT  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">KING <span class="number">10.0</span> <span class="number">10.0</span></span><br><span class="line">TURNER <span class="number">0.0</span> <span class="number">0.0</span></span><br><span class="line">ADAMS  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">JAMES  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">FORD  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line">MILLER  <span class="keyword">NULL</span> <span class="number">-1.0</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.184</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）查询：如果员工的 comm 为 NULL，则用领导 id 代替</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> comm, nvl(comm,mgr) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">comm  _c1</span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7902.0</span></span><br><span class="line"><span class="number">300.0</span> <span class="number">300.0</span></span><br><span class="line"><span class="number">500.0</span> <span class="number">500.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7839.0</span></span><br><span class="line"><span class="number">1400.0</span> <span class="number">1400.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7839.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7839.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7566.0</span></span><br><span class="line"><span class="number">10.0</span> <span class="number">10.0</span></span><br><span class="line"><span class="number">0.0</span> <span class="number">0.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7788.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7698.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7566.0</span></span><br><span class="line"><span class="keyword">NULL</span> <span class="number">7782.0</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.242</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="CASE-WHEN-THEN-ELSE-END"><a href="#CASE-WHEN-THEN-ELSE-END" class="headerlink" title="CASE WHEN THEN ELSE END"></a>CASE WHEN THEN ELSE END</h3><p><strong>UDF:一进一出，UDAF：多进多出，UDTF：一进多出</strong></p>
<blockquote>
<p>1）数据准备</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">悟空 A 男</span><br><span class="line">大海 A 男</span><br><span class="line">宋宋 B 男</span><br><span class="line">凤姐 A 女</span><br><span class="line">婷姐 B 女</span><br><span class="line">婷婷 B 女</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）需求</p>
</blockquote>
<p>求出不同部门男女各多少人。结果如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dept_Id 男女</span><br><span class="line">A <span class="number">2</span> <span class="number">1</span></span><br><span class="line">B <span class="number">1</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）创建本地 emp_sex.txt，导入数据</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi emp_sex.txt</span><br><span class="line">悟空 A 男</span><br><span class="line">大海 A 男</span><br><span class="line">宋宋 B 男</span><br><span class="line">凤姐 A 女</span><br><span class="line">婷姐 B 女</span><br><span class="line">婷婷 B 女</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）创建 hive 表并导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> emp_sex(name string,dept_id string,sex string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.105</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp_sex.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.emp_sex</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.431</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>5）按需求查询数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> dept_id,<span class="built_in">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) maleCount,<span class="built_in">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;女&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) femaleCount <span class="keyword">from</span> emp_sex <span class="keyword">group</span> <span class="keyword">by</span> dept_id;</span><br><span class="line">dept_id malecount  femalecount</span><br><span class="line">A <span class="number">2</span> <span class="number">1</span></span><br><span class="line">B <span class="number">1</span> <span class="number">2</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">39.575</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>更换一种查询方式</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> dept_id,<span class="built_in">sum</span>(if(sex<span class="operator">=</span><span class="string">&#x27;男&#x27;</span>,<span class="number">1</span>,<span class="number">0</span>)) maleCount,<span class="built_in">sum</span>(if(sex<span class="operator">=</span><span class="string">&#x27;女&#x27;</span>,<span class="number">1</span>,<span class="number">0</span>)) femaleCount <span class="keyword">from</span> emp_sex <span class="keyword">group</span> <span class="keyword">by</span> dept_id;</span><br><span class="line">dept_id malecount  femalecount</span><br><span class="line">A <span class="number">2</span> <span class="number">1</span></span><br><span class="line">B <span class="number">1</span> <span class="number">2</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">36.895</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><blockquote>
<p>1）相关函数说明</p>
</blockquote>
<p>CONCAT(string A&#x2F;col, string B&#x2F;col…)：返回输入字符串连接后的结果，支持任意个输入字符串; </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> concat(ename,sal) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">SMITH800<span class="number">.0</span></span><br><span class="line">ALLEN1600<span class="number">.0</span></span><br><span class="line">WARD1250<span class="number">.0</span></span><br><span class="line">JONES2975<span class="number">.0</span></span><br><span class="line">MARTIN1250<span class="number">.0</span></span><br><span class="line">BLAKE2850<span class="number">.0</span></span><br><span class="line">CLARK2450<span class="number">.0</span></span><br><span class="line">SCOTT3000<span class="number">.0</span></span><br><span class="line"><span class="keyword">NULL</span></span><br><span class="line">TURNER1500<span class="number">.0</span></span><br><span class="line">ADAMS1100<span class="number">.0</span></span><br><span class="line">JAMES950<span class="number">.0</span></span><br><span class="line">FORD3000<span class="number">.0</span></span><br><span class="line">MILLER1300<span class="number">.0</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.07</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p>
<p><strong>注意: CONCAT_WS must be “string or array</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> concat_ws(<span class="string">&#x27;-&#x27;</span>,ename,job,hiredate) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">SMITH<span class="operator">-</span>CLERK<span class="number">-1980</span><span class="number">-12</span><span class="number">-17</span></span><br><span class="line">ALLEN<span class="operator">-</span>SALESMAN<span class="number">-1981</span><span class="number">-2</span><span class="number">-20</span></span><br><span class="line">WARD<span class="operator">-</span>SALESMAN<span class="number">-1981</span><span class="number">-2</span><span class="number">-22</span></span><br><span class="line">JONES<span class="operator">-</span>MANAGER<span class="number">-1981</span><span class="number">-4</span><span class="number">-2</span></span><br><span class="line">MARTIN<span class="operator">-</span>SALESMAN<span class="number">-1981</span><span class="number">-9</span><span class="number">-28</span></span><br><span class="line">BLAKE<span class="operator">-</span>MANAGER<span class="number">-1981</span><span class="number">-5</span><span class="number">-1</span></span><br><span class="line">CLARK<span class="operator">-</span>MANAGER<span class="number">-1981</span><span class="number">-6</span><span class="number">-9</span></span><br><span class="line">SCOTT<span class="operator">-</span>ANALYST<span class="number">-1987</span><span class="number">-4</span><span class="number">-19</span></span><br><span class="line">KING<span class="operator">-</span>PRESIDENT<span class="number">-5000.00</span></span><br><span class="line">TURNER<span class="operator">-</span>SALESMAN<span class="number">-1981</span><span class="number">-9</span><span class="number">-8</span></span><br><span class="line">ADAMS<span class="operator">-</span>CLERK<span class="number">-1987</span><span class="number">-5</span><span class="number">-23</span></span><br><span class="line">JAMES<span class="operator">-</span>CLERK<span class="number">-1981</span><span class="number">-12</span><span class="number">-3</span></span><br><span class="line">FORD<span class="operator">-</span>ANALYST<span class="number">-1981</span><span class="number">-12</span><span class="number">-3</span></span><br><span class="line">MILLER<span class="operator">-</span>CLERK<span class="number">-1982</span><span class="number">-1</span><span class="number">-23</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.249</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 Array 类型字段。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> collect_set(id) <span class="keyword">from</span> emp;</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">3</span> seconds <span class="number">620</span> msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">[<span class="number">1001</span>,<span class="number">1002</span>,<span class="number">1111</span>]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">37.866</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> collect_list(id) <span class="keyword">from</span> student;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）数据准备</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 hive]$ vim person_info.txt</span><br><span class="line">孙悟空白羊座 A</span><br><span class="line">大海射手座 A</span><br><span class="line">宋宋白羊座 B</span><br><span class="line">猪八戒白羊座 A</span><br><span class="line">凤姐射手座 A</span><br><span class="line">苍老师白羊座 B</span><br></pre></td></tr></table></figure>



<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">孙悟空白羊座 A</span><br><span class="line">大海射手座 A</span><br><span class="line">宋宋白羊座 B</span><br><span class="line">猪八戒白羊座 A</span><br><span class="line">凤姐射手座 A</span><br><span class="line">苍老师白羊座 B</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）需求</p>
</blockquote>
<p>把星座和血型一样的人归类到一起。结果如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A 大海<span class="operator">|</span>凤姐</span><br><span class="line">白羊座,A 孙悟空<span class="operator">|</span>猪八戒</span><br><span class="line">白羊座,B 宋宋<span class="operator">|</span>苍老师</span><br></pre></td></tr></table></figure>



<blockquote>
<p>4）创建 hive 表并导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> person_info(name string,constellation string,blood_type string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.293</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/person_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.person_info</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.362</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>6）按需求查询数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"> concat(constellation,<span class="string">&#x27;,&#x27;</span>,blood_type) cb,</span><br><span class="line"> name</span><br><span class="line"> <span class="keyword">from</span> person_info;</span><br><span class="line">#<span class="comment">-----------进化------------</span></span><br><span class="line"> <span class="keyword">select</span> t1.cb,collect_set(t1.name) <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> concat(constellation,<span class="string">&#x27;,&#x27;</span>,blood_type) cb,name <span class="keyword">from</span> person_info) t1 </span><br><span class="line"> <span class="keyword">group</span> <span class="keyword">by</span> t1.cb;</span><br><span class="line"></span><br><span class="line"> OK</span><br><span class="line">t1.cb  _c1</span><br><span class="line">射手座,A [&quot;大海&quot;,&quot;凤姐&quot;]</span><br><span class="line">白羊座,A [&quot;孙悟空&quot;,&quot;猪八戒&quot;]</span><br><span class="line">白羊座,B [&quot;宋宋&quot;,&quot;苍老师&quot;]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">38.329</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br><span class="line">#<span class="comment">-----------再次进化------------</span></span><br><span class="line"> <span class="keyword">select</span> t1.cb,concat_ws(<span class="string">&#x27;|&#x27;</span>,collect_set(t1.name)) <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> concat(constellation,<span class="string">&#x27;,&#x27;</span>,blood_type) cb,name <span class="keyword">from</span> person_info) t1 </span><br><span class="line"> <span class="keyword">group</span> <span class="keyword">by</span> t1.cb;</span><br><span class="line"></span><br><span class="line"> OK</span><br><span class="line">t1.cb  _c1</span><br><span class="line">射手座,A 大海<span class="operator">|</span>凤姐</span><br><span class="line">白羊座,A 孙悟空<span class="operator">|</span>猪八戒</span><br><span class="line">白羊座,B 宋宋<span class="operator">|</span>苍老师</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">29.741</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><blockquote>
<p>1）函数说明</p>
</blockquote>
<p>EXPLODE(col)：将 hive 一列中复杂的 Array 或者 Map 结构拆分成多行。</p>
<p>LATERAL VIEW：侧写</p>
<p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p>
<p>解释：用于和 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<blockquote>
<p>2）数据准备</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》<span class="operator">-</span>悬疑,动作,科幻,剧情</span><br><span class="line">《Lie <span class="keyword">to</span> me》<span class="operator">-</span>悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼<span class="number">2</span>》<span class="operator">-</span>战争,动作,灾难</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）需求</p>
</blockquote>
<p>将电影分类中的数组数据展开。结果如下：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211220085233344.png" alt="image-20211220085233344"></p>
<blockquote>
<p>4）创建本地 movie.txt，导入数据</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop100 bin]$ vi movie.txt</span><br><span class="line">《疑犯追踪》-悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》-悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》-战争,动作,灾难</span><br></pre></td></tr></table></figure>



<blockquote>
<p>5）创建 hive 表并导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> movie_info(movie string,category string)<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;-&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.671</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/movie.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.movie_info</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.103</span> seconds</span><br></pre></td></tr></table></figure>

<blockquote>
<p>6）按需求查询数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#先切开城数组</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> split(category,&quot;,&quot;) <span class="keyword">from</span> movie_info;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">[&quot;悬疑&quot;,&quot;动作&quot;,&quot;科幻&quot;,&quot;剧情&quot;]</span><br><span class="line">[&quot;悬疑&quot;,&quot;警匪&quot;,&quot;动作&quot;,&quot;心理&quot;,&quot;剧情&quot;]</span><br><span class="line">[&quot;战争&quot;,&quot;动作&quot;,&quot;灾难&quot;]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.828</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br><span class="line">#切成数组之后再展开</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> explode(split(category,&quot;,&quot;)) <span class="keyword">from</span> movie_info;</span><br><span class="line">OK</span><br><span class="line">col</span><br><span class="line">悬疑</span><br><span class="line">动作</span><br><span class="line">科幻</span><br><span class="line">剧情</span><br><span class="line">悬疑</span><br><span class="line">警匪</span><br><span class="line">动作</span><br><span class="line">心理</span><br><span class="line">剧情</span><br><span class="line">战争</span><br><span class="line">动作</span><br><span class="line">灾难</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.223</span> seconds, Fetched: <span class="number">12</span> <span class="type">row</span>(s)</span><br><span class="line">#需要使用侧写关联电影名</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> movie,category_name</span><br><span class="line"><span class="operator">&gt;</span> <span class="keyword">from</span> movie_info</span><br><span class="line"><span class="operator">&gt;</span> <span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(split(category,&quot;,&quot;)) movie_info_tmp <span class="keyword">as</span> category_name;</span><br><span class="line">OK</span><br><span class="line">movie  category_name</span><br><span class="line">《疑犯追踪》悬疑</span><br><span class="line">《疑犯追踪》动作</span><br><span class="line">《疑犯追踪》科幻</span><br><span class="line">《疑犯追踪》剧情</span><br><span class="line">《Lie <span class="keyword">to</span> me》悬疑</span><br><span class="line">《Lie <span class="keyword">to</span> me》警匪</span><br><span class="line">《Lie <span class="keyword">to</span> me》动作</span><br><span class="line">《Lie <span class="keyword">to</span> me》心理</span><br><span class="line">《Lie <span class="keyword">to</span> me》剧情</span><br><span class="line">《战狼<span class="number">2</span>》战争</span><br><span class="line">《战狼<span class="number">2</span>》动作</span><br><span class="line">《战狼<span class="number">2</span>》灾难</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.15</span> seconds, Fetched: <span class="number">12</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="窗口函数（开窗函数）"><a href="#窗口函数（开窗函数）" class="headerlink" title="窗口函数（开窗函数）"></a>窗口函数（开窗函数）</h3><blockquote>
<p>1）相关函数说明</p>
</blockquote>
<p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。</p>
<p>CURRENT ROW：当前行</p>
<p>n PRECEDING：往前 n 行数据</p>
<p>n FOLLOWING：往后 n 行数据</p>
<p>UNBOUNDED：起点，</p>
<p>​	UNBOUNDED PRECEDING 表示从前面的起点，</p>
<p>​	UNBOUNDED FOLLOWING 表示到后面的终点</p>
<p>LAG(col,n,default_val)：往前第 n 行数据</p>
<p>LEAD(col,n, default_val)：往后第 n 行数据</p>
<p>NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1 开始，对于每一行，NTILE 返回此行所属的组的编号。<strong>注意：n 必须为 int 类型</strong>。</p>
<blockquote>
<p>2）数据准备：name，orderdate，cost</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,<span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>,<span class="number">10</span></span><br><span class="line">tony,<span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>,<span class="number">15</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>,<span class="number">23</span></span><br><span class="line">tony,<span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>,<span class="number">29</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>,<span class="number">46</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>,<span class="number">42</span></span><br><span class="line">tony,<span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>,<span class="number">50</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>,<span class="number">55</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>,<span class="number">62</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>,<span class="number">68</span></span><br><span class="line">neil,<span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>,<span class="number">12</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>,<span class="number">75</span></span><br><span class="line">neil,<span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>,<span class="number">80</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>,<span class="number">94</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）需求</p>
</blockquote>
<p>（1）查询在2017 年4 月份购买过的顾客及总人数</p>
<p>（2）查询顾客的购买明细及月购买总额</p>
<p>（3）上述的场景,将每个顾客的 cost 按照日期进行累加</p>
<p>（4）查询每个顾客上次的购买时间</p>
<p>（5）查询前20%时间的订单信息</p>
<blockquote>
<p>4）创建本地 business.txt，导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[atguigu<span class="variable">@hadoop100</span> bin]$ vim business.txt</span><br><span class="line">jack,<span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>,<span class="number">10</span></span><br><span class="line">tony,<span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>,<span class="number">15</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>,<span class="number">23</span></span><br><span class="line">tony,<span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>,<span class="number">29</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>,<span class="number">46</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>,<span class="number">42</span></span><br><span class="line">tony,<span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>,<span class="number">50</span></span><br><span class="line">jack,<span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>,<span class="number">55</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>,<span class="number">62</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>,<span class="number">68</span></span><br><span class="line">neil,<span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>,<span class="number">12</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>,<span class="number">75</span></span><br><span class="line">neil,<span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>,<span class="number">80</span></span><br><span class="line">mart,<span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>,<span class="number">94</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>5）创建 hive 表并导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> business(name string,orderdate string,cose <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.127</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/business.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.business</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.31</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>6）按需求查询数据</p>
</blockquote>
<p>（1）查询在2017 年4 月份购买过的顾客及总人数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#先找出<span class="number">2017</span><span class="number">-04</span>购物的人员信息</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> business <span class="keyword">where</span> <span class="built_in">substring</span>(orderdate,<span class="number">0</span>,<span class="number">7</span>)<span class="operator">=</span> <span class="string">&#x27;2017-04&#x27;</span>;</span><br><span class="line">business.name  business.orderdate  business.cose</span><br><span class="line">jack <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span> <span class="number">42</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span> <span class="number">62</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span> <span class="number">68</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span> <span class="number">75</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span> <span class="number">94</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.298</span> seconds, Fetched: <span class="number">5</span> <span class="type">row</span>(s)</span><br><span class="line">#直接使用<span class="keyword">over</span></span><br><span class="line"><span class="keyword">select</span> name,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">over</span> ()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> <span class="built_in">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>)<span class="operator">=</span> <span class="string">&#x27;2017-04&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure>

<p>这里的over()可以理解为进行赋值，如果（）里面没有值，那么就按照前面字段的值对每一行进行赋值。</p>
<blockquote>
<p>（2）查询顾客的购买明细及月购买总额</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> name,orderdate,cose,<span class="built_in">sum</span>(cose) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name,<span class="keyword">month</span>(orderdate)) <span class="keyword">as</span> sum_cose <span class="keyword">from</span> business;</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211220100739_814d4128<span class="number">-1025</span><span class="number">-44</span>de<span class="number">-8</span>d98<span class="number">-6177</span>b2eaf8c4</span><br><span class="line">name  orderdate  cose  sum_cose</span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span> <span class="number">46</span> <span class="number">111</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span> <span class="number">55</span> <span class="number">111</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span> <span class="number">10</span> <span class="number">111</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span> <span class="number">23</span> <span class="number">23</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span> <span class="number">42</span> <span class="number">42</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span> <span class="number">94</span> <span class="number">299</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span> <span class="number">75</span> <span class="number">299</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span> <span class="number">68</span> <span class="number">299</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span> <span class="number">62</span> <span class="number">299</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span> <span class="number">12</span> <span class="number">12</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span> <span class="number">80</span> <span class="number">80</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span> <span class="number">29</span> <span class="number">94</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span> <span class="number">15</span> <span class="number">94</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span> <span class="number">50</span> <span class="number">94</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">32.289</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<blockquote>
<p>（3）将每个顾客的 cost 按照日期进行累加</p>
</blockquote>
<p>方法一：再over里面group by</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> name,orderdate,cose,<span class="built_in">sum</span>(cose) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">from</span> business;</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211220101500_4d0b26bf<span class="number">-504</span>a<span class="number">-4</span>fac<span class="number">-941</span>d<span class="operator">-</span>aef9c90861ae</span><br><span class="line">name  orderdate  cose  sum_window_0</span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span> <span class="number">10</span> <span class="number">10</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span> <span class="number">46</span> <span class="number">56</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span> <span class="number">55</span> <span class="number">111</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span> <span class="number">23</span> <span class="number">134</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span> <span class="number">42</span> <span class="number">176</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span> <span class="number">62</span> <span class="number">62</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span> <span class="number">68</span> <span class="number">130</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span> <span class="number">75</span> <span class="number">205</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span> <span class="number">94</span> <span class="number">299</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span> <span class="number">12</span> <span class="number">12</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span> <span class="number">80</span> <span class="number">92</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span> <span class="number">15</span> <span class="number">15</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span> <span class="number">29</span> <span class="number">44</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span> <span class="number">50</span> <span class="number">94</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">30.275</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> name,orderdate,cost,</span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name) <span class="keyword">as</span> sample2,<span class="comment">--按 name 分组，组内数据相加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按 name</span></span><br><span class="line">分组，组内数据累加</span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> </span><br><span class="line">UNBOUNDED PRECEDING <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和 sample3 一样,由起点到</span></span><br><span class="line">当前行的聚合</span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> </span><br><span class="line">PRECEDING <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>) <span class="keyword">as</span> sample5,<span class="comment">--当前行和前面一行做聚合</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> </span><br><span class="line">PRECEDING <span class="keyword">AND</span> <span class="number">1</span> FOLLOWING ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> </span><br><span class="line"><span class="type">row</span> <span class="keyword">and</span> UNBOUNDED FOLLOWING ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行</span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>rows 必须跟在 order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量。</p>
<p>（4）查看顾客上次的购买时间</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> name,orderdate,<span class="built_in">lag</span>(orderdate,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">from</span> business;</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211220110225_469d7d71<span class="number">-0</span>dc7<span class="number">-4e34</span><span class="operator">-</span>be38<span class="number">-466</span>b10dda527</span><br><span class="line">OK</span><br><span class="line">name  orderdate  lag_window_0</span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="keyword">NULL</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="keyword">NULL</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="keyword">NULL</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="keyword">NULL</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">28.747</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（5）查询前20%时间的订单信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#先进行分组</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> name,orderdate,cose,<span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) groupId <span class="keyword">from</span> business;</span><br><span class="line">name  orderdate  cose  groupid</span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span> <span class="number">10</span> <span class="number">1</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span> <span class="number">15</span> <span class="number">1</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span> <span class="number">29</span> <span class="number">1</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span> <span class="number">46</span> <span class="number">2</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span> <span class="number">50</span> <span class="number">2</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span> <span class="number">55</span> <span class="number">2</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span> <span class="number">23</span> <span class="number">3</span></span><br><span class="line">jack <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span> <span class="number">42</span> <span class="number">3</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span> <span class="number">62</span> <span class="number">3</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span> <span class="number">68</span> <span class="number">4</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span> <span class="number">75</span> <span class="number">4</span></span><br><span class="line">mart <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span> <span class="number">94</span> <span class="number">4</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span> <span class="number">12</span> <span class="number">5</span></span><br><span class="line">neil <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span> <span class="number">80</span> <span class="number">5</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">27.289</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br><span class="line"><span class="keyword">select</span> name,orderdate,cose</span><br><span class="line"> <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> name,orderdate,cose,<span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) groupId <span class="keyword">from</span> business) t1</span><br><span class="line"> <span class="keyword">where</span> t1.groupId <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">#获取前<span class="number">20</span><span class="operator">%</span></span><br><span class="line"> hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> name,orderdate,cose</span><br><span class="line"><span class="operator">&gt;</span>  <span class="keyword">from</span> </span><br><span class="line"><span class="operator">&gt;</span> (<span class="keyword">select</span> name,orderdate,cose,<span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) groupId <span class="keyword">from</span> business) t1</span><br><span class="line"><span class="operator">&gt;</span>  <span class="keyword">where</span> t1.groupId <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211220112433_9da80326<span class="number">-137</span>a<span class="number">-47</span>db<span class="number">-9390</span><span class="number">-03</span>fcbdaa2e23</span><br><span class="line">OK</span><br><span class="line">name  orderdate  cose</span><br><span class="line">jack <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span> <span class="number">10</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span> <span class="number">15</span></span><br><span class="line">tony <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span> <span class="number">29</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">28.922</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h3 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h3><blockquote>
<p>1）函数说明</p>
</blockquote>
<p>RANK()排序相同时会重复，总数不会变</p>
<p>DENSE_RANK()排序相同时会重复，总数会减少</p>
<p>ROW_NUMBER()会根据顺序计算</p>
<blockquote>
<p>2）数据准备</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">孙悟空语文 87</span><br><span class="line">孙悟空数学 95</span><br><span class="line">孙悟空英语 68</span><br><span class="line">大海语文 94</span><br><span class="line">大海数学 56</span><br><span class="line">大海英语 84</span><br><span class="line">宋宋语文 64</span><br><span class="line">宋宋数学 86</span><br><span class="line">宋宋英语 84</span><br><span class="line">婷婷语文 65</span><br><span class="line">婷婷数学 85</span><br><span class="line">婷婷英语 78</span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）需求</p>
</blockquote>
<p>计算每门学科成绩排名。</p>
<blockquote>
<p>4）创建本地 score.txt，导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[atguigu<span class="variable">@hadoop100</span> hive]$ vim score.txt</span><br><span class="line">孙悟空语文 <span class="number">87</span></span><br><span class="line">孙悟空数学 <span class="number">95</span></span><br><span class="line">孙悟空英语 <span class="number">68</span></span><br><span class="line">大海语文 <span class="number">94</span></span><br><span class="line">大海数学 <span class="number">56</span></span><br><span class="line">大海英语 <span class="number">84</span></span><br><span class="line">宋宋语文 <span class="number">64</span></span><br><span class="line">宋宋数学 <span class="number">86</span></span><br><span class="line">宋宋英语 <span class="number">84</span></span><br><span class="line">婷婷语文 <span class="number">65</span></span><br><span class="line">婷婷数学 <span class="number">85</span></span><br><span class="line">婷婷英语 <span class="number">78</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>5）创建 hive 表并导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> score(name string,subject string,score <span class="type">int</span>)<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">5.288</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/score.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive3.score</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">3.227</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>6）按需求查询数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span>,<span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">from</span> score;</span><br><span class="line">score.name  score.subject  score.score  rank_window_0</span><br><span class="line">大海数学 <span class="number">56</span> <span class="number">1</span></span><br><span class="line">婷婷数学 <span class="number">85</span> <span class="number">2</span></span><br><span class="line">宋宋数学 <span class="number">86</span> <span class="number">3</span></span><br><span class="line">孙悟空数学 <span class="number">95</span> <span class="number">4</span></span><br><span class="line">孙悟空英语 <span class="number">68</span> <span class="number">1</span></span><br><span class="line">婷婷英语 <span class="number">78</span> <span class="number">2</span></span><br><span class="line">宋宋英语 <span class="number">84</span> <span class="number">3</span></span><br><span class="line">大海英语 <span class="number">84</span> <span class="number">3</span></span><br><span class="line">宋宋语文 <span class="number">64</span> <span class="number">1</span></span><br><span class="line">婷婷语文 <span class="number">65</span> <span class="number">2</span></span><br><span class="line">孙悟空语文 <span class="number">87</span> <span class="number">3</span></span><br><span class="line">大海语文 <span class="number">94</span> <span class="number">4</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">63.935</span> seconds, Fetched: <span class="number">12</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>扩展：求出每门学科前三名的学生？</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="operator">&gt;</span> <span class="keyword">from</span></span><br><span class="line"><span class="operator">&gt;</span> (<span class="keyword">select</span> <span class="operator">*</span>,<span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score) tk <span class="keyword">from</span> score) t1</span><br><span class="line"><span class="operator">&gt;</span> <span class="keyword">where</span> tk<span class="operator">&lt;=</span><span class="number">3</span>;</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211220154153_5acd7d2b<span class="operator">-</span>d9aa<span class="number">-479</span>c<span class="number">-8</span>bf0<span class="operator">-</span>dfedfdc517db</span><br><span class="line">OK</span><br><span class="line">t1.name t1.subject  t1.score  t1.tk</span><br><span class="line">大海数学<span class="number">56</span> <span class="number">1</span></span><br><span class="line">婷婷数学<span class="number">85</span> <span class="number">2</span></span><br><span class="line">宋宋数学<span class="number">86</span> <span class="number">3</span></span><br><span class="line">孙悟空英语<span class="number">68</span> <span class="number">1</span></span><br><span class="line">婷婷英语<span class="number">78</span> <span class="number">2</span></span><br><span class="line">大海英语<span class="number">84</span> <span class="number">3</span></span><br><span class="line">宋宋英语<span class="number">84</span> <span class="number">3</span></span><br><span class="line">宋宋语文<span class="number">64</span> <span class="number">1</span></span><br><span class="line">婷婷语文<span class="number">65</span> <span class="number">2</span></span><br><span class="line">孙悟空语文<span class="number">87</span> <span class="number">3</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">68.69</span> seconds, Fetched: <span class="number">10</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p>1）Hive 自带了一些函数，比如：max&#x2F;min 等，但是数量有限，自己可以通过自定义 UDF 来方便的扩展。</p>
<p>2）当 Hive 提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。</p>
<p>3）根据用户自定义函数类别分为以下三种：</p>
<blockquote>
<p>（1）UDF（User-Defined-Function）一进一出</p>
<p>（2）UDAF（User-Defined Aggregation Function）聚集函数，多进一出类似于：count&#x2F;max&#x2F;min </p>
<p>（3）UDTF（User-Defined Table-Generating Functions）一进多出如 lateral view explode()</p>
</blockquote>
<p>4）官方文档地址 <a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p>
<p>5）编程步骤：</p>
<p>（1）继承 Hive 提供的类</p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</p>
<p> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; </p>
<p>（2）实现类中的抽象方法</p>
<blockquote>
<p>（3）在 hive 的命令行窗口创建函数</p>
</blockquote>
<p>添加 jar</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path </span><br></pre></td></tr></table></figure>

<p>创建 function </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name; </span><br></pre></td></tr></table></figure>



<blockquote>
<p>（4）在 hive 的命令行窗口删除函数</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop [temporary] function [if exists][dbname.]function_name;</span><br></pre></td></tr></table></figure>



<h2 id="自定义-UDF-函数"><a href="#自定义-UDF-函数" class="headerlink" title="自定义 UDF 函数"></a>自定义 UDF 函数</h2><blockquote>
<p>0）需求:</p>
</blockquote>
<p>自定义一个 UDF 实现计算给定字符串的长度，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> my_len(&quot;abcd&quot;);</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）创建一个 Maven 工程 Hive</p>
</blockquote>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211220165849296.png" alt="image-20211220165849296"></p>
<blockquote>
<p>2）导入依赖</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>3）创建一个类</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*<span class="doctag">@author</span>:左泽林</span></span><br><span class="line"><span class="comment">*<span class="doctag">@date</span>:日期:2021-12-20-时间:16:28</span></span><br><span class="line"><span class="comment">*<span class="doctag">@message</span>:</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyUDF</span> <span class="keyword">extends</span> <span class="title class_">GenericUDF</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//校验数据参数个数</span></span><br><span class="line"> <span class="keyword">public</span> ObjectInspector <span class="title function_">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException &#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> (arguments.length != <span class="number">1</span>)&#123;</span><br><span class="line"> <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentException</span>(<span class="string">&quot;参数个数不为1&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//处理数据</span></span><br><span class="line"> <span class="keyword">public</span> Object <span class="title function_">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//1.取出输入数据</span></span><br><span class="line"> <span class="type">String</span> <span class="variable">input</span> <span class="operator">=</span> arguments[<span class="number">0</span>].toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.判断输入数据是否为null</span></span><br><span class="line"> <span class="keyword">if</span> (input == <span class="literal">null</span>)&#123;</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.返回输入数据的长度</span></span><br><span class="line"> <span class="keyword">return</span> input.length();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span> String <span class="title function_">getDisplayString</span><span class="params">(String[] strings)</span>&#123;</span><br><span class="line"> <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）打成 jar 包上传到服务器&#x2F;opt&#x2F;module&#x2F;data&#x2F;myudf.jar</p>
</blockquote>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211220170003503.png" alt="image-20211220170003503"></p>
<blockquote>
<p>5）将 jar 包添加到 hive 的 classpath</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>hive<span class="operator">-</span>demo<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line">Added [<span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>hive<span class="operator">-</span>demo<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar] <span class="keyword">to</span> class path</span><br><span class="line">Added resources: [<span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>hive<span class="operator">-</span>demo<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>6）创建临时函数与开发好的 java class 关联</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">create</span> temporary <span class="keyword">function</span> my_len <span class="keyword">as</span> &quot;com.atguigu.udf.MyUDF&quot;;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.104</span> seconds</span><br></pre></td></tr></table></figure>



<blockquote>
<p>7）即可在 hql 中使用自定义的函数</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> my_len(name) <span class="keyword">from</span> business;</span><br><span class="line">FAILED: SemanticException [Error <span class="number">10011</span>]: Invalid <span class="keyword">function</span> my_len</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>hive<span class="operator">-</span>demo<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line">Added [<span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>hive<span class="operator">-</span>demo<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar] <span class="keyword">to</span> class path</span><br><span class="line">Added resources: [<span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>hive<span class="operator">-</span>demo<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar]</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">create</span> temporary <span class="keyword">function</span> my_len <span class="keyword">as</span> &quot;com.atguigu.udf.MyUDF&quot;;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.062</span> seconds</span><br><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> my_len(name) <span class="keyword">from</span> business;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">3.51</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="自定义-UDTF-函数"><a href="#自定义-UDTF-函数" class="headerlink" title="自定义 UDTF 函数"></a>自定义 UDTF 函数</h2><blockquote>
<p>0）需求</p>
</blockquote>
<p>自定义一个 UDTF 实现将一个任意分割符的字符串切割成独立的单词，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive(<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> my_len(&quot;hello,world,hadoop,hive&quot;,&quot;,&quot;);</span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">hadoop</span><br><span class="line">hive</span><br></pre></td></tr></table></figure>



<blockquote>
<p>1）代码实现</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.inject.Inject;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*<span class="doctag">@author</span>:左泽林</span></span><br><span class="line"><span class="comment">*<span class="doctag">@date</span>:日期:2021-12-20-时间:17:40</span></span><br><span class="line"><span class="comment">*<span class="doctag">@message</span>:输入数据：hello,atguigu,give</span></span><br><span class="line"><span class="comment">*输出：</span></span><br><span class="line"><span class="comment">* hello</span></span><br><span class="line"><span class="comment">* atguigu</span></span><br><span class="line"><span class="comment">* give</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyUDTF</span> <span class="keyword">extends</span> <span class="title class_">GenericUDTF</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出数据的集合</span></span><br><span class="line"> <span class="keyword">private</span> ArrayList&lt;String&gt; outPutList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="keyword">public</span> StructObjectInspector <span class="title function_">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出数据的默认名，可以被别名覆盖</span></span><br><span class="line"> List&lt;String&gt; fieldNames=  <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;();</span><br><span class="line"> fieldNames.add(<span class="string">&quot;word&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出数据的类型</span></span><br><span class="line"> List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;ObjectInspector&gt;();</span><br><span class="line"> fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line"><span class="comment">//最终返回值</span></span><br><span class="line"> <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//处理输入数据的方法:hello,atguigu,give</span></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//1.取出输出数据</span></span><br><span class="line"> <span class="type">String</span> <span class="variable">input</span> <span class="operator">=</span> args[<span class="number">0</span>].toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.按照“，”分割字符串</span></span><br><span class="line"> String[] words = input.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.遍历数据的写出</span></span><br><span class="line"> <span class="keyword">for</span> (String word : words)&#123;</span><br><span class="line"><span class="comment">//清空集合</span></span><br><span class="line"> outPutList.clear();</span><br><span class="line"><span class="comment">//将数据放入集合</span></span><br><span class="line"> outPutList.add(word);</span><br><span class="line"><span class="comment">//输出数据</span></span><br><span class="line"> forward(outPutList);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//收尾方法</span></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）打成 jar 包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;myudtf.jar </p>
<p>3）将 jar 包添加到 hive 的 classpath 下</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hive<span class="operator">/</span>data<span class="operator">/</span>myudtf.jar;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）创建临时函数与开发好的 java class 关联</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span>  <span class="keyword">create</span> temporary <span class="keyword">function</span> my_len <span class="keyword">as</span> &quot;com.atguigu.udf.MyUDTF&quot;;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>5）使用自定义的函数</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (hive3)<span class="operator">&gt;</span> <span class="keyword">select</span> my_len(&quot;hello,world,hadoop,hive&quot;);</span><br><span class="line">OK</span><br><span class="line">word</span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">hadoop</span><br><span class="line">hive</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">3.614</span> seconds, Fetched: <span class="number">4</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>



<h2 id="grouping-set"><a href="#grouping-set" class="headerlink" title="grouping_set"></a>grouping_set</h2><h1 id="第9-章-压缩和存储"><a href="#第9-章-压缩和存储" class="headerlink" title="第9 章 压缩和存储"></a>第9 章 压缩和存储</h1><h2 id="Hadoop-压缩配置"><a href="#Hadoop-压缩配置" class="headerlink" title="Hadoop 压缩配置"></a>Hadoop 压缩配置</h2><h3 id="MR-支持的压缩编码"><a href="#MR-支持的压缩编码" class="headerlink" title="MR 支持的压缩编码"></a>MR 支持的压缩编码</h3><table>
<thead>
<tr>
<th>压缩格式</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码&#x2F;解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211221085443340.png" alt="image-20211221085443340"></p>
<h3 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h3><p>要在 Hadoop 中启用压缩，可以配置如下参数（mapred-site.xml 文件中）：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211221091907933.png" alt="image-20211221091907933"></p>
<h2 id="开启-Map-输出阶段压缩（MR-引擎）"><a href="#开启-Map-输出阶段压缩（MR-引擎）" class="headerlink" title="开启 Map 输出阶段压缩（MR 引擎）"></a>开启 Map 输出阶段压缩（MR 引擎）</h2><p>开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。具体配置如下：</p>
<blockquote>
<p>1）案例实操：</p>
</blockquote>
<p>（1）开启 hive 中间传输数据压缩功能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>（2）开启 mapreduce 中 map 输出压缩功能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>（3）设置 mapreduce 中 map 输出数据的压缩方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>（4）执行查询语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(ename) name <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<h2 id="开启-Reduce-输出阶段压缩"><a href="#开启-Reduce-输出阶段压缩" class="headerlink" title="开启 Reduce 输出阶段压缩"></a>开启 Reduce 输出阶段压缩</h2><p>当 Hive 将输出写入到表中时，输出内容同样可以进行压缩。属性 hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为 true，来开启输出结果压缩功能。</p>
<blockquote>
<p>1）案例实操：</p>
</blockquote>
<p>（1）开启 hive 最终输出数据压缩功能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>（2）开启 mapreduce 最终输出数据压缩</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>（3）设置 mapreduce 最终数据输出压缩方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>（4）设置 mapreduce 最终数据输出压缩为块压缩</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br></pre></td></tr></table></figure>

<p>（5）测试一下输出结果是否是压缩文件</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/data/distribute-result&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>



<h2 id="文件存储格式"><a href="#文件存储格式" class="headerlink" title="文件存储格式"></a>文件存储格式</h2><p><strong>Hive 支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</strong></p>
<h3 id="列式存储和行式存储"><a href="#列式存储和行式存储" class="headerlink" title="列式存储和行式存储"></a>列式存储和行式存储</h3><p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211224201811178.png" alt="image-20211224201811178"></p>
<p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<blockquote>
<p>1）行存储的特点</p>
</blockquote>
<p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<blockquote>
<p>2）列存储的特点</p>
</blockquote>
<p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p>TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的；</p>
<p>ORC 和 PARQUET 是基于列式存储的。</p>
<h3 id="TextFile-格式"><a href="#TextFile-格式" class="headerlink" title="TextFile 格式"></a>TextFile 格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合 Gzip、Bzip2 使用，但使用 Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="Orc-格式"><a href="#Orc-格式" class="headerlink" title="Orc 格式"></a>Orc 格式</h3><p>Orc (Optimized Row Columnar)是 Hive 0.11 版里引入的新的存储格式。</p>
<p>如下图所示可以看到每个 Orc 文件由1 个或多个 stripe 组成，每个 stripe 一般为 HDFS 的块大小，每一个 stripe 包含多条记录，这些记录按照列进行独立存储，对应到 Parquet 中的 row group 的概念。每个 Stripe 里有三部分组成，分别是 Index Data，Row Data，Stripe  Footer：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20211224202248580.png" alt="image-20211224202248580"></p>
<p>1）Index Data：一个轻量级的 index，默认是每隔1W 行做一个索引。这里做的索引应该只是记录某行的各字段在 Row Data 中的 offset。</p>
<p>2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 Stream 来存储。</p>
<p>3）Stripe Footer：存的是各个 Stream 的类型，长度等信息。</p>
<p>每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个 PostScript，这里面记录了整个文件的压缩类型以及 FileFooter 的长度信息等。在读取文件时，会 seek 到文件尾部读 PostScript，从里面解析到 File Footer 长度，再读 FileFooter，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读。</p>
<h3 id="Parquet-格式"><a href="#Parquet-格式" class="headerlink" title="Parquet 格式"></a>Parquet 格式</h3><p>Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此 Parquet 格式文件是自解析的。</p>
<p>（1）行组(Row Group)：每一个行组包含一定的行数，在一个 HDFS 文件中至少存储一个行组，类似于 orc 的 stripe 的概念。</p>
<p>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p>
<p>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p>
<p>通常情况下，在存储 Parquet 数据的时候会按照 Block 大小设置行组的大小，由于一般 情况下每一个 Mapper 任务处理数据的最小单位是一个 Block，这样可以把每一个行组由一 个 Mapper 任务处理，增大任务执行并行度。Parquet 文件的格式。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111104237244.png" alt="image-20220111104237244"></p>
<p>上图展示了一个 Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是 该文件的 Magic Code，用于校验它是否是一个 Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行 组的元数据信息和该文件存储数据的 Schema 信息。除了文件中每一个行组的元数据，每一 页的开始都会存储该页的元数据，在 Parquet 中，有三种类型的页：<strong>数据页、字典页和索引 页</strong>。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最 多包含一个字典页，索引页用来存储当前行组下该列的索引，目前 Parquet 中还不支持索引 页。</p>
<h3 id="主流文件存储格式对比实验"><a href="#主流文件存储格式对比实验" class="headerlink" title="主流文件存储格式对比实验"></a>主流文件存储格式对比实验</h3><p>从存储文件的压缩比和查询速度两个角度对比。 </p>
<p>存储文件的压缩比测试：</p>
<blockquote>
<p>1）测试数据</p>
</blockquote>
<p>上传测试数据：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111110616751.png" alt="image-20220111110616751"></p>
<blockquote>
<p>2）TextFile </p>
</blockquote>
<p>（1）创建表，存储数据格式为 TEXTFILE</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/log.data&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> log_text;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> default.log_text</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">7.286</span> seconds</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span>  dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_text;</span><br><span class="line"><span class="number">18.1</span> M  <span class="number">54.4</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_text<span class="operator">/</span>log.data</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）ORC</p>
</blockquote>
<p>（1）创建表，存储数据格式为 ORC</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;NONE&quot;); <span class="comment">-- 设置 orc 存储不使用压缩</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>（2）向表中加载数据 </p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text; </span><br></pre></td></tr></table></figure>

<blockquote>
<p>（3）查看表中数据大小 </p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc<span class="operator">/</span> ;</span><br><span class="line"> <span class="number">7.7</span> M <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc<span class="operator">/</span><span class="number">000000</span>_0</span><br></pre></td></tr></table></figure>



<blockquote>
<p>4）Parquet </p>
</blockquote>
<p>（1）创建表，存储数据格式为 parquet</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_parquet <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text; </span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_parquet<span class="operator">/</span>; </span><br><span class="line"><span class="number">13.1</span> M <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_parquet<span class="operator">/</span><span class="number">000000</span>_0</span><br></pre></td></tr></table></figure>

<p><strong>存储文件的对比总结： ORC &gt; Parquet &gt; textFile</strong></p>
<p>存储文件的查询速度测试：</p>
<p>（1）TextFile </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory  <span class="string">&#x27;/opt/module/data/log_text&#x27;</span> <span class="keyword">select</span> <span class="built_in">substring</span>(url,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">from</span> log_text; </span><br></pre></td></tr></table></figure>

<p>（2）ORC </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory  <span class="string">&#x27;/opt/module/data/log_orc&#x27;</span> <span class="keyword">select</span> <span class="built_in">substring</span>(url,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">from</span> log_orc; </span><br></pre></td></tr></table></figure>

<p>（3）Parquet </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory  <span class="string">&#x27;/opt/module/data/log_parquet&#x27;</span> <span class="keyword">select</span> <span class="built_in">substring</span>(url,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">from</span>  log_parquet; </span><br></pre></td></tr></table></figure>

<p><strong>存储文件的查询速度总结：查询速度相近。</strong></p>
<h2 id="存储和压缩结合"><a href="#存储和压缩结合" class="headerlink" title="存储和压缩结合"></a>存储和压缩结合</h2><h3 id="测试存储和压缩"><a href="#测试存储和压缩" class="headerlink" title="测试存储和压缩"></a>测试存储和压缩</h3><p>官网：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a> </p>
<p>ORC 存储方式的压缩：</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111115446173.png" alt="image-20220111115446173"></p>
<p>注意：所有关于 ORCFile 的参数都是在 HQL 语句的 TBLPROPERTIES 字段里面出现。</p>
<blockquote>
<p>1）创建一个 ZLIB 压缩的 ORC 存储方式</p>
</blockquote>
<p>（1）建表语句 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_zlib(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;ZLIB&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_orc_zlib <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text; </span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc_zlib<span class="operator">/</span> ; </span><br><span class="line"><span class="number">2.78</span> M <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc_none<span class="operator">/</span><span class="number">000000</span>_0</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）创建一个 SNAPPY 压缩的 ORC 存储方式 </p>
</blockquote>
<p>（1）建表语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_orc_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text; </span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc_snappy<span class="operator">/</span>; </span><br><span class="line"><span class="number">3.75</span> M <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc_snappy<span class="operator">/</span><span class="number">000000</span>_0 </span><br></pre></td></tr></table></figure>

<p>ZLIB 比 Snappy 压缩的还小。原因是 ZLIB 采用的是 deflate 压缩算法。比 snappy 压缩的 压缩率高。</p>
<blockquote>
<p>3）创建一个 SNAPPY 压缩的 parquet 存储方式</p>
</blockquote>
<p>（1）建表语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet_snappy(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> parquet</span><br><span class="line">tblproperties(&quot;parquet.compression&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_parquet_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text; </span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_parquet_snappy<span class="operator">/</span>; </span><br><span class="line"><span class="number">6.39</span> MB <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span> log_parquet_snappy <span class="operator">/</span><span class="number">000000</span>_0</span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）存储方式和压缩总结 </p>
<p>在实际的项目开发当中，hive 表的数据存储格式一般选择：orc 或 parquet。压缩方式一 般选择 snappy，lzo。</p>
</blockquote>
<h1 id="第-10-章-企业级调优"><a href="#第-10-章-企业级调优" class="headerlink" title="第 10 章 企业级调优"></a>第 10 章 企业级调优</h1><h2 id="执行计划（Explain）"><a href="#执行计划（Explain）" class="headerlink" title="执行计划（Explain）"></a>执行计划（Explain）</h2><blockquote>
<p>1）基本语法</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）案例实操 </p>
</blockquote>
<p>（1）查看下面这条语句的执行计划 </p>
<p>没有生成 MR 任务的</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        TableScan</span><br><span class="line">          alias: emp</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          <span class="keyword">Select</span> Operator</span><br><span class="line">            expressions: empno (type: <span class="type">int</span>), ename (type: string), job (type: string), mgr (type: <span class="type">int</span>), hiredate (type: string), sal (type: <span class="keyword">double</span>), comm (type: <span class="keyword">double</span>), deptno (type: <span class="type">int</span>)</span><br><span class="line">            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            ListSink</span><br><span class="line"></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">14.564</span> seconds, Fetched: <span class="number">17</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>有生成 MR 任务的</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>  #stage<span class="number">-1</span>依赖于stage<span class="number">-0</span>阶段</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree: #Map操作</span><br><span class="line">          TableScan</span><br><span class="line">            alias: emp #扫描的表</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Select</span> Operator #查询的操作</span><br><span class="line">              expressions: sal (type: <span class="keyword">double</span>), deptno (type: <span class="type">int</span>)</span><br><span class="line">              outputColumnNames: sal, deptno</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Group</span> <span class="keyword">By</span> Operator  #读取的字段</span><br><span class="line">                aggregations: <span class="built_in">sum</span>(sal), <span class="built_in">count</span>(sal)</span><br><span class="line">                keys: deptno (type: <span class="type">int</span>)</span><br><span class="line">                mode: hash</span><br><span class="line">                outputColumnNames: _col0, _col1, _col2</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                Reduce Output Operator</span><br><span class="line">                  key expressions: _col0 (type: <span class="type">int</span>)</span><br><span class="line">                  sort <span class="keyword">order</span>: <span class="operator">+</span></span><br><span class="line">                  Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: <span class="type">int</span>)</span><br><span class="line">                  Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  <span class="keyword">value</span> expressions: _col1 (type: <span class="keyword">double</span>), _col2 (type: <span class="type">bigint</span>)</span><br><span class="line">      Execution mode: vectorized</span><br><span class="line">      Reduce Operator Tree: #reduce操作</span><br><span class="line">        <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">          aggregations: <span class="built_in">sum</span>(VALUE._col0), <span class="built_in">count</span>(VALUE._col1)</span><br><span class="line">          keys: KEY._col0 (type: <span class="type">int</span>)</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0, _col1, _col2</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          <span class="keyword">Select</span> Operator</span><br><span class="line">            expressions: _col0 (type: <span class="type">int</span>), (_col1 <span class="operator">/</span> _col2) (type: <span class="keyword">double</span>)</span><br><span class="line">            outputColumnNames: _col0, _col1</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            File Output Operator</span><br><span class="line">              compressed: <span class="literal">false</span></span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6460</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">table</span>:</span><br><span class="line">                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span> #第二个阶段</span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br><span class="line"></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">2.475</span> seconds, Fetched: <span class="number">53</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（2）查看详细执行计划（多加一个extends）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>



<h2 id="Fetch-抓取"><a href="#Fetch-抓取" class="headerlink" title="Fetch 抓取"></a>Fetch 抓取</h2><p>Fetch 抓取是指，<strong>Hive 中对某些情况的查询可以不必使用 MapReduce 计算</strong>。例如：SELECT  * FROM employees;在这种情况下，Hive 可以简单地读取 employee 对应的存储目录下的文件， 然后输出查询结果到控制台。</p>
<p>在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive 默认是 <strong>minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走 mapreduce</strong>。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line"> Expects one of [none, minimal, more].Some select queries can be converted to single FETCH task minimizing </span><br><span class="line">latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and </span><br><span class="line">virtual columns --虚拟字段)</span><br><span class="line"> <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）案例实操： </p>
</blockquote>
<p>（1）把 hive.fetch.task.conversion 设置成 none，然后执行查询语句，都会执行 mapreduce 程序。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.fetch.task.conversion<span class="operator">=</span><span class="keyword">none</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">from</span> emp limit <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p>（2）把 hive.fetch.task.conversion 设置成 more，然后执行查询语句，如下查询方式都不 会执行 mapreduce 程序。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.fetch.task.conversion<span class="operator">=</span>more; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">from</span> emp; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">from</span> emp limit <span class="number">3</span>;</span><br></pre></td></tr></table></figure>



<h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p>大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过， 有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能 会比实际 job 的执行时间要多的多。对于大多数这种情况，<strong>Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</strong>。</p>
<p>用户可以通过设置 hive.exec.mode.local.auto 的值为 true，来让 Hive 在适当的时候自动 启动这个优化。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>; <span class="operator">/</span><span class="operator">/</span>开启本地 mr</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>设置 <span class="keyword">local</span> mr 的最大输入数据量，当输入数据量小于这个值时采用 <span class="keyword">local</span> mr 的方式，默认为 <span class="number">134217728</span>，即 <span class="number">128</span>M</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max<span class="operator">=</span><span class="number">50000000</span>;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>设置 <span class="keyword">local</span> mr 的最大输入文件个数，当输入文件个数小于这个值时采用 <span class="keyword">local</span> mr 的方式，默认为 <span class="number">4</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max<span class="operator">=</span><span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1）案例实操：</p>
</blockquote>
<p>（1）开启本地模式，并执行查询语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>

<p>（2）关闭本地模式（默认是关闭的），并执行查询语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno; </span><br></pre></td></tr></table></figure>



<h2 id="表的优化"><a href="#表的优化" class="headerlink" title="表的优化"></a>表的优化</h2><h3 id="小表大表-Join（MapJOIN）"><a href="#小表大表-Join（MapJOIN）" class="headerlink" title="小表大表 Join（MapJOIN）"></a>小表大表 Join（MapJOIN）</h3><p>将 key 相对分散，并且数据量小的表放在 join 的左边，可以使用 map join 让小的维度表 先进内存。</p>
<p>**在 map 端完成 join。 **</p>
<p><strong>实际测试发现：新版的 hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化。小表放 在左边和右边已经没有区别</strong>。</p>
<p>案例实操</p>
<blockquote>
<p>1）需求介绍</p>
</blockquote>
<p>测试大表 JOIN 小表和小表 JOIN 大表的效率</p>
<blockquote>
<p>2）开启 MapJoin 参数设置</p>
</blockquote>
<p>（1）设置自动选择Mapjoin </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; </span><br><span class="line">默认为 <span class="literal">true</span> </span><br></pre></td></tr></table></figure>

<p>（2）大表小表的阈值设置（默认 25M 以下认为是小表）： </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize <span class="operator">=</span> <span class="number">25000000</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）MapJoin 工作机制</p>
</blockquote>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111152924695.png" alt="image-20220111152924695"></p>
<blockquote>
<p>4）建大表、小表和 JOIN 后表的语句</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建大表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string, </span><br><span class="line">url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span>;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建小表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string, </span><br><span class="line">url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span>;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建 <span class="keyword">join</span> 后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string, </span><br><span class="line">url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>5）分别向大表和小表中导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span>load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/smalltable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>6）小表 JOIN 大表语句</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">join</span> bigtable b</span><br><span class="line"><span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>7）大表 JOIN 小表语句</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> smalltable s</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>





<h3 id="大表-Join-大表"><a href="#大表-Join-大表" class="headerlink" title="大表 Join 大表"></a>大表 Join 大表</h3><blockquote>
<p>1）空 KEY 过滤</p>
</blockquote>
<p>(inner join不需要)有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同 的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下， 这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。例如 key 对应的字段为空，操作如下：</p>
<p><strong>案例实操</strong> </p>
<p>（1）配置历史服务器 </p>
<p>配置 mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>启动历史服务器 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin<span class="operator">/</span>mr<span class="operator">-</span>jobhistory<span class="operator">-</span>daemon.sh <span class="keyword">start</span> historyserver </span><br></pre></td></tr></table></figure>

<p>查看 jobhistory </p>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<p>（2）创建原始数据空 id 表 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建空 id 表 </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> nullidtable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string,  url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited  fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>（3）分别加载原始数据和空 id 数据到对应表中：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/nullid&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> nullidtable;</span><br></pre></td></tr></table></figure>

<p>（4）测试不过滤空 id </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span>  nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span> n.id <span class="operator">=</span> o.id; </span><br></pre></td></tr></table></figure>

<p>（5）测试过滤空 id </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> (<span class="keyword">select</span>  <span class="operator">*</span> <span class="keyword">from</span> nullidtable <span class="keyword">where</span> id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span>) n <span class="keyword">left</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span> n.id <span class="operator">=</span>  o.id;</span><br></pre></td></tr></table></figure>

<p><strong>空Key过滤使用的场景</strong></p>
<ol>
<li>非 inner join </li>
<li>不需要字段为null的</li>
</ol>
<blockquote>
<p>2）空 key 转换</p>
</blockquote>
<p>有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上（防止数据倾斜）。例如：</p>
<p>案例实操： </p>
<p>不随机分布空 null 值： </p>
<p>（1）设置 5 个 reduce 个数 </p>
<p>set mapreduce.job.reduces &#x3D; 5; </p>
<p>（2）JOIN 两张表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> bigtable b <span class="keyword">on</span> n.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>

<p>结果：如下图所示，可以看出来，出现了数据倾斜，某些 reducer 的资源消耗远大于其 他 reducer。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111191309660.png" alt="image-20220111191309660"></p>
<p>随机分布空 null 值 </p>
<p>（1）设置 5 个 reduce 个数</p>
<p>set mapreduce.job.reduces &#x3D; 5; </p>
<p>（2）JOIN 两张表 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span>  nvl(n.id,rand()) <span class="operator">=</span> o.id; </span><br></pre></td></tr></table></figure>

<p>这里给的随机数需要他们的id不能相同</p>
<p>结果：如下图所示，可以看出来，消除了数据倾斜，负载均衡 reducer 的资源消耗</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111191620462.png" alt="image-20220111191620462"></p>
<blockquote>
<p>3）SMB(Sort Merge Bucket join) 分桶表</p>
</blockquote>
<p>进行表的拆分</p>
<p>（1）创建第二张大表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable2(</span><br><span class="line">     id <span class="type">bigint</span>,</span><br><span class="line">     t <span class="type">bigint</span>,</span><br><span class="line">     uid string,</span><br><span class="line">     keyword string,</span><br><span class="line">     url_rank <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable2;</span><br></pre></td></tr></table></figure>

<p>测试大表直接 JOIN</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable s</span><br><span class="line"><span class="keyword">join</span> bigtable2 b</span><br><span class="line"><span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br></pre></td></tr></table></figure>

<p>（2）创建分通表 1,桶的个数不要超过可用 CPU 的核数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck1(</span><br><span class="line">     id <span class="type">bigint</span>,</span><br><span class="line">     t <span class="type">bigint</span>,</span><br><span class="line">     uid string,</span><br><span class="line">     keyword string,</span><br><span class="line">     url_rank <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(id) </span><br><span class="line">sorted <span class="keyword">by</span>(id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">6</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">bigtable_buck1;</span><br></pre></td></tr></table></figure>

<p>（3）创建分通表 2,桶的个数不要超过可用 CPU 的核数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck2(</span><br><span class="line">     id <span class="type">bigint</span>,</span><br><span class="line">     t <span class="type">bigint</span>,</span><br><span class="line">     uid string,</span><br><span class="line">     keyword string,</span><br><span class="line">     url_rank <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string)</span><br><span class="line">clustered <span class="keyword">by</span>(id)</span><br><span class="line">sorted <span class="keyword">by</span>(id) </span><br><span class="line"><span class="keyword">into</span> <span class="number">6</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">bigtable_buck2;</span><br></pre></td></tr></table></figure>

<p>（4）设置参数 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat; </span><br></pre></td></tr></table></figure>

<p>（5）测试 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url <span class="keyword">from</span> bigtable_buck1 s <span class="keyword">join</span> bigtable_buck2 b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br></pre></td></tr></table></figure>



<h3 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h3><p>默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜 了。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220111194005297.png" alt="image-20220111194005297"></p>
<p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行 部分聚合，最后在 Reduce 端得出最终结果。</p>
<blockquote>
<p>1）开启 Map 端聚合参数设置</p>
</blockquote>
<p>（1）是否在 Map 端进行聚合，默认为 True </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span> </span><br></pre></td></tr></table></figure>

<p>（2）在 Map 端进行聚合操作的条目数目 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span> </span><br></pre></td></tr></table></figure>

<p>（3）有数据倾斜的时候进行负载均衡（默认是 false） </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p><strong>当选项设定为 true，生成的查询计划会有两个 MR Job</strong>。第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二 个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">23.68</span> sec HDFS Read: </span><br><span class="line"><span class="number">19987</span> HDFS Write: <span class="number">9</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">23</span> seconds <span class="number">680</span> msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>

<p>优化以后</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">28.53</span> sec HDFS Read: </span><br><span class="line"><span class="number">18209</span> HDFS Write: <span class="number">534</span> SUCCESS</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">38.32</span> sec HDFS Read: </span><br><span class="line"><span class="number">15014</span> HDFS Write: <span class="number">9</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">1</span> minutes <span class="number">6</span> seconds <span class="number">850</span> msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>



<h3 id="Count-Distinct-去重统计"><a href="#Count-Distinct-去重统计" class="headerlink" title="Count(Distinct) 去重统计"></a>Count(Distinct) 去重统计</h3><p>数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个 Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成， 一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换,但是需要注意 group by 造成的数据倾斜问题。</p>
<blockquote>
<p>1）案例实操 </p>
</blockquote>
<p>（1）创建一张大表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> bigtable(id <span class="type">bigint</span>, <span class="type">time</span> <span class="type">bigint</span>, uid string, </span><br><span class="line">keyword</span><br><span class="line">string, url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format </span><br><span class="line">delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>（2）加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br></pre></td></tr></table></figure>

<p>（3）设置 5 个 reduce 个数 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure>

<p>（4）执行去重 id 查询</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> id) <span class="keyword">from</span> bigtable;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span> Reduce: <span class="number">1</span> Cumulative CPU: <span class="number">7.12</span> sec HDFS Read: </span><br><span class="line"><span class="number">120741990</span> HDFS Write: <span class="number">7</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">7</span> seconds <span class="number">120</span> msec</span><br><span class="line">OK</span><br><span class="line">c0</span><br><span class="line"><span class="number">100001</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">23.607</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>（5）采用 GROUP by 去重 id</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(id) <span class="keyword">from</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> id) a;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">17.53</span> sec HDFS Read: </span><br><span class="line"><span class="number">120752703</span> HDFS Write: <span class="number">580</span> SUCCESS</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span> Reduce: <span class="number">1</span> Cumulative CPU: <span class="number">4.29</span> sec2 HDFS Read: </span><br><span class="line"><span class="number">9409</span> HDFS Write: <span class="number">7</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">21</span> seconds <span class="number">820</span> msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line"><span class="number">100001</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">50.795</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>虽然会多用一个 Job 来完成，但在数据量大的情况下，这个绝对是值得的。</p>
<h3 id="笛卡尔积-1"><a href="#笛卡尔积-1" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><p>尽量避免笛卡尔积，join 的时候不加 on 条件，或者无效的 on 条件，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p>
<h3 id="行列过滤"><a href="#行列过滤" class="headerlink" title="行列过滤"></a>行列过滤</h3><p>列处理：在 SELECT 中，只拿需要的列，如果有分区，尽量使用分区过滤，少用 SELECT  *。</p>
<p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面， 那么就会先全表关联，之后再过滤，比如：</p>
<p>案例实操：</p>
<blockquote>
<p>1）测试先关联两张表，再用 where 条件过滤 </p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> bigtable o <span class="keyword">on</span> o.id <span class="operator">=</span> b.id <span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>; </span><br><span class="line"><span class="type">Time</span> taken: <span class="number">34.406</span> seconds, Fetched: <span class="number">100</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）通过子查询后，再关联表</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o <span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">30.058</span> seconds, Fetched: <span class="number">100</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>详见 7.1 章。 </p>
<h3 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h3><p>详见 7.2 章。</p>
<h2 id="合理设置-Map-及-Reduce-数"><a href="#合理设置-Map-及-Reduce-数" class="headerlink" title="合理设置 Map 及 Reduce 数"></a>合理设置 Map 及 Reduce 数</h2><p>1）通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。 </p>
<p>主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。 </p>
<p>2）是不是 map 数越多越好？ </p>
<p>答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件 也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大 于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。 </p>
<p>3）是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？ </p>
<p>答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只 有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map 任务去做，肯定也比较耗时。针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数；</p>
<h3 id="复杂文件增加-Map-数"><a href="#复杂文件增加-Map-数" class="headerlink" title="复杂文件增加 Map 数"></a>复杂文件增加 Map 数</h3><p>当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数， 来使得每个 map 处理的数据量减少，从而提高任务的执行效率。</p>
<p>增加 map 的方法为：根据 computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M 公式， 调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p>
<p>案例实操：</p>
<blockquote>
<p>1）执行查询</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> </span><br><span class="line">reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）设置最大切片值为 100 个字节</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">6</span>; number <span class="keyword">of</span> </span><br><span class="line">reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h3 id="小文件进行合并"><a href="#小文件进行合并" class="headerlink" title="小文件进行合并"></a>小文件进行合并</h3><p>1）在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合 并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>  org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>2）在 Map-Reduce 的任务结束时合并小文件的设置：</p>
<p>在 map-only 任务结束时合并小文件，默认 true </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure>

<p>在 map-reduce 任务结束时合并小文件，默认 false </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure>

<p>合并文件的大小，默认 256M </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">268435456</span>; </span><br></pre></td></tr></table></figure>

<p>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.smallfiles.avgsize <span class="operator">=</span> <span class="number">16777216</span>;</span><br></pre></td></tr></table></figure>

<h3 id="合理设置-Reduce-数"><a href="#合理设置-Reduce-数" class="headerlink" title="合理设置 Reduce 数"></a>合理设置 Reduce 数</h3><blockquote>
<p>1）调整 reduce 个数方法一 </p>
</blockquote>
<p>（1）每个 Reduce 处理的数据量默认是 256MB </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer=256000000</span><br></pre></td></tr></table></figure>

<p>（2）每个任务最大的 reduce 数，默认为 1009 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.max=1009 </span><br></pre></td></tr></table></figure>

<p>（3）计算 reducer 数的公式 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N=min(参数 2，总输入数据量/参数 1)</span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）调整 reduce 个数方法二 </p>
</blockquote>
<p>在 hadoop 的 mapred-default.xml 文件中修改 </p>
<p>设置每个 job 的 Reduce 个数 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces = 15;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）reduce 个数并不是越多越好</p>
</blockquote>
<p>（1）过多的启动和初始化 reduce 也会消耗时间和资源； </p>
<p>（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那 么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题； </p>
<p>在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数； 使单个 reduce 任务处理数据量大小要合适；</p>
<h2 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h2><p>Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是 MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者 Hive 执行过程中可能需要的其他阶段。默认情况下， Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。 </p>
<p>通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中， 需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>; <span class="operator">/</span><span class="operator">/</span>打开任务并行执行</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">16</span>; <span class="operator">/</span><span class="operator">/</span>同一个 <span class="keyword">sql</span> 允许最大并行度，默认为<span class="number">8</span>。</span><br></pre></td></tr></table></figure>

<p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p>
<h2 id="严格模式"><a href="#严格模式" class="headerlink" title="严格模式"></a>严格模式</h2><p>Hive 可以通过设置防止一些危险操作：</p>
<blockquote>
<p>1）分区表不使用分区过滤</p>
</blockquote>
<p>将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，<strong>除非 where 语句中含 有分区字段过滤条件来限制范围，否则不允许执行</strong>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有 进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p>
<blockquote>
<p>2）使用 order by 没有 limit 过滤 </p>
</blockquote>
<p>将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一 段时间。</p>
<blockquote>
<p>3）笛卡尔积 </p>
</blockquote>
<p>将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数 据库非常了解的用户可能期望在 执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语 句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸 的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情 况。</p>
<h2 id="JVM-重用"><a href="#JVM-重用" class="headerlink" title="JVM 重用"></a>JVM 重用</h2><p>开启在hadoop的marped-site.xml文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.reuse.jvm.num.tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span> <span class="comment">&lt;!--10表示JVM实例在同一个job种重新使用10次--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>优点：</strong></p>
<p>避免jvm的启动过程造成相当大的资源开销。特别是在大量小文件的场景下。</p>
<p><strong>缺点：</strong></p>
<p>开启JVM重用将会一直占用使用到的task插槽，以便进行重用，直到任务完成后才释放，</p>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>详见第 9 章。</p>
<h1 id="第-11-章-HiveQL索引"><a href="#第-11-章-HiveQL索引" class="headerlink" title="第 11 章 HiveQL索引"></a>第 11 章 HiveQL索引</h1><p>我们可以对一些字段建立索引来加速某些操作。</p>
<p>一张表的索引数据存储在另一张表中。</p>
<p><strong>优点：</strong> 建立索引可以帮助减裁掉一些数据块，这样能减少MpReduce的输入数据量。</p>
<p><strong>缺点</strong>： 维护索引也需要额外的存储空间，同时创建索引也需要消耗计算资源。</p>
<p>建立索引需要权衡一下。</p>
<h2 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> index employee_index</span><br><span class="line"><span class="keyword">on</span> <span class="keyword">table</span> employees (country)</span><br><span class="line"><span class="keyword">as</span> <span class="string">&#x27;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#x27;</span></span><br><span class="line"><span class="keyword">with</span> deferred rebuild</span><br><span class="line">inxproperties (<span class="string">&#x27;createor&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;me&#x27;</span>,<span class="string">&#x27;created_at&#x27;</span><span class="operator">=</span><span class="string">&#x27;some_time&#x27;</span>)</span><br><span class="line"><span class="keyword">in</span> <span class="keyword">table</span> employees_index_table</span><br><span class="line">partitioned <span class="keyword">by</span> (country,name)</span><br><span class="line"><span class="keyword">commit</span> <span class="string">&#x27;employees indexed by country and name.&#x27;</span></span><br></pre></td></tr></table></figure>

<p>如果没有指定分区的话，那么默认的就是所有分区</p>
<p>AS 后面是指定索引的处理器，也就是一个实现了索引接口的Java类。</p>
<p><strong>bitmap索引</strong>：bitmap索引普遍应用于排重后值较少的列。</p>
<h2 id="重建索引"><a href="#重建索引" class="headerlink" title="重建索引"></a>重建索引</h2><p><strong>重建索引的操作是原子性的。</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> deferred rebuild</span><br></pre></td></tr></table></figure>

<h2 id="显示索引"><a href="#显示索引" class="headerlink" title="显示索引"></a>显示索引</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> formatted index <span class="keyword">on</span> employees;</span><br></pre></td></tr></table></figure>



<h2 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h2><p>如果有索引的话，删除一个索引会删除这个索引表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop index if exists employees_index on table employees;</span><br></pre></td></tr></table></figure>

<ul>
<li>如果被索引的表删除了，那么其对应的索引和索引表也会被删除。</li>
<li>如果原始表的某个分区被删除了，那么这个分区对应的分区索引也同时被删除</li>
</ul>
<h1 id="第-12-章-锁"><a href="#第-12-章-锁" class="headerlink" title="第 12 章 锁"></a>第 12 章 锁</h1><p>当一张表被两个用户同时操作时，可能会造成后面操作的用户产生失败或无效的结果。</p>
<h2 id="Hive结合Zookeeper支持锁功能"><a href="#Hive结合Zookeeper支持锁功能" class="headerlink" title="Hive结合Zookeeper支持锁功能"></a>Hive结合Zookeeper支持锁功能</h2><p>在hive-site.xml 这个配置文件中。我们需要增加如下熟悉配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>zk1.site.pvt,zk1.site.pvt,zk1.site.pvt<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>要与之·对话的zookeeper服务器列表，仅用于读、写锁<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>配置单元是否支持并发，zookeeper实例必须启动并运行，默认配置单元锁管理器才能支持读写锁<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置好属性后，hive会对特定的查询自动启动获取锁，用户可以通过show locks命令查看当前所有锁。</p>
<h2 id="显示锁和独占锁"><a href="#显示锁和独占锁" class="headerlink" title="显示锁和独占锁"></a>显示锁和独占锁</h2><p>用户可以显示的管理锁：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lock <span class="keyword">table</span> people exclusive;</span><br></pre></td></tr></table></figure>

<p>对表进行解锁：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unlock <span class="keyword">table</span> people;</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">DingQuan Zuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/">http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">ccbigs blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="/img/ahead.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/" title="大数据——Hive调优"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据——Hive调优</div></div></a></div><div class="next-post pull-right"><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94ClickHouse/" title="大数据——ClickHouse"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据——ClickHouse</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hbase/" title="大数据——Hbase"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">大数据——Hbase</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Flume/" title="大数据——Flume"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Flume</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Scala/" title="大数据——Scala"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Scala</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94ClickHouse/" title="大数据——ClickHouse"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——ClickHouse</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/" title="大数据——Hive调优"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Hive调优</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/" title="大数据——Hadoop.2.x"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Hadoop.2.x</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/ahead.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">DingQuan Zuo</div><div class="author-info__description">技术路上少不了自我怀疑，往往你的决定，会让你看到不同的风景</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccbigs" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1692062014@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC1-%E7%AB%A0-Hive-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">第1 章 Hive 基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-Hive"><span class="toc-number">1.1.</span> <span class="toc-text">什么是 Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">Hive 的优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">1.2.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">Hive 架构原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AF%94%E8%BE%83"><span class="toc-number">1.4.</span> <span class="toc-text">Hive 和数据库比较</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80"><span class="toc-number">1.4.1.</span> <span class="toc-text">查询语言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">数据更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%BB%B6%E8%BF%9F"><span class="toc-number">1.4.3.</span> <span class="toc-text">执行延迟</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1"><span class="toc-number">1.4.4.</span> <span class="toc-text">数据规模</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC2-%E7%AB%A0-Hive-%E5%AE%89%E8%A3%85"><span class="toc-number">2.</span> <span class="toc-text">第2 章 Hive 安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%AE%89%E8%A3%85%E5%9C%B0%E5%9D%80"><span class="toc-number">2.1.</span> <span class="toc-text">Hive 安装地址</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">2.2.</span> <span class="toc-text">Hive 安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-Hive"><span class="toc-number">2.2.1.</span> <span class="toc-text">安装 Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%B9%B6%E4%BD%BF%E7%94%A8-Hive"><span class="toc-number">2.2.2.</span> <span class="toc-text">启动并使用 Hive</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MySQL%E5%AE%89%E8%A3%85"><span class="toc-number">2.3.</span> <span class="toc-text">MySQL安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%85%83%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE%E5%88%B0-MySQL"><span class="toc-number">2.4.</span> <span class="toc-text">Hive 元数据配置到 MySQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%B7%E8%B4%9D%E9%A9%B1%E5%8A%A8"><span class="toc-number">2.4.1.</span> <span class="toc-text">拷贝驱动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-Metastore-%E5%88%B0-MySQL"><span class="toc-number">2.4.2.</span> <span class="toc-text">配置 Metastore 到 MySQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%8D%E6%AC%A1%E5%90%AF%E5%8A%A8-Hive"><span class="toc-number">2.4.3.</span> <span class="toc-text">再次启动 Hive</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AE-Hive"><span class="toc-number">2.5.</span> <span class="toc-text">使用元数据服务的方式访问 Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-JDBC-%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AE-Hive"><span class="toc-number">2.6.</span> <span class="toc-text">使用 JDBC 方式访问 Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%B8%B8%E7%94%A8%E4%BA%A4%E4%BA%92%E5%91%BD%E4%BB%A4"><span class="toc-number">2.7.</span> <span class="toc-text">Hive 常用交互命令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%85%B6%E4%BB%96%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-number">2.8.</span> <span class="toc-text">Hive 其他命令操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%B8%B8%E8%A7%81%E5%B1%9E%E6%80%A7%E9%85%8D%E7%BD%AE"><span class="toc-number">2.9.</span> <span class="toc-text">Hive 常见属性配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E9%85%8D%E7%BD%AE"><span class="toc-number">2.9.1.</span> <span class="toc-text">Hive 运行日志信息配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%93%E5%8D%B0%E5%BD%93%E5%89%8D%E5%BA%93%E5%92%8C%E8%A1%A8%E5%A4%B4"><span class="toc-number">2.9.2.</span> <span class="toc-text">打印当前库和表头</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F"><span class="toc-number">2.9.3.</span> <span class="toc-text">参数配置方式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC3-%E7%AB%A0-Hive-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">第3 章 Hive 数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">基本数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.2.</span> <span class="toc-text">集合数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">类型转化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC4-%E7%AB%A0-DDL-%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89"><span class="toc-number">4.</span> <span class="toc-text">第4 章 DDL 数据定义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.1.</span> <span class="toc-text">创建数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.2.</span> <span class="toc-text">查询数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.2.1.</span> <span class="toc-text">显示数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%A6%E6%83%85"><span class="toc-number">4.2.2.</span> <span class="toc-text">查看数据库详情</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E6%8D%A2%E5%BD%93%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.2.3.</span> <span class="toc-text">切换当前数据库</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.3.</span> <span class="toc-text">修改数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.4.</span> <span class="toc-text">删除数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-number">4.5.</span> <span class="toc-text">创建表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%A1%E7%90%86%E8%A1%A8"><span class="toc-number">4.5.1.</span> <span class="toc-text">管理表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="toc-number">4.5.2.</span> <span class="toc-text">外部表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%A1%E7%90%86%E8%A1%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9A%84%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.5.3.</span> <span class="toc-text">管理表与外部表的互相转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E8%A1%A8"><span class="toc-number">4.6.</span> <span class="toc-text">修改表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%91%BD%E5%90%8D%E8%A1%A8"><span class="toc-number">4.6.1.</span> <span class="toc-text">重命名表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0-x2F-%E4%BF%AE%E6%94%B9-x2F-%E6%9B%BF%E6%8D%A2%E5%88%97%E4%BF%A1%E6%81%AF"><span class="toc-number">4.6.2.</span> <span class="toc-text">增加&#x2F;修改&#x2F;替换列信息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E8%A1%A8"><span class="toc-number">4.7.</span> <span class="toc-text">删除表</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC5-%E7%AB%A0-DML-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">5.</span> <span class="toc-text">第5 章 DML 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5"><span class="toc-number">5.1.</span> <span class="toc-text">数据导入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E8%A1%A8%E4%B8%AD%E8%A3%85%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88Load%EF%BC%89"><span class="toc-number">5.1.1.</span> <span class="toc-text">向表中装载数据（Load）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E5%90%91%E8%A1%A8%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%EF%BC%88Insert%EF%BC%89"><span class="toc-number">5.1.2.</span> <span class="toc-text">通过查询语句向表中插入数据（Insert）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%A1%A8%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88As-Select%EF%BC%89"><span class="toc-number">5.1.3.</span> <span class="toc-text">查询语句中创建表并加载数据（As Select）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8%E6%97%B6%E9%80%9A%E8%BF%87-Location-%E6%8C%87%E5%AE%9A%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84"><span class="toc-number">5.1.4.</span> <span class="toc-text">创建表时通过 Location 指定加载数据路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Import-%E6%95%B0%E6%8D%AE%E5%88%B0%E6%8C%87%E5%AE%9A-Hive-%E8%A1%A8%E4%B8%AD"><span class="toc-number">5.1.5.</span> <span class="toc-text">Import 数据到指定 Hive 表中</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.</span> <span class="toc-text">数据导出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Insert-%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.1.</span> <span class="toc-text">Insert 导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-number">5.2.2.</span> <span class="toc-text">Hadoop 命令导出到本地</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-Shell-%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.3.</span> <span class="toc-text">Hive Shell 命令导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Export-%E5%AF%BC%E5%87%BA%E5%88%B0-HDFS-%E4%B8%8A"><span class="toc-number">5.2.4.</span> <span class="toc-text">Export 导出到 HDFS 上</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sqoop-%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.5.</span> <span class="toc-text">Sqoop 导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B8%85%E9%99%A4%E8%A1%A8%E4%B8%AD%E6%95%B0%E6%8D%AE%EF%BC%88Truncate%EF%BC%89"><span class="toc-number">5.2.6.</span> <span class="toc-text">清除表中数据（Truncate）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC6-%E7%AB%A0%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.</span> <span class="toc-text">第6 章查询</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2%EF%BC%88Select%E2%80%A6From%EF%BC%89"><span class="toc-number">6.1.</span> <span class="toc-text">基本查询（Select…From）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%A1%A8%E5%92%8C%E7%89%B9%E5%AE%9A%E5%88%97%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.1.1.</span> <span class="toc-text">全表和特定列查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%97%E5%88%AB%E5%90%8D"><span class="toc-number">6.1.2.</span> <span class="toc-text">列别名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-number">6.1.3.</span> <span class="toc-text">算术运算符</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.4.</span> <span class="toc-text">常用函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limit-%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.1.5.</span> <span class="toc-text">Limit 语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Where-%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.1.6.</span> <span class="toc-text">Where 语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%88Between-x2F-In-x2F-Is-Null%EF%BC%89"><span class="toc-number">6.1.7.</span> <span class="toc-text">比较运算符（Between&#x2F;In&#x2F; Is Null）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Like-%E5%92%8C-RLike"><span class="toc-number">6.1.8.</span> <span class="toc-text">Like 和 RLike</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%88And-x2F-Or-x2F-Not%EF%BC%89"><span class="toc-number">6.1.9.</span> <span class="toc-text">逻辑运算符（And&#x2F;Or&#x2F;Not）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%BB%84"><span class="toc-number">6.2.</span> <span class="toc-text">分组</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Group-By-%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.2.1.</span> <span class="toc-text">Group By 语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Having-%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.2.2.</span> <span class="toc-text">Having 语句</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Join-%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.3.</span> <span class="toc-text">Join 语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%89%E5%80%BC-Join"><span class="toc-number">6.3.1.</span> <span class="toc-text">等值 Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E7%9A%84%E5%88%AB%E5%90%8D"><span class="toc-number">6.3.2.</span> <span class="toc-text">表的别名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.3.3.</span> <span class="toc-text">内连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.3.4.</span> <span class="toc-text">左外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%B3%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.3.5.</span> <span class="toc-text">右外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BB%A1%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.3.6.</span> <span class="toc-text">满外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.3.7.</span> <span class="toc-text">多表连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF"><span class="toc-number">6.3.8.</span> <span class="toc-text">笛卡尔积</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F"><span class="toc-number">6.4.</span> <span class="toc-text">排序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F%EF%BC%88Order-By%EF%BC%89"><span class="toc-number">6.4.1.</span> <span class="toc-text">全局排序（Order By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%89%E7%85%A7%E5%88%AB%E5%90%8D%E6%8E%92%E5%BA%8F"><span class="toc-number">6.4.2.</span> <span class="toc-text">按照别名排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E4%B8%AA%E5%88%97%E6%8E%92%E5%BA%8F"><span class="toc-number">6.4.3.</span> <span class="toc-text">多个列排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%8F%E4%B8%AA-Reduce-%E5%86%85%E9%83%A8%E6%8E%92%E5%BA%8F%EF%BC%88Sort-By%EF%BC%89%E9%87%8D%E7%82%B9"><span class="toc-number">6.4.4.</span> <span class="toc-text">每个 Reduce 内部排序（Sort By）重点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%EF%BC%88Distribute-By%EF%BC%89"><span class="toc-number">6.4.5.</span> <span class="toc-text">分区（Distribute By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cluster-By%EF%BC%88%E4%B8%8A%E9%9D%A2%E4%B8%A4%E4%B8%AA%E7%BB%93%E5%90%88%EF%BC%89"><span class="toc-number">6.4.6.</span> <span class="toc-text">Cluster By（上面两个结合）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC7-%E7%AB%A0%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8"><span class="toc-number">7.</span> <span class="toc-text">第7 章分区表和分桶表</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="toc-number">7.1.</span> <span class="toc-text">分区表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%A1%A8%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">7.1.1.</span> <span class="toc-text">分区表基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5"><span class="toc-number">7.1.2.</span> <span class="toc-text">分区的增删改查</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E7%BA%A7%E5%88%86%E5%8C%BA"><span class="toc-number">7.1.3.</span> <span class="toc-text">二级分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%B0%83%E6%95%B4"><span class="toc-number">7.1.4.</span> <span class="toc-text">动态分区调整</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%A1%B6%E8%A1%A8"><span class="toc-number">7.2.</span> <span class="toc-text">分桶表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%BD%E6%A0%B7%E6%9F%A5%E8%AF%A2"><span class="toc-number">7.3.</span> <span class="toc-text">抽样查询</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC8-%E7%AB%A0-%E5%87%BD%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">第8 章 函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-number">8.1.</span> <span class="toc-text">系统内置函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-number">8.2.</span> <span class="toc-text">常用内置函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%BA%E5%AD%97%E6%AE%B5%E8%B5%8B%E5%80%BC"><span class="toc-number">8.2.1.</span> <span class="toc-text">空字段赋值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CASE-WHEN-THEN-ELSE-END"><span class="toc-number">8.2.2.</span> <span class="toc-text">CASE WHEN THEN ELSE END</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E8%BD%AC%E5%88%97"><span class="toc-number">8.2.3.</span> <span class="toc-text">行转列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%97%E8%BD%AC%E8%A1%8C"><span class="toc-number">8.2.4.</span> <span class="toc-text">列转行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%EF%BC%88%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-number">8.2.5.</span> <span class="toc-text">窗口函数（开窗函数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rank"><span class="toc-number">8.2.6.</span> <span class="toc-text">Rank</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">8.3.</span> <span class="toc-text">自定义函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-UDF-%E5%87%BD%E6%95%B0"><span class="toc-number">8.4.</span> <span class="toc-text">自定义 UDF 函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-UDTF-%E5%87%BD%E6%95%B0"><span class="toc-number">8.5.</span> <span class="toc-text">自定义 UDTF 函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grouping-set"><span class="toc-number">8.6.</span> <span class="toc-text">grouping_set</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC9-%E7%AB%A0-%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8"><span class="toc-number">9.</span> <span class="toc-text">第9 章 压缩和存储</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop-%E5%8E%8B%E7%BC%A9%E9%85%8D%E7%BD%AE"><span class="toc-number">9.1.</span> <span class="toc-text">Hadoop 压缩配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MR-%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81"><span class="toc-number">9.1.1.</span> <span class="toc-text">MR 支持的压缩编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-number">9.1.2.</span> <span class="toc-text">压缩参数配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%90%AF-Map-%E8%BE%93%E5%87%BA%E9%98%B6%E6%AE%B5%E5%8E%8B%E7%BC%A9%EF%BC%88MR-%E5%BC%95%E6%93%8E%EF%BC%89"><span class="toc-number">9.2.</span> <span class="toc-text">开启 Map 输出阶段压缩（MR 引擎）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%90%AF-Reduce-%E8%BE%93%E5%87%BA%E9%98%B6%E6%AE%B5%E5%8E%8B%E7%BC%A9"><span class="toc-number">9.3.</span> <span class="toc-text">开启 Reduce 输出阶段压缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.4.</span> <span class="toc-text">文件存储格式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="toc-number">9.4.1.</span> <span class="toc-text">列式存储和行式存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TextFile-%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.4.2.</span> <span class="toc-text">TextFile 格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Orc-%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.4.3.</span> <span class="toc-text">Orc 格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parquet-%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.4.4.</span> <span class="toc-text">Parquet 格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E6%B5%81%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C"><span class="toc-number">9.4.5.</span> <span class="toc-text">主流文件存储格式对比实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%92%8C%E5%8E%8B%E7%BC%A9%E7%BB%93%E5%90%88"><span class="toc-number">9.5.</span> <span class="toc-text">存储和压缩结合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E5%AD%98%E5%82%A8%E5%92%8C%E5%8E%8B%E7%BC%A9"><span class="toc-number">9.5.1.</span> <span class="toc-text">测试存储和压缩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-10-%E7%AB%A0-%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%B0%83%E4%BC%98"><span class="toc-number">10.</span> <span class="toc-text">第 10 章 企业级调优</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%EF%BC%88Explain%EF%BC%89"><span class="toc-number">10.1.</span> <span class="toc-text">执行计划（Explain）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fetch-%E6%8A%93%E5%8F%96"><span class="toc-number">10.2.</span> <span class="toc-text">Fetch 抓取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-number">10.3.</span> <span class="toc-text">本地模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A8%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-number">10.4.</span> <span class="toc-text">表的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E8%A1%A8%E5%A4%A7%E8%A1%A8-Join%EF%BC%88MapJOIN%EF%BC%89"><span class="toc-number">10.4.1.</span> <span class="toc-text">小表大表 Join（MapJOIN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E8%A1%A8-Join-%E5%A4%A7%E8%A1%A8"><span class="toc-number">10.4.2.</span> <span class="toc-text">大表 Join 大表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Group-By"><span class="toc-number">10.4.3.</span> <span class="toc-text">Group By</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Count-Distinct-%E5%8E%BB%E9%87%8D%E7%BB%9F%E8%AE%A1"><span class="toc-number">10.4.4.</span> <span class="toc-text">Count(Distinct) 去重统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF-1"><span class="toc-number">10.4.5.</span> <span class="toc-text">笛卡尔积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E5%88%97%E8%BF%87%E6%BB%A4"><span class="toc-number">10.4.6.</span> <span class="toc-text">行列过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA"><span class="toc-number">10.4.7.</span> <span class="toc-text">分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E6%A1%B6"><span class="toc-number">10.4.8.</span> <span class="toc-text">分桶</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE-Map-%E5%8F%8A-Reduce-%E6%95%B0"><span class="toc-number">10.5.</span> <span class="toc-text">合理设置 Map 及 Reduce 数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E6%96%87%E4%BB%B6%E5%A2%9E%E5%8A%A0-Map-%E6%95%B0"><span class="toc-number">10.5.1.</span> <span class="toc-text">复杂文件增加 Map 数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6"><span class="toc-number">10.5.2.</span> <span class="toc-text">小文件进行合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE-Reduce-%E6%95%B0"><span class="toc-number">10.5.3.</span> <span class="toc-text">合理设置 Reduce 数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="toc-number">10.6.</span> <span class="toc-text">并行执行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A5%E6%A0%BC%E6%A8%A1%E5%BC%8F"><span class="toc-number">10.7.</span> <span class="toc-text">严格模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JVM-%E9%87%8D%E7%94%A8"><span class="toc-number">10.8.</span> <span class="toc-text">JVM 重用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9"><span class="toc-number">10.9.</span> <span class="toc-text">压缩</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-11-%E7%AB%A0-HiveQL%E7%B4%A2%E5%BC%95"><span class="toc-number">11.</span> <span class="toc-text">第 11 章 HiveQL索引</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95"><span class="toc-number">11.1.</span> <span class="toc-text">创建索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E5%BB%BA%E7%B4%A2%E5%BC%95"><span class="toc-number">11.2.</span> <span class="toc-text">重建索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA%E7%B4%A2%E5%BC%95"><span class="toc-number">11.3.</span> <span class="toc-text">显示索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E7%B4%A2%E5%BC%95"><span class="toc-number">11.4.</span> <span class="toc-text">删除索引</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-12-%E7%AB%A0-%E9%94%81"><span class="toc-number">12.</span> <span class="toc-text">第 12 章 锁</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E7%BB%93%E5%90%88Zookeeper%E6%94%AF%E6%8C%81%E9%94%81%E5%8A%9F%E8%83%BD"><span class="toc-number">12.1.</span> <span class="toc-text">Hive结合Zookeeper支持锁功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA%E9%94%81%E5%92%8C%E7%8B%AC%E5%8D%A0%E9%94%81"><span class="toc-number">12.2.</span> <span class="toc-text">显示锁和独占锁</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/SSM%E2%80%94%E2%80%94Mybatis/" title="SSM——Mybatis">SSM——Mybatis</a><time datetime="2023-06-28T07:35:16.000Z" title="发表于 2023-06-28 15:35:16">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/SSM%E2%80%94%E2%80%94Spring/" title="SSM——Spring">SSM——Spring</a><time datetime="2023-06-28T07:22:40.000Z" title="发表于 2023-06-28 15:22:40">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/" title="大数据——Hadoop.2.x">大数据——Hadoop.2.x</a><time datetime="2023-06-27T10:09:04.000Z" title="发表于 2023-06-27 18:09:04">2023-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/" title="大数据——Hive调优">大数据——Hive调优</a><time datetime="2023-06-27T10:01:35.000Z" title="发表于 2023-06-27 18:01:35">2023-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/" title="大数据——Hive">大数据——Hive</a><time datetime="2023-06-27T04:21:43.000Z" title="发表于 2023-06-27 12:21:43">2023-06-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By DingQuan Zuo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>