<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据——Hive调优 | ccbigs blog</title><meta name="author" content="DingQuan Zuo"><meta name="copyright" content="DingQuan Zuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第 1 章 Explain 查看执行计划（重点）创建测试用表 1）建大表、小表和JOIN后表的语句  123456789101112&#x2F;&#x2F; 创建大表create table bigtable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row forma">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据——Hive调优">
<meta property="og:url" content="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/index.html">
<meta property="og:site_name" content="ccbigs blog">
<meta property="og:description" content="第 1 章 Explain 查看执行计划（重点）创建测试用表 1）建大表、小表和JOIN后表的语句  123456789101112&#x2F;&#x2F; 创建大表create table bigtable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row forma">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/ahead.jpg">
<meta property="article:published_time" content="2023-06-27T10:01:35.000Z">
<meta property="article:modified_time" content="2023-06-27T10:06:52.124Z">
<meta property="article:author" content="DingQuan Zuo">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/ahead.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据——Hive调优',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-27 18:06:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/ahead.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/1558.png')"><nav id="nav"><span id="blog-info"><a href="/" title="ccbigs blog"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">ccbigs blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据——Hive调优</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-27T10:01:35.000Z" title="发表于 2023-06-27 18:01:35">2023-06-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-27T10:06:52.124Z" title="更新于 2023-06-27 18:06:52">2023-06-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据——Hive调优"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="第-1-章-Explain-查看执行计划（重点）"><a href="#第-1-章-Explain-查看执行计划（重点）" class="headerlink" title="第 1 章 Explain 查看执行计划（重点）"></a>第 1 章 Explain 查看执行计划（重点）</h1><h2 id="创建测试用表"><a href="#创建测试用表" class="headerlink" title="创建测试用表"></a>创建测试用表</h2><blockquote>
<p>1）建大表、小表和JOIN后表的语句</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建大表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string, </span><br><span class="line">url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建小表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string, </span><br><span class="line">url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 创建 <span class="keyword">JOIN</span> 后表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(id <span class="type">bigint</span>, t <span class="type">bigint</span>, uid string, keyword string, </span><br><span class="line">url_rank <span class="type">int</span>, click_num <span class="type">int</span>, click_url string) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）分别向大表和小表中导入数据</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/smalltable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure>



<h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query<span class="operator">-</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure>



<h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><blockquote>
<p>1）查看下面这条语句的执行计划</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage #根阶段，也就是说从stage0开始执行</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator         #抓取操作</span><br><span class="line">      limit: <span class="number">-1</span>            #表示：并没有做limit限制，能读多少数据就读多少数据</span><br><span class="line">      Processor Tree:      #操作树</span><br><span class="line">        TableScan</span><br><span class="line">          alias: bigtable  #操作的表</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          <span class="keyword">Select</span> Operator  #查询操作</span><br><span class="line">            expressions: id (type: <span class="type">bigint</span>), t (type: <span class="type">bigint</span>), uid (type: string), keyword (type: string), url_rank (type: <span class="type">int</span>), click_num (type: <span class="type">int</span>), click_url (type: string)</span><br><span class="line">            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            ListSink</span><br><span class="line"></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">5.564</span> seconds, Fetched: <span class="number">17</span> <span class="type">row</span>(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#######################################################################################################</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> click_url,<span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage #根阶段</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>  #stage0 表示是依赖于我们的根阶段</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span>  #当前表示执行顺序，stage1是开始执行的阶段</span><br><span class="line">    Map Reduce    #MapReducer的任务</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: bigtable  #表名</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Select</span> Operator  #查询操作</span><br><span class="line">              expressions: click_url (type: string)  </span><br><span class="line">              outputColumnNames: click_url</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Group</span> <span class="keyword">By</span> Operator  #<span class="keyword">group</span> <span class="keyword">by</span> 操作</span><br><span class="line">                aggregations: <span class="built_in">count</span>()</span><br><span class="line">                keys: click_url (type: string)</span><br><span class="line">                mode: hash  #采用的模式</span><br><span class="line">                outputColumnNames: _col0, _col1 #输出字段有两个字段</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                Reduce Output Operator</span><br><span class="line">                  key expressions: _col0 (type: string)</span><br><span class="line">                  sort <span class="keyword">order</span>: <span class="operator">+</span></span><br><span class="line">                  Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: string) #分区字段</span><br><span class="line">                  Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  <span class="keyword">value</span> expressions: _col1 (type: <span class="type">bigint</span>)</span><br><span class="line">      Execution mode: vectorized</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">          aggregations: <span class="built_in">count</span>(VALUE._col0)  #统计count值</span><br><span class="line">          keys: KEY._col0 (type: string)</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">table</span>:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br><span class="line"></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">2.034</span> seconds, Fetched: <span class="number">49</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）查看详细执行计划 </p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> click_url,<span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: bigtable</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            GatherStats: <span class="literal">false</span></span><br><span class="line">            <span class="keyword">Select</span> Operator</span><br><span class="line">              expressions: click_url (type: string)</span><br><span class="line">              outputColumnNames: click_url</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">                aggregations: <span class="built_in">count</span>()</span><br><span class="line">                keys: click_url (type: string)</span><br><span class="line">                mode: hash</span><br><span class="line">                outputColumnNames: _col0, _col1</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                Reduce Output Operator</span><br><span class="line">                  key expressions: _col0 (type: string)</span><br><span class="line">                  <span class="keyword">null</span> sort <span class="keyword">order</span>: a</span><br><span class="line">                  sort <span class="keyword">order</span>: <span class="operator">+</span></span><br><span class="line">                  Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: string)</span><br><span class="line">                  Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  tag: <span class="number">-1</span></span><br><span class="line">                  <span class="keyword">value</span> expressions: _col1 (type: <span class="type">bigint</span>)</span><br><span class="line">                  auto parallelism: <span class="literal">false</span></span><br><span class="line">      Execution mode: vectorized</span><br><span class="line">      Path <span class="operator">-</span><span class="operator">&gt;</span> Alias:</span><br><span class="line">        hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>bigtable [bigtable]</span><br><span class="line">      Path <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Partition</span>:</span><br><span class="line">        hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>bigtable </span><br><span class="line">          <span class="keyword">Partition</span></span><br><span class="line">            base file name: bigtable</span><br><span class="line">            input format: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">            properties:</span><br><span class="line">              COLUMN_STATS_ACCURATE &#123;&quot;BASIC_STATS&quot;:&quot;true&quot;,&quot;COLUMN_STATS&quot;:&#123;&quot;click_num&quot;:&quot;true&quot;,&quot;click_url&quot;:&quot;true&quot;,&quot;id&quot;:&quot;true&quot;,&quot;keyword&quot;:&quot;true&quot;,&quot;t&quot;:&quot;true&quot;,&quot;uid&quot;:&quot;true&quot;,&quot;url_rank&quot;:&quot;true&quot;&#125;&#125;</span><br><span class="line">              bucket_count <span class="number">-1</span></span><br><span class="line">              bucketing_version <span class="number">2</span></span><br><span class="line">              column.name.delimiter ,</span><br><span class="line">              columns id,t,uid,keyword,url_rank,click_num,click_url</span><br><span class="line">              columns.comments </span><br><span class="line">              columns.types <span class="type">bigint</span>:<span class="type">bigint</span>:string:string:<span class="type">int</span>:<span class="type">int</span>:string</span><br><span class="line">              field.delim  </span><br><span class="line">              file.inputformat org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">              location hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>bigtable</span><br><span class="line">              name default.bigtable</span><br><span class="line">              numFiles <span class="number">0</span></span><br><span class="line">              numRows <span class="number">0</span></span><br><span class="line">              rawDataSize <span class="number">0</span></span><br><span class="line">              serialization.ddl struct bigtable &#123; i64 id, i64 t, string uid, string keyword, i32 url_rank, i32 click_num, string click_url&#125;</span><br><span class="line">              serialization.format  </span><br><span class="line">              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">              totalSize <span class="number">0</span></span><br><span class="line">              transient_lastDdlTime <span class="number">1641896417</span></span><br><span class="line">            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">          </span><br><span class="line">              input format: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">              properties:</span><br><span class="line">                COLUMN_STATS_ACCURATE &#123;&quot;BASIC_STATS&quot;:&quot;true&quot;,&quot;COLUMN_STATS&quot;:&#123;&quot;click_num&quot;:&quot;true&quot;,&quot;click_url&quot;:&quot;true&quot;,&quot;id&quot;:&quot;true&quot;,&quot;keyword&quot;:&quot;true&quot;,&quot;t&quot;:&quot;true&quot;,&quot;uid&quot;:&quot;true&quot;,&quot;url_rank&quot;:&quot;true&quot;&#125;&#125;</span><br><span class="line">                bucket_count <span class="number">-1</span></span><br><span class="line">                bucketing_version <span class="number">2</span></span><br><span class="line">                column.name.delimiter ,</span><br><span class="line">                columns id,t,uid,keyword,url_rank,click_num,click_url</span><br><span class="line">                columns.comments </span><br><span class="line">                columns.types <span class="type">bigint</span>:<span class="type">bigint</span>:string:string:<span class="type">int</span>:<span class="type">int</span>:string</span><br><span class="line">                field.delim  </span><br><span class="line">                file.inputformat org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">                location hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>bigtable</span><br><span class="line">                name default.bigtable</span><br><span class="line">                numFiles <span class="number">0</span></span><br><span class="line">                numRows <span class="number">0</span></span><br><span class="line">                rawDataSize <span class="number">0</span></span><br><span class="line">                serialization.ddl struct bigtable &#123; i64 id, i64 t, string uid, string keyword, i32 url_rank, i32 click_num, string click_url&#125;</span><br><span class="line">                serialization.format  </span><br><span class="line">                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">                totalSize <span class="number">0</span></span><br><span class="line">                transient_lastDdlTime <span class="number">1641896417</span></span><br><span class="line">              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">              name: default.bigtable</span><br><span class="line">            name: default.bigtable</span><br><span class="line">      Truncated Path <span class="operator">-</span><span class="operator">&gt;</span> Alias:</span><br><span class="line">        <span class="operator">/</span>bigtable [bigtable]</span><br><span class="line">      Needs Tagging: <span class="literal">false</span></span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">          aggregations: <span class="built_in">count</span>(VALUE._col0)</span><br><span class="line">          keys: KEY._col0 (type: string)</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            GlobalTableId: <span class="number">0</span></span><br><span class="line">            directory: hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span>tmp<span class="operator">/</span>hive<span class="operator">/</span>atguigu<span class="operator">/</span><span class="number">62982</span>c5f<span class="operator">-</span>af45<span class="number">-484</span>f<span class="number">-85</span>c7<span class="number">-2816214</span>ca2e9<span class="operator">/</span>hive_2022<span class="number">-01</span><span class="number">-12</span>_11<span class="number">-11</span><span class="number">-59</span>_537_3273976675533502080<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>mr<span class="number">-10001</span><span class="operator">/</span>.hive<span class="operator">-</span>staging_hive_2022<span class="number">-01</span><span class="number">-12</span>_11<span class="number">-11</span><span class="number">-59</span>_537_3273976675533502080<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>ext<span class="number">-10002</span></span><br><span class="line">            NumFilesPerFileSink: <span class="number">1</span></span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">0</span> Basic stats: PARTIAL <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            Stats Publishing Key Prefix: hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop100:<span class="number">8020</span><span class="operator">/</span>tmp<span class="operator">/</span>hive<span class="operator">/</span>atguigu<span class="operator">/</span><span class="number">62982</span>c5f<span class="operator">-</span>af45<span class="number">-484</span>f<span class="number">-85</span>c7<span class="number">-2816214</span>ca2e9<span class="operator">/</span>hive_2022<span class="number">-01</span><span class="number">-12</span>_11<span class="number">-11</span><span class="number">-59</span>_537_3273976675533502080<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>mr<span class="number">-10001</span><span class="operator">/</span>.hive<span class="operator">-</span>staging_hive_2022<span class="number">-01</span><span class="number">-12</span>_11<span class="number">-11</span><span class="number">-59</span>_537_3273976675533502080<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>ext<span class="number">-10002</span><span class="operator">/</span></span><br><span class="line">            <span class="keyword">table</span>:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                properties:</span><br><span class="line">                  columns _col0,_col1</span><br><span class="line">                  columns.types string:<span class="type">bigint</span></span><br><span class="line">                  escape.delim \</span><br><span class="line">                  hive.serialization.extend.additional.nesting.levels <span class="literal">true</span></span><br><span class="line">                  serialization.escape.crlf <span class="literal">true</span></span><br><span class="line">                  serialization.format <span class="number">1</span></span><br><span class="line">                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">            TotalFiles: <span class="number">1</span></span><br><span class="line">            GatherStats: <span class="literal">false</span></span><br><span class="line">            MultiFileSpray: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br><span class="line"></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.498</span> seconds, Fetched: <span class="number">128</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>





<h1 id="第-2-章-Hive-建表优化"><a href="#第-2-章-Hive-建表优化" class="headerlink" title="第 2 章 Hive 建表优化"></a>第 2 章 Hive 建表优化</h1><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所 有的数据文件。 </p>
<p><strong>Hive 中的分区就是分目录</strong>，把一个大的数据集根据业务需要分割成小的数据集。 </p>
<p>在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率 会提高很多，所以我们需要把常常用在 WHERE 语句中的字段指定为表的分区字段。</p>
<h1 id="第-3-章-HQL-语法优化"><a href="#第-3-章-HQL-语法优化" class="headerlink" title="第 3 章 HQL 语法优化"></a>第 3 章 HQL 语法优化</h1><p>3.1-3.5 单表操作的优化   3.6 -3.10 多表操作优化</p>
<h2 id="列裁剪与分区裁剪"><a href="#列裁剪与分区裁剪" class="headerlink" title="列裁剪与分区裁剪"></a>列裁剪与分区裁剪</h2><p>列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者 数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低。 </p>
<p>Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他的列。这样做 可以节省读取开销：中间表存储开销和数据整合开销。</p>
<h2 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h2><p>默认情况下，Map 阶段同一 Key 数据分发给一个 Reduce，当一个 key 数据过大时就倾 斜了。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220112114926254.png" alt="image-20220112114926254"></p>
<p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行 部分聚合，最后在 Reduce 端得出最终结果。</p>
<p><strong>开启 Map 端聚合参数设置</strong></p>
<p>（1）是否在 Map 端进行聚合，默认为 True </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure>

<p>（2）在 Map 端进行聚合操作的条目数目 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>; </span><br></pre></td></tr></table></figure>

<p>（3）有数据倾斜的时候进行负载均衡（默认是 false） </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure>

<p>当选项设定为 true，生成的查询计划会有两个 MR Job。</p>
<p>第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合 操作，并输出结果，这样处理的结果<strong>是相同的 Group By Key 有可能被分发到不同的 Reduce 中</strong>，从而达到负载均衡的目的；</p>
<p>第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作（虽然 能解决数据倾斜，但是不能让运行速度的更快）。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">23.68</span> sec HDFS Read: </span><br><span class="line"><span class="number">19987</span> HDFS Write: <span class="number">9</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">23</span> seconds <span class="number">680</span> msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>

<p>优化以后</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">28.53</span> sec HDFS Read: </span><br><span class="line"><span class="number">18209</span> HDFS Write: <span class="number">534</span> SUCCESS</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span> Reduce: <span class="number">5</span> Cumulative CPU: <span class="number">38.32</span> sec HDFS Read: </span><br><span class="line"><span class="number">15014</span> HDFS Write: <span class="number">9</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">1</span> minutes <span class="number">6</span> seconds <span class="number">850</span> msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>

<h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>vectorization : 矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization 技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。</p>
<p><img src="https://fastly.jsdelivr.net/gh/dexShop/ps/image-20220112121200561.png" alt="image-20220112121200561"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.reduce.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>



<h2 id="多重模式"><a href="#多重模式" class="headerlink" title="多重模式"></a>多重模式</h2><p>如果你碰到一堆 SQL，并且这一堆 SQL 的模式还一样。都是从同一个表进行扫描，做不 同的逻辑。有可优化的地方：如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">17</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">18</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">19</span>;</span><br></pre></td></tr></table></figure>

<p>– 隐藏了一个问题：这种类型的 SQL 有多少个，那么最终。这张表就被全表扫描了多少次</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student </span><br><span class="line"><span class="keyword">where</span> city<span class="operator">=</span> A;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student </span><br><span class="line"><span class="keyword">where</span> city<span class="operator">=</span> B;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>c). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student </span><br><span class="line"><span class="keyword">where</span> city<span class="operator">=</span> c;</span><br><span class="line">修改为：</span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> A</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> B</span><br></pre></td></tr></table></figure>

<p>如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高， 多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后， 要多次利用。</p>
<h2 id="in-x2F-exists-语句"><a href="#in-x2F-exists-语句" class="headerlink" title="in&#x2F;exists 语句"></a>in&#x2F;exists 语句</h2><p>在 Hive 的早期版本中，in&#x2F;exists 语法是不被支持的，但是从 hive-0.8x 以后就开始支持 这个语法。但是不推荐使用这个语法。虽然经过测验，Hive-2.3.6 也支持 in&#x2F;exists 操作，但 还是推荐使用 Hive 的一个高效替代方案：left semi join。</p>
<p>比如说：– in &#x2F; exists 实现</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b <span class="keyword">where</span> a.id <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure>

<p>可以使用 join 来改写：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>

<p>应该转换成： </p>
<p>– left semi join 实现</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>





<h2 id="CBO-优化"><a href="#CBO-优化" class="headerlink" title="CBO 优化"></a>CBO 优化</h2><p>join 的时候表的顺序的关系：前面的表都会被加载到内存中。后面的表进行磁盘扫描。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span>, b.<span class="operator">*</span>, c.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id <span class="keyword">join</span> c <span class="keyword">on</span> a.id <span class="operator">=</span> c.id;</span><br></pre></td></tr></table></figure>

<p>Hive 自 0.14.0 开始，加入了一项 “Cost based Optimizer” 来对 HQL 执行计划进行优化， 这个功能通过 “hive.cbo.enable” 来开启。在 Hive 1.1.0 之后，这个 feature 是默认开启的， 它可以 自动优化 HQL 中多个 Join 的顺序，并选择合适的 Join 算法。</p>
<p>CBO，成本优化器，代价最小的执行计划就是最好的执行计划。传统的数据库，成本优 化器做出最优化的执行计划是依据统计信息来计算的。 </p>
<p>Hive 的成本优化器也一样，Hive 在提供最终执行前，优化每个查询的执行逻辑和物理 执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生 潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。</p>
<p>要使用基于成本的优化（也称为 CBO），请在查询开始设置以下参数：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cbo.enable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.compute.query.using.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.column.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.partition.stats<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>



<h2 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h2><p>将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻 辑优化器是 PredicatePushDown，配置项为 hive.optimize.ppd，默认为 true。 </p>
<p>案例实操：</p>
<p>1）打开谓词下推优化属性 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.optimize.ppd <span class="operator">=</span> <span class="literal">true</span>; #谓词下推，默认是 <span class="literal">true</span> ，所以优不优化都是一样的</span><br></pre></td></tr></table></figure>

<p>2）查看先关联两张表，再用 where 条件过滤的执行计划 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> bigtable o <span class="keyword">on</span> o.id <span class="operator">=</span> b.id <span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>; </span><br></pre></td></tr></table></figure>

<p>3）查看子查询后，再关联表的执行计划</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o <span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure>





<h2 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h2><p>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，这样就不用进行 Reduce 步骤，从而提高了速度。如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在 Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map 端进行 Join，避免 Reducer 处理。</p>
<blockquote>
<p>1）开启 MapJoin 参数设置 </p>
</blockquote>
<p>（1）设置自动选择 MapJoin </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join=true; #默认为 true </span><br></pre></td></tr></table></figure>

<p>（2）大表小表的阈值设置（默认 25M 以下认为是小表）： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.mapjoin.smalltable.filesize=25000000;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）MapJoin 工作机制 </p>
</blockquote>
<p>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进 程中进行 Join 操作，这样就不用进行 Reduce 步骤，从而提高了速度。</p>
<blockquote>
<p>3）案例实操： </p>
</blockquote>
<p>（1）开启 MapJoin 功能 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; #默认为 <span class="literal">true</span>，测试的时候改成<span class="literal">false</span>可以查看效果 </span><br></pre></td></tr></table></figure>

<p>（2）执行小表 JOIN 大表语句 </p>
<p>注意：此时小表(左连接)作为主表，所有数据都要写出去，因此此时会走 reduce，mapjoin 失效</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable b</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>

<p>Time taken: 24.594 seconds</p>
<p>（3）执行大表 JOIN 小表语句 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url <span class="keyword">from</span> bigtable b <span class="keyword">left</span> <span class="keyword">join</span> smalltable s <span class="keyword">on</span> s.id <span class="operator">=</span> b.id; </span><br></pre></td></tr></table></figure>

<p>Time taken: 24.315 seconds</p>
<h2 id="大表、大表-SMB-Join（重点）"><a href="#大表、大表-SMB-Join（重点）" class="headerlink" title="大表、大表 SMB Join（重点）"></a>大表、大表 SMB Join（重点）</h2><p>SMB Join ：Sort Merge Bucket Join</p>
<blockquote>
<p>1）创建第二张大表</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable2(</span><br><span class="line">id <span class="type">bigint</span>,</span><br><span class="line"> t <span class="type">bigint</span>,</span><br><span class="line"> uid string,</span><br><span class="line"> keyword string,</span><br><span class="line"> url_rank <span class="type">int</span>,</span><br><span class="line"> click_num <span class="type">int</span>,</span><br><span class="line"> click_url string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/log.data&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable2;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）测试大表直接 JOIN</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable a</span><br><span class="line"><span class="keyword">join</span> bigtable2 b</span><br><span class="line"><span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>

<p>测试结果：<strong>Time taken: 157.815 seconds</strong></p>
<p>分桶JOIN将大表分成很多的小表，让后将小表的数据进行JOIN。因为是hash分区，所以相同的id能够被划分在一起。</p>
<blockquote>
<p>3）创建分通表 1</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck1(</span><br><span class="line"> id <span class="type">bigint</span>,</span><br><span class="line"> t <span class="type">bigint</span>,</span><br><span class="line"> uid string,</span><br><span class="line"> keyword string,</span><br><span class="line"> url_rank <span class="type">int</span>,</span><br><span class="line"> click_num <span class="type">int</span>,</span><br><span class="line"> click_url string)</span><br><span class="line">clustered <span class="keyword">by</span>(id)</span><br><span class="line">sorted <span class="keyword">by</span>(id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">6</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">bigtable_buck1;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>4）创建分通表 2，分桶数和第一张表的分桶数为倍数关系</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck2(</span><br><span class="line"> id <span class="type">bigint</span>,</span><br><span class="line"> t <span class="type">bigint</span>,</span><br><span class="line"> uid string,</span><br><span class="line"> keyword string,</span><br><span class="line"> url_rank <span class="type">int</span>,</span><br><span class="line"> click_num <span class="type">int</span>,</span><br><span class="line"> click_url string)</span><br><span class="line">clustered <span class="keyword">by</span>(id)</span><br><span class="line">sorted <span class="keyword">by</span>(id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">6</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">bigtable_buck2;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>5）设置参数</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>6）测试 Time taken: 34.685 seconds</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url <span class="keyword">from</span> bigtable_buck1 s <span class="keyword">join</span> bigtable_buck2 b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br></pre></td></tr></table></figure>

<p><strong>这里需要注意的是，分桶的数量&lt;&#x3D;CPU的核数</strong></p>
<h2 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h2><p>Join 的时候不加 on 条件，或者无效的 on 条件，因为找不到 Join key，Hive 只能使用 1 个 Reducer 来完成笛卡尔积。当 Hive 设定为严格模式（hive.mapred.mode&#x3D;strict，nonstrict） 时，不允许在 HQL 语句中出现笛卡尔积。</p>
<h1 id="第-4-章-数据倾斜（重点）"><a href="#第-4-章-数据倾斜（重点）" class="headerlink" title="第 4 章 数据倾斜（重点）"></a>第 4 章 数据倾斜（重点）</h1><p><strong>绝大部分任务</strong>都很快完成，<strong>只有一个或者少数几个任务执行的很慢甚至最终执行失败</strong>， 这样的现象为数据倾斜现象。 </p>
<p>一定要和数据过量导致的现象区分开，数据过量的表现为所有任务都执行的很慢，这个 时候只有提高执行资源才可以优化 HQL 的执行效率。 </p>
<p>综合来看，导致数据倾斜的原因在于按照 Key 分组以后，<strong>少量的任务负责绝大部分数据 的计算</strong>，也就是说产生数据倾斜的 HQL 中一定存在分组操作，那么从 HQL 的角度，我们可 以将数据倾斜分为<strong>单表携带了 GroupBy 字段的查询和两表（或者多表）Join 的查询</strong>。</p>
<h2 id="单表数据倾斜优化"><a href="#单表数据倾斜优化" class="headerlink" title="单表数据倾斜优化"></a>单表数据倾斜优化</h2><h3 id="使用参数"><a href="#使用参数" class="headerlink" title="使用参数"></a>使用参数</h3><p>当任务中存在 GroupBy 操作同时聚合函数为 count 或者 sum 可以设置参数来处理数据 倾斜问题。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">是否在 Map 端进行聚合，默认为 <span class="literal">True</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">在 Map 端进行聚合操作的条目数目</span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>;</span><br></pre></td></tr></table></figure>

<p>有数据倾斜的时候进行负载均衡（默认是 false）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p><strong>当选项设定为 true，生成的查询计划会有两个 MR Job。</strong></p>
<h3 id="增加-Reduce-数量（多个-Key-同时导致数据倾斜）"><a href="#增加-Reduce-数量（多个-Key-同时导致数据倾斜）" class="headerlink" title="增加 Reduce 数量（多个 Key 同时导致数据倾斜）"></a>增加 Reduce 数量（多个 Key 同时导致数据倾斜）</h3><blockquote>
<p>1）调整 reduce 个数方法一</p>
</blockquote>
<p>（1）每个 Reduce 处理的数据量默认是 256MB </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.reducers.bytes.per.reducer = 256000000 </span><br></pre></td></tr></table></figure>

<p>（2）每个任务最大的 reduce 数，默认为 1009 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.reducers.max = 1009 </span><br></pre></td></tr></table></figure>

<p>（3）计算 reducer 数的公式 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）调整 reduce 个数方法二</p>
</blockquote>
<p>在 hadoop 的 mapred-default.xml 文件中修改 </p>
<p>设置每个 job 的 Reduce 个数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Join-数据倾斜优化"><a href="#Join-数据倾斜优化" class="headerlink" title="Join 数据倾斜优化"></a>Join 数据倾斜优化</h2><h3 id="使用参数-1"><a href="#使用参数-1" class="headerlink" title="使用参数"></a>使用参数</h3><p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">join</span> 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"># 如果是 <span class="keyword">join</span> 过程出现倾斜应该设置为 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">false</span>;</span><br></pre></td></tr></table></figure>

<p>如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的 倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过 hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认 10000。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.skewjoin.mapjoin.map.tasks<span class="operator">=</span><span class="number">10000</span>;</span><br></pre></td></tr></table></figure>

<h3 id="MapJoin-1"><a href="#MapJoin-1" class="headerlink" title="MapJoin"></a>MapJoin</h3><p>详情见 3.9 节</p>
<h1 id="第-5-章-Hive-Job-优化"><a href="#第-5-章-Hive-Job-优化" class="headerlink" title="第 5 章 Hive Job 优化"></a>第 5 章 Hive Job 优化</h1><h2 id="Hive-Map-优化"><a href="#Hive-Map-优化" class="headerlink" title="Hive Map 优化"></a>Hive Map 优化</h2><h3 id="复杂文件增加-Map-数"><a href="#复杂文件增加-Map-数" class="headerlink" title="复杂文件增加 Map 数"></a>复杂文件增加 Map 数</h3><p>当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。</p>
<p>增加 map 的方法为：根据</p>
<p><strong>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M</strong> 公式， 调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p>
<p>案例实操：</p>
<blockquote>
<p>1）执行查询 </p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp; </span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span>  reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）设置最大切片值为 100 个字节</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">6</span>; number <span class="keyword">of</span> </span><br><span class="line">reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="小文件进行合并"><a href="#小文件进行合并" class="headerlink" title="小文件进行合并"></a>小文件进行合并</h3><blockquote>
<p>1）在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合 并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span> org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>2）在 Map-Reduce 的任务结束时合并小文件的设置： </p>
</blockquote>
<p>在 map-only 任务结束时合并小文件，默认 true</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapfiles = true; </span><br></pre></td></tr></table></figure>

<p>在 map-reduce 任务结束时合并小文件，默认 false </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapredfiles = true; </span><br></pre></td></tr></table></figure>

<p>合并文件的大小，默认 256M </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.size.per.task = 268435456; </span><br></pre></td></tr></table></figure>

<p>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.smallfiles.avgsize = 16777216;</span><br></pre></td></tr></table></figure>

<h3 id="Map-端聚合"><a href="#Map-端聚合" class="headerlink" title="Map 端聚合"></a>Map 端聚合</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;相当于 map 端执行 combiner</span><br></pre></td></tr></table></figure>

<h3 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.map.tasks.speculative.execution = true #默认是 true</span><br></pre></td></tr></table></figure>



<h2 id="Hive-Reduce-优化"><a href="#Hive-Reduce-优化" class="headerlink" title="Hive Reduce 优化"></a>Hive Reduce 优化</h2><h3 id="合理设置-Reduce-数"><a href="#合理设置-Reduce-数" class="headerlink" title="合理设置 Reduce 数"></a>合理设置 Reduce 数</h3><blockquote>
<p>1）调整 reduce 个数方法一</p>
</blockquote>
<p>（1）每个 Reduce 处理的数据量默认是 256MB </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.reducers.bytes.per.reducer = 256000000 </span><br></pre></td></tr></table></figure>

<p>（2）每个任务最大的 reduce 数，默认为 1009 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.reducers.max = 1009 </span><br></pre></td></tr></table></figure>

<p>（3）计算 reducer 数的公式 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)</span><br></pre></td></tr></table></figure>



<blockquote>
<p>2）调整 reduce 个数方法二 </p>
</blockquote>
<p>在 hadoop 的 mapred-default.xml 文件中修改 </p>
<p>设置每个 job 的 Reduce 个数 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>; </span><br></pre></td></tr></table></figure>

<blockquote>
<p>3）reduce 个数并不是越多越好 </p>
</blockquote>
<p>（1）过多的启动和初始化 reduce 也会消耗时间和资源；</p>
<p>（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那 么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题； </p>
<p>在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数； 使单个 reduce 任务处理数据量大小要合适；</p>
<h3 id="推测执行-1"><a href="#推测执行-1" class="headerlink" title="推测执行"></a>推测执行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mapred.reduce.tasks.speculative.execution （hadoop 里面的） </span><br><span class="line">hive.mapred.reduce.tasks.speculative.execution（hive 里面相同的参数，效果和 hadoop 里面的一样两个随便哪个都行）</span><br></pre></td></tr></table></figure>



<h1 id="第-6-章-Hive-On-Spark"><a href="#第-6-章-Hive-On-Spark" class="headerlink" title="第 6 章 Hive On Spark"></a>第 6 章 Hive On Spark</h1><h2 id="Executor-参数"><a href="#Executor-参数" class="headerlink" title="Executor 参数"></a>Executor 参数</h2><p>以单台服务器 128G 内存，32 线程为例。</p>
<h3 id="spark-executor-cores"><a href="#spark-executor-cores" class="headerlink" title="spark.executor.cores"></a>spark.executor.cores</h3><p>该参数表示每个 Executor 可利用的 CPU 核心数。其值不宜设定过大，因为 Hive 的底层 以 HDFS 存储，而 HDFS 有时对高并发写入处理不太好，容易造成 race condition。根据经验 实践，设定在 3~6 之间比较合理。</p>
<p>假设我们使用的服务器单节点有 32 个 CPU 核心可供使用。考虑到系统基础服务和 HDFS 等组件的余量，一般会将 YARN NodeManager 的 yarn.nodemanager.resource.cpu-vcores 参数 设为 28，也就是 YARN 能够利用其中的 28 核，此时将 spark.executor.cores 设为 4 最合适， 最多可以正好分配给 7 个 Executor 而不造成浪费。又假设 yarn.nodemanager.resource.cpuvcores 为 26，那么将 spark.executor.cores 设为 5 最合适，只会剩余 1 个核。</p>
<p>由于一个 Executor 需要一个 YARN Container 来运行，所以还需保证 spark.executor.cores 的值不能大于单个 Container 能申请到的最大核心数，即 yarn.scheduler.maximum-allocationvcores 的值。</p>
<h3 id="spark-executor-memory-x2F-spark-yarn-executor-memoryOverhead"><a href="#spark-executor-memory-x2F-spark-yarn-executor-memoryOverhead" class="headerlink" title="spark.executor.memory&#x2F;spark.yarn.executor.memoryOverhead"></a>spark.executor.memory&#x2F;spark.yarn.executor.memoryOverhead</h3><p>这两个参数分别表示每个 Executor 可利用的堆内内存量和堆外内存量。堆内内存越大，Executor 就能缓存更多的数据，在做诸如 map join 之类的操作时就会更快，但同时也会使得 GC 变得更麻烦。spark.yarn.executor.memoryOverhead 的默认值是 executorMemory * 0.10， 最小值为 384M(每个 Executor)。</p>
<p>Hive 官方提供了一个计算 Executor 总内存量的经验公式，如下： yarn.nodemanager.resource.memory-mb*(spark.executor.cores&#x2F; yarn.nodemanager.resource.cpu-vcores)。</p>
<p>其实就是按核心数的比例分配。在计算出来的总内存量中，80%~85%划分给堆内内存， 剩余的划分给堆外内存。</p>
<p>假设集群中单节点有 128G 物理内存，yarn.nodemanager.resource.memory-mb（即单个 NodeManager 能够利用的主机内存量）设为 100G，那么每个 Executor 大概就是 100*(4&#x2F;28)&#x3D; 约 14G。</p>
<p>再 按 8:2 比 例 划 分 的 话 ， 最 终 spark.executor.memory 设 为 约 11.2G ， spark.yarn.executor.memoryOverhead 设为约 2.8G。</p>
<p>通过这些配置，每个主机一次可以运行多达 7 个 executor。每个 executor 最多可以运行 4 个 task(每个核一个)。因此，每个 task 平均有 3.5 GB(14 &#x2F; 4)内存。在 executor 中运行的所 有 task 共享相同的堆空间。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.executor.memory<span class="operator">=</span><span class="number">11.2</span>g;</span><br><span class="line"><span class="keyword">set</span> spark.yarn.executor.memoryOverhead<span class="operator">=</span><span class="number">2.8</span>g;</span><br></pre></td></tr></table></figure>

<p>同理，这两个内存参数相加的总量也不能超过单个 Container 最多能申请到的内存量， 即 yarn.scheduler.maximum-allocation-mb 配置的值。</p>
<h3 id="spark-executor-instances"><a href="#spark-executor-instances" class="headerlink" title="spark.executor.instances"></a>spark.executor.instances</h3><p>该参数表示执行查询时一共启动多少个 Executor 实例，这取决于每个节点的资源分配 情况以及集群的节点数。若我们一共有 10 台 32C&#x2F;128G 的节点，并按照上述配置（即每个节 点承载 7 个 Executor），那么理论上讲我们可以将 spark.executor.instances 设为 70，以使集群 资源最大化利用。但是实际上一般都会适当设小一些（推荐是理论值的一半左右，比如 40）， 因为 Driver 也要占用资源，并且一个 YARN 集群往往还要承载除了 Hive on Spark 之外的其他 业务。</p>
<h3 id="spark-dynamicAllocation-enabled"><a href="#spark-dynamicAllocation-enabled" class="headerlink" title="spark.dynamicAllocation.enabled"></a>spark.dynamicAllocation.enabled</h3><p>上面所说的固定分配 Executor 数量的方式可能不太灵活，尤其是在 Hive 集群面向很多用户提供分析服务的情况下。所以更推荐将 spark.dynamicAllocation.enabled 参数设为 true， 以启用 Executor 动态分配。</p>
<h3 id="参数配置样例参考"><a href="#参数配置样例参考" class="headerlink" title="参数配置样例参考"></a>参数配置样例参考</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.execution.engine<span class="operator">=</span>spark;</span><br><span class="line"><span class="keyword">set</span> spark.executor.memory<span class="operator">=</span><span class="number">11.2</span>g;</span><br><span class="line"><span class="keyword">set</span> spark.yarn.executor.memoryOverhead<span class="operator">=</span><span class="number">2.8</span>g;</span><br><span class="line"><span class="keyword">set</span> spark.executor.cores<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="keyword">set</span> spark.executor.instances<span class="operator">=</span><span class="number">40</span>;</span><br><span class="line"><span class="keyword">set</span> spark.dynamicAllocation.enabled<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> spark.serializer<span class="operator">=</span>org.apache.spark.serializer.KryoSerializer;</span><br></pre></td></tr></table></figure>

<h2 id="Driver-参数"><a href="#Driver-参数" class="headerlink" title="Driver 参数"></a>Driver 参数</h2><h3 id="spark-driver-cores"><a href="#spark-driver-cores" class="headerlink" title="spark.driver.cores"></a>spark.driver.cores</h3><p>该参数表示每个 Driver 可利用的 CPU 核心数。绝大多数情况下设为 1 都够用。 </p>
<h3 id="spark-driver-memory-x2F-spark-driver-memoryOverhead"><a href="#spark-driver-memory-x2F-spark-driver-memoryOverhead" class="headerlink" title="spark.driver.memory&#x2F;spark.driver.memoryOverhead"></a>spark.driver.memory&#x2F;spark.driver.memoryOverhead</h3><p>这两个参数分别表示每个 Driver 可利用的堆内内存量和堆外内存量。根据资源富余程 度和作业的大小，一般是将总量控制在 512MB~4GB 之间，并且沿用 Executor 内存的“二八分 配方式”。例如，spark.driver.memory 可以设为约 819MB，spark.driver.memoryOverhead 设为 约 205MB，加起来正好 1G</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">DingQuan Zuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/">http://example.com/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive%E8%B0%83%E4%BC%98/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">ccbigs blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="/img/ahead.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/" title="大数据——Hadoop.2.x"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据——Hadoop.2.x</div></div></a></div><div class="next-post pull-right"><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/" title="大数据——Hive"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据——Hive</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Flume/" title="大数据——Flume"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Flume</div></div></a></div><div><a href="/2023/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hbase/" title="大数据——Hbase"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">大数据——Hbase</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94ClickHouse/" title="大数据——ClickHouse"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——ClickHouse</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hadoop2/" title="大数据——Hadoop.2.x"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Hadoop.2.x</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Hive/" title="大数据——Hive"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Hive</div></div></a></div><div><a href="/2023/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94Scala/" title="大数据——Scala"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="title">大数据——Scala</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/ahead.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">DingQuan Zuo</div><div class="author-info__description">计算机很适合我这样的蠢人学，因为代码就在那里</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccbigs" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/zuodingquan666" target="_blank" title="CSDN"><i class="fas fa-c" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1692062014@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-1-%E7%AB%A0-Explain-%E6%9F%A5%E7%9C%8B%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-text">第 1 章 Explain 查看执行计划（重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%B5%8B%E8%AF%95%E7%94%A8%E8%A1%A8"><span class="toc-text">创建测试用表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-2-%E7%AB%A0-Hive-%E5%BB%BA%E8%A1%A8%E4%BC%98%E5%8C%96"><span class="toc-text">第 2 章 Hive 建表优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="toc-text">分区表</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-3-%E7%AB%A0-HQL-%E8%AF%AD%E6%B3%95%E4%BC%98%E5%8C%96"><span class="toc-text">第 3 章 HQL 语法优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%97%E8%A3%81%E5%89%AA%E4%B8%8E%E5%88%86%E5%8C%BA%E8%A3%81%E5%89%AA"><span class="toc-text">列裁剪与分区裁剪</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Group-By"><span class="toc-text">Group By</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorization"><span class="toc-text">Vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E9%87%8D%E6%A8%A1%E5%BC%8F"><span class="toc-text">多重模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#in-x2F-exists-%E8%AF%AD%E5%8F%A5"><span class="toc-text">in&#x2F;exists 语句</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CBO-%E4%BC%98%E5%8C%96"><span class="toc-text">CBO 优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8"><span class="toc-text">谓词下推</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapJoin"><span class="toc-text">MapJoin</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E8%A1%A8%E3%80%81%E5%A4%A7%E8%A1%A8-SMB-Join%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-text">大表、大表 SMB Join（重点）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF"><span class="toc-text">笛卡尔积</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-4-%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-text">第 4 章 数据倾斜（重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96"><span class="toc-text">单表数据倾斜优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0"><span class="toc-text">使用参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0-Reduce-%E6%95%B0%E9%87%8F%EF%BC%88%E5%A4%9A%E4%B8%AA-Key-%E5%90%8C%E6%97%B6%E5%AF%BC%E8%87%B4%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%89"><span class="toc-text">增加 Reduce 数量（多个 Key 同时导致数据倾斜）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Join-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96"><span class="toc-text">Join 数据倾斜优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0-1"><span class="toc-text">使用参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapJoin-1"><span class="toc-text">MapJoin</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-5-%E7%AB%A0-Hive-Job-%E4%BC%98%E5%8C%96"><span class="toc-text">第 5 章 Hive Job 优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-Map-%E4%BC%98%E5%8C%96"><span class="toc-text">Hive Map 优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E6%96%87%E4%BB%B6%E5%A2%9E%E5%8A%A0-Map-%E6%95%B0"><span class="toc-text">复杂文件增加 Map 数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6"><span class="toc-text">小文件进行合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map-%E7%AB%AF%E8%81%9A%E5%90%88"><span class="toc-text">Map 端聚合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C"><span class="toc-text">推测执行</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-Reduce-%E4%BC%98%E5%8C%96"><span class="toc-text">Hive Reduce 优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE-Reduce-%E6%95%B0"><span class="toc-text">合理设置 Reduce 数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C-1"><span class="toc-text">推测执行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC-6-%E7%AB%A0-Hive-On-Spark"><span class="toc-text">第 6 章 Hive On Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Executor-%E5%8F%82%E6%95%B0"><span class="toc-text">Executor 参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-executor-cores"><span class="toc-text">spark.executor.cores</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-executor-memory-x2F-spark-yarn-executor-memoryOverhead"><span class="toc-text">spark.executor.memory&#x2F;spark.yarn.executor.memoryOverhead</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-executor-instances"><span class="toc-text">spark.executor.instances</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-dynamicAllocation-enabled"><span class="toc-text">spark.dynamicAllocation.enabled</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E6%A0%B7%E4%BE%8B%E5%8F%82%E8%80%83"><span class="toc-text">参数配置样例参考</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Driver-%E5%8F%82%E6%95%B0"><span class="toc-text">Driver 参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-driver-cores"><span class="toc-text">spark.driver.cores</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-driver-memory-x2F-spark-driver-memoryOverhead"><span class="toc-text">spark.driver.memory&#x2F;spark.driver.memoryOverhead</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/21/appInstall%E2%80%94%E2%80%94Java%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4/" title="appInstall——Java安装步骤">appInstall——Java安装步骤</a><time datetime="2024-10-21T02:20:13.000Z" title="发表于 2024-10-21 10:20:13">2024-10-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/15/Rust%20%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80/" title="Rust 程序设计语言">Rust 程序设计语言</a><time datetime="2024-09-15T11:19:19.000Z" title="发表于 2024-09-15 19:19:19">2024-09-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/appInstall%E2%80%94%E2%80%94mysql_windows%E7%89%88%E8%87%AA%E5%AE%9A%E4%B9%89%E7%AE%80%E6%98%93%E5%8C%96%E5%AE%89%E8%A3%85/" title="appInstall——mysql_windows版自定义简易化安装">appInstall——mysql_windows版自定义简易化安装</a><time datetime="2024-06-28T07:20:13.000Z" title="发表于 2024-06-28 15:20:13">2024-06-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/1558.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By DingQuan Zuo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">道可道，非恒道也！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>